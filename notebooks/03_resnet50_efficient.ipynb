{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87334838",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a7c8d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Host: gorina6\n",
      "CUDA: True\n",
      "Fri Oct  3 14:24:36 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.144.03             Driver Version: 550.144.03     CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n"
     ]
    }
   ],
   "source": [
    "import platform, torch, subprocess\n",
    "print(\"Host:\", platform.node())               # gorina6\n",
    "print(\"CUDA:\", torch.cuda.is_available())     # True\n",
    "print(subprocess.getoutput(\"nvidia-smi | head -n 5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf0e1811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Host: gorina6\n",
      "Torch: 2.7.1+cu118\n",
      "CUDA (build): 11.8\n",
      "CUDA available: True\n",
      "cuDNN: 90100\n",
      "GPU count: 8\n",
      "0 Tesla V100-PCIE-32GB\n",
      "1 Tesla V100-PCIE-32GB\n",
      "2 Tesla V100-PCIE-32GB\n",
      "3 Tesla V100-PCIE-32GB\n",
      "4 Tesla V100-PCIE-32GB\n",
      "5 Tesla V100-PCIE-32GB\n",
      "6 Tesla V100-PCIE-32GB\n",
      "7 Tesla V100-PCIE-32GB\n"
     ]
    }
   ],
   "source": [
    "import torch, platform\n",
    "print(\"Host:\", platform.node())\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"CUDA (build):\", torch.version.cuda)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"cuDNN:\", torch.backends.cudnn.version())\n",
    "print(\"GPU count:\", torch.cuda.device_count())\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(i, torch.cuda.get_device_name(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "627da106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU mem free/total: 15.83 / 34.07 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "free, total = torch.cuda.mem_get_info()\n",
    "print(f\"GPU mem free/total: {free/1e9:.2f} / {total/1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c696d380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla V100-PCIE-32GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bfe05eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA_VISIBLE_DEVICES = None\n",
      "Visible count = 8\n",
      "Using: Tesla V100-PCIE-32GB\n"
     ]
    }
   ],
   "source": [
    "import os, torch\n",
    "print(\"CUDA_VISIBLE_DEVICES =\", os.environ.get(\"CUDA_VISIBLE_DEVICES\"))\n",
    "print(\"Visible count =\", torch.cuda.device_count())\n",
    "print(\"Using:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b723e309",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11289a7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.device_count()  # should be 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "580e99d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing: 786 / 47248\n",
      "                       filename          video_id  frame_number  \\\n",
      "6865  eb0203196e284797_1157.jpg  eb0203196e284797          1157   \n",
      "6866  eb0203196e284797_1158.jpg  eb0203196e284797          1158   \n",
      "6867  eb0203196e284797_1160.jpg  eb0203196e284797          1160   \n",
      "6868  eb0203196e284797_1167.jpg  eb0203196e284797          1167   \n",
      "6869  eb0203196e284797_1168.jpg  eb0203196e284797          1168   \n",
      "\n",
      "     finding_category     finding_class     x1     y1     x2     y2     x3  \\\n",
      "6865          Anatomy  Ampulla of Vater  238.0  196.0  327.0  196.0  327.0   \n",
      "6866          Anatomy  Ampulla of Vater  138.0    0.0  251.0    0.0  251.0   \n",
      "6867          Anatomy  Ampulla of Vater   69.0    0.0  153.0    0.0  153.0   \n",
      "6868          Anatomy  Ampulla of Vater    4.0  115.0   56.0  115.0   56.0   \n",
      "6869          Anatomy  Ampulla of Vater   57.0  182.0  137.0  182.0  137.0   \n",
      "\n",
      "         y3     x4     y4                                  image_path  \n",
      "6865  289.0  238.0  289.0  Ampulla of Vater/eb0203196e284797_1157.jpg  \n",
      "6866   66.0  138.0   66.0  Ampulla of Vater/eb0203196e284797_1158.jpg  \n",
      "6867   45.0   69.0   45.0  Ampulla of Vater/eb0203196e284797_1160.jpg  \n",
      "6868  190.0    4.0  190.0  Ampulla of Vater/eb0203196e284797_1167.jpg  \n",
      "6869  258.0   57.0  258.0  Ampulla of Vater/eb0203196e284797_1168.jpg  \n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "metadata_csv = Path(\"/home/stud/fwag/bhome/ele670_project/data/raw/metadata.csv\")\n",
    "dataset_root = Path(\"/home/stud/fwag/bhome/ele670_project/data/raw/labelled_images\")\n",
    "\n",
    "df = pd.read_csv(metadata_csv, sep=\";\")\n",
    "df[\"finding_class\"] = df[\"finding_class\"].astype(str).str.strip()\n",
    "df[\"filename\"] = df[\"filename\"].astype(str).str.strip()\n",
    "# Build relative image_path that matches your folder layout\n",
    "# labelled_images/<finding_class>/<filename>\n",
    "df[\"image_path\"] = df.apply(\n",
    "    lambda r: str(Path(r[\"finding_class\"]) / r[\"filename\"]), axis=1\n",
    ")\n",
    "\n",
    "bad = df[~df[\"image_path\"].apply(lambda p: (dataset_root / p).exists())]\n",
    "print(f\"Missing: {len(bad)} / {len(df)}\")\n",
    "print(bad.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e18a1398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Class names with no folder match (case-insensitive): 0\n",
      "[]\n",
      "[CHECK] Missing after folder mapping: 0 / 47248\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>video_id</th>\n",
       "      <th>frame_number</th>\n",
       "      <th>finding_category</th>\n",
       "      <th>finding_class</th>\n",
       "      <th>x1</th>\n",
       "      <th>y1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y2</th>\n",
       "      <th>x3</th>\n",
       "      <th>y3</th>\n",
       "      <th>x4</th>\n",
       "      <th>y4</th>\n",
       "      <th>image_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [filename, video_id, frame_number, finding_category, finding_class, x1, y1, x2, y2, x3, y3, x4, y4, image_path]\n",
       "Index: []"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "dataset_root = Path(\"/home/stud/fwag/bhome/ele670_project/data/raw/labelled_images\")\n",
    "\n",
    "# Build case-insensitive map of class folders that exist on disk\n",
    "classes_on_disk = {p.name.lower().strip(): p.name for p in dataset_root.iterdir() if p.is_dir()}\n",
    "\n",
    "# Which CSV class names don't have a direct case-insensitive match?\n",
    "csv_classes = sorted(df[\"finding_class\"].astype(str).str.strip().unique())\n",
    "no_match = [c for c in csv_classes if c.lower().strip() not in classes_on_disk]\n",
    "print(f\"[INFO] Class names with no folder match (case-insensitive): {len(no_match)}\")\n",
    "print(no_match[:20])  # peek\n",
    "\n",
    "# Optional: add manual fixes if needed (e.g., punctuation variants)\n",
    "manual_map = {\n",
    "    # \"CSV value\": \"Exact folder name on disk\"\n",
    "    # e.g., \"Reduced Mucosal View\": \"Reduced mucosal view\",\n",
    "    # add entries if you see them in `no_match`\n",
    "}\n",
    "\n",
    "def resolve_class_folder(name: str) -> str:\n",
    "    name = str(name).strip()\n",
    "    if name in manual_map:\n",
    "        return manual_map[name]\n",
    "    return classes_on_disk.get(name.lower().strip(), name)  # fallback to original (will show as missing if wrong)\n",
    "\n",
    "# Rebuild image_path using the resolved folder name\n",
    "df[\"image_path\"] = df.apply(lambda r: str(Path(resolve_class_folder(r[\"finding_class\"])) / r[\"filename\"]), axis=1)\n",
    "\n",
    "# Check again\n",
    "bad = df[~df[\"image_path\"].apply(lambda p: (dataset_root / p).exists())]\n",
    "print(f\"[CHECK] Missing after folder mapping: {len(bad)} / {len(df)}\")\n",
    "bad.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8954c230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SAVED] Normalized metadata CSV → /home/stud/fwag/bhome/ele670_project/data/processed/metadata_normalized.csv\n",
      "                                       image_path        finding_class  \\\n",
      "0  Normal clean mucosa/0728084c8da942d9_22803.jpg  Normal clean mucosa   \n",
      "1  Normal clean mucosa/0728084c8da942d9_22804.jpg  Normal clean mucosa   \n",
      "2  Normal clean mucosa/0728084c8da942d9_22805.jpg  Normal clean mucosa   \n",
      "3  Normal clean mucosa/0728084c8da942d9_22806.jpg  Normal clean mucosa   \n",
      "4  Normal clean mucosa/0728084c8da942d9_22807.jpg  Normal clean mucosa   \n",
      "\n",
      "           video_id  encoded_label  \n",
      "0  0728084c8da942d9              9  \n",
      "1  0728084c8da942d9              9  \n",
      "2  0728084c8da942d9              9  \n",
      "3  0728084c8da942d9              9  \n",
      "4  0728084c8da942d9              9  \n"
     ]
    }
   ],
   "source": [
    "# Save a cleaned / normalized CSV for training\n",
    "\n",
    "out_csv = Path(\"/home/stud/fwag/bhome/ele670_project/data/processed/metadata_normalized.csv\")\n",
    "\n",
    "# Pick only the columns you actually need for training\n",
    "df_out = df[[\"image_path\", \"finding_class\", \"video_id\"]].copy()\n",
    "\n",
    "# Optional: add the encoded label column (integer codes for classes)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "lb = LabelEncoder()\n",
    "df_out[\"encoded_label\"] = lb.fit_transform(df_out[\"finding_class\"])\n",
    "\n",
    "df_out.to_csv(out_csv, index=False)\n",
    "\n",
    "print(f\"[SAVED] Normalized metadata CSV → {out_csv}\")\n",
    "print(df_out.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "543501f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] How many classes each video has (top 10):\n",
      "video_id\n",
      "04a78ef00c5245e0    6\n",
      "48579eec79784294    6\n",
      "64440803f87b4843    6\n",
      "8ebf0e483cac48d6    6\n",
      "8885668afb844852    5\n",
      "5e59c7fdb16c4228    5\n",
      "eb0203196e284797    5\n",
      "3ada4222967f421d    4\n",
      "131368cc17e44240    4\n",
      "7a47e8eacea04e64    4\n",
      "Name: finding_class, dtype: int64\n",
      "\n",
      "[INFO] Distribution of class counts per video:\n",
      "finding_class\n",
      "1     2\n",
      "2     8\n",
      "3    18\n",
      "4     8\n",
      "5     3\n",
      "6     4\n",
      "Name: count, dtype: int64\n",
      "\n",
      "[INFO] Classes found in video 04a78ef00c5245e0:\n",
      "['Pylorus' 'Normal clean mucosa' 'Ileocecal valve' 'Lymphangiectasia'\n",
      " 'Angiectasia' 'Blood - fresh']\n"
     ]
    }
   ],
   "source": [
    "# Number of unique classes per video\n",
    "per_video_classes = df.groupby(\"video_id\")[\"finding_class\"].nunique().sort_values(ascending=False)\n",
    "\n",
    "print(\"[INFO] How many classes each video has (top 10):\")\n",
    "print(per_video_classes.head(10))\n",
    "\n",
    "# Distribution of \"number of classes per video\"\n",
    "print(\"\\n[INFO] Distribution of class counts per video:\")\n",
    "print(per_video_classes.value_counts().sort_index())\n",
    "\n",
    "# Example: list which classes appear together in a specific video\n",
    "example_video = per_video_classes.index[0]  # video with most classes\n",
    "classes_in_example = df.loc[df[\"video_id\"] == example_video, \"finding_class\"].unique()\n",
    "print(f\"\\n[INFO] Classes found in video {example_video}:\")\n",
    "print(classes_in_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9aebad13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total distinct videos: 43\n"
     ]
    }
   ],
   "source": [
    "n_videos = df[\"video_id\"].nunique()\n",
    "print(f\"Total distinct videos: {n_videos}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96f36011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA visible devices: 1\n",
      "Using GPU: Tesla V100-PCIE-32GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "\n",
    "import torch\n",
    "print(\"CUDA visible devices:\", torch.cuda.device_count())\n",
    "print(\"Using GPU:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9787aab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity check done ✅ | Batch size: 8, Loss: 3.3002\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# ==== CONFIG ====\n",
    "CSV_PATH = \"/bhome/fwag/ele670_project/data/processed/metadata_normalized.csv\"\n",
    "IMG_ROOT = \"/bhome/fwag/ele670_project/data/raw/labelled_images\"\n",
    "BATCH_SIZE = 8\n",
    "NUM_WORKERS = 4\n",
    "\n",
    "# ==== DATASET ====\n",
    "class KvasirDataset(Dataset):\n",
    "    def __init__(self, csv_path, img_root, transform=None):\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.img_root = img_root\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = os.path.join(self.img_root, row[\"image_path\"])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        label = int(row[\"encoded_label\"])\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "# Simple transforms for sanity check\n",
    "transform = T.Compose([\n",
    "    T.Resize((224,224)),\n",
    "    T.ToTensor()\n",
    "])\n",
    "\n",
    "dataset = KvasirDataset(CSV_PATH, IMG_ROOT, transform)\n",
    "loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "\n",
    "# ==== MODEL ====\n",
    "num_classes = dataset.df[\"encoded_label\"].nunique()\n",
    "model = models.resnet50(weights=None)   # no pretrained, just sanity check\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "\n",
    "# ==== SANITY TRAIN LOOP (1 batch only) ====\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "\n",
    "images, labels = next(iter(loader))\n",
    "images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# forward\n",
    "outputs = model(images)\n",
    "loss = criterion(outputs, labels)\n",
    "\n",
    "# backward\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "print(f\"Sanity check done ✅ | Batch size: {images.size(0)}, Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6402707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Tesla V100-PCIE-32GB\n",
      "Memory allocated: 0.38 GB\n",
      "First rows of metadata.csv:\n",
      "                                       image_path        finding_class  \\\n",
      "0  Normal clean mucosa/0728084c8da942d9_22803.jpg  Normal clean mucosa   \n",
      "1  Normal clean mucosa/0728084c8da942d9_22804.jpg  Normal clean mucosa   \n",
      "2  Normal clean mucosa/0728084c8da942d9_22805.jpg  Normal clean mucosa   \n",
      "3  Normal clean mucosa/0728084c8da942d9_22806.jpg  Normal clean mucosa   \n",
      "4  Normal clean mucosa/0728084c8da942d9_22807.jpg  Normal clean mucosa   \n",
      "\n",
      "           video_id  encoded_label  \n",
      "0  0728084c8da942d9              9  \n",
      "1  0728084c8da942d9              9  \n",
      "2  0728084c8da942d9              9  \n",
      "3  0728084c8da942d9              9  \n",
      "4  0728084c8da942d9              9  \n",
      "Number of classes: 14\n",
      "Train samples: 37430 | Val samples: 9818\n",
      "Train videos: 34 | Val videos: 9\n",
      "One training batch (images, labels):\n",
      "Image batch shape: torch.Size([32, 3, 224, 224])\n",
      "Label batch shape: torch.Size([32])\n",
      "\n",
      "==== START TRAINING ====\n",
      "\n",
      "\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3190418/2432252680.py:126: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n",
      "/tmp/ipykernel_3190418/2432252680.py:141: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=True):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 1/1170 | Loss: 2.7401 | Batch acc: 0.00\n",
      "  Step 10/1170 | Loss: 0.9073 | Batch acc: 0.75\n",
      "  Step 20/1170 | Loss: 0.5375 | Batch acc: 0.84\n",
      "  Step 30/1170 | Loss: 1.3675 | Batch acc: 0.69\n",
      "  Step 40/1170 | Loss: 1.3706 | Batch acc: 0.62\n",
      "  Step 50/1170 | Loss: 0.9294 | Batch acc: 0.75\n",
      "  Step 60/1170 | Loss: 0.6208 | Batch acc: 0.78\n",
      "  Step 70/1170 | Loss: 1.0341 | Batch acc: 0.59\n",
      "  Step 80/1170 | Loss: 0.5050 | Batch acc: 0.88\n",
      "  Step 90/1170 | Loss: 0.6382 | Batch acc: 0.84\n",
      "  Step 100/1170 | Loss: 0.4670 | Batch acc: 0.84\n",
      "  Step 110/1170 | Loss: 0.3199 | Batch acc: 0.94\n",
      "  Step 120/1170 | Loss: 0.6109 | Batch acc: 0.78\n",
      "  Step 130/1170 | Loss: 0.3193 | Batch acc: 0.94\n",
      "  Step 140/1170 | Loss: 0.5663 | Batch acc: 0.81\n",
      "  Step 150/1170 | Loss: 0.4441 | Batch acc: 0.84\n",
      "  Step 160/1170 | Loss: 0.6226 | Batch acc: 0.81\n",
      "  Step 170/1170 | Loss: 0.3706 | Batch acc: 0.94\n",
      "  Step 180/1170 | Loss: 0.4049 | Batch acc: 0.97\n",
      "  Step 190/1170 | Loss: 0.4931 | Batch acc: 0.84\n",
      "  Step 200/1170 | Loss: 0.1632 | Batch acc: 0.97\n",
      "  Step 210/1170 | Loss: 0.4126 | Batch acc: 0.78\n",
      "  Step 220/1170 | Loss: 0.6373 | Batch acc: 0.78\n",
      "  Step 230/1170 | Loss: 0.5492 | Batch acc: 0.84\n",
      "  Step 240/1170 | Loss: 0.5981 | Batch acc: 0.81\n",
      "  Step 250/1170 | Loss: 0.6370 | Batch acc: 0.84\n",
      "  Step 260/1170 | Loss: 0.6786 | Batch acc: 0.78\n",
      "  Step 270/1170 | Loss: 0.4745 | Batch acc: 0.88\n",
      "  Step 280/1170 | Loss: 0.4842 | Batch acc: 0.94\n",
      "  Step 290/1170 | Loss: 0.4380 | Batch acc: 0.91\n",
      "  Step 300/1170 | Loss: 0.1687 | Batch acc: 0.97\n",
      "  Step 310/1170 | Loss: 0.3805 | Batch acc: 0.91\n",
      "  Step 320/1170 | Loss: 0.2664 | Batch acc: 0.94\n",
      "  Step 330/1170 | Loss: 0.2141 | Batch acc: 0.94\n",
      "  Step 340/1170 | Loss: 0.2777 | Batch acc: 0.97\n",
      "  Step 350/1170 | Loss: 0.3091 | Batch acc: 0.88\n",
      "  Step 360/1170 | Loss: 0.2249 | Batch acc: 0.94\n",
      "  Step 370/1170 | Loss: 0.4003 | Batch acc: 0.84\n",
      "  Step 380/1170 | Loss: 0.4517 | Batch acc: 0.88\n",
      "  Step 390/1170 | Loss: 0.3130 | Batch acc: 0.91\n",
      "  Step 400/1170 | Loss: 0.8656 | Batch acc: 0.84\n",
      "  Step 410/1170 | Loss: 0.3821 | Batch acc: 0.91\n",
      "  Step 420/1170 | Loss: 0.3958 | Batch acc: 0.81\n",
      "  Step 430/1170 | Loss: 0.2929 | Batch acc: 0.84\n",
      "  Step 440/1170 | Loss: 0.5165 | Batch acc: 0.88\n",
      "  Step 450/1170 | Loss: 0.3159 | Batch acc: 0.88\n",
      "  Step 460/1170 | Loss: 0.3719 | Batch acc: 0.97\n",
      "  Step 470/1170 | Loss: 0.4839 | Batch acc: 0.88\n",
      "  Step 480/1170 | Loss: 0.1268 | Batch acc: 0.97\n",
      "  Step 490/1170 | Loss: 0.0888 | Batch acc: 1.00\n",
      "  Step 500/1170 | Loss: 0.2672 | Batch acc: 0.91\n",
      "  Step 510/1170 | Loss: 0.2020 | Batch acc: 0.94\n",
      "  Step 520/1170 | Loss: 0.2766 | Batch acc: 0.91\n",
      "  Step 530/1170 | Loss: 0.0954 | Batch acc: 0.97\n",
      "  Step 540/1170 | Loss: 0.3133 | Batch acc: 0.84\n",
      "  Step 550/1170 | Loss: 0.1501 | Batch acc: 0.94\n",
      "  Step 560/1170 | Loss: 0.4750 | Batch acc: 0.81\n",
      "  Step 570/1170 | Loss: 0.4003 | Batch acc: 0.91\n",
      "  Step 580/1170 | Loss: 0.0754 | Batch acc: 1.00\n",
      "  Step 590/1170 | Loss: 0.2187 | Batch acc: 0.91\n",
      "  Step 600/1170 | Loss: 0.3067 | Batch acc: 0.91\n",
      "  Step 610/1170 | Loss: 0.3444 | Batch acc: 0.88\n",
      "  Step 620/1170 | Loss: 0.2338 | Batch acc: 0.97\n",
      "  Step 630/1170 | Loss: 0.2647 | Batch acc: 0.94\n",
      "  Step 640/1170 | Loss: 0.0409 | Batch acc: 1.00\n",
      "  Step 650/1170 | Loss: 0.1695 | Batch acc: 0.97\n",
      "  Step 660/1170 | Loss: 0.1780 | Batch acc: 0.94\n",
      "  Step 670/1170 | Loss: 0.0670 | Batch acc: 1.00\n",
      "  Step 680/1170 | Loss: 0.1702 | Batch acc: 0.94\n",
      "  Step 690/1170 | Loss: 0.2318 | Batch acc: 0.91\n",
      "  Step 700/1170 | Loss: 0.3792 | Batch acc: 0.88\n",
      "  Step 710/1170 | Loss: 0.1884 | Batch acc: 0.91\n",
      "  Step 720/1170 | Loss: 0.4390 | Batch acc: 0.88\n",
      "  Step 730/1170 | Loss: 0.2949 | Batch acc: 0.91\n",
      "  Step 740/1170 | Loss: 0.3494 | Batch acc: 0.88\n",
      "  Step 750/1170 | Loss: 0.1610 | Batch acc: 0.94\n",
      "  Step 760/1170 | Loss: 0.1003 | Batch acc: 0.97\n",
      "  Step 770/1170 | Loss: 0.0838 | Batch acc: 0.97\n",
      "  Step 780/1170 | Loss: 0.1739 | Batch acc: 0.94\n",
      "  Step 790/1170 | Loss: 0.4317 | Batch acc: 0.84\n",
      "  Step 800/1170 | Loss: 0.4819 | Batch acc: 0.84\n",
      "  Step 810/1170 | Loss: 0.1751 | Batch acc: 0.94\n",
      "  Step 820/1170 | Loss: 0.1797 | Batch acc: 0.94\n",
      "  Step 830/1170 | Loss: 0.2215 | Batch acc: 0.91\n",
      "  Step 840/1170 | Loss: 0.3197 | Batch acc: 0.91\n",
      "  Step 850/1170 | Loss: 0.1629 | Batch acc: 0.97\n",
      "  Step 860/1170 | Loss: 0.0991 | Batch acc: 1.00\n",
      "  Step 870/1170 | Loss: 0.3195 | Batch acc: 0.91\n",
      "  Step 880/1170 | Loss: 0.2964 | Batch acc: 0.88\n",
      "  Step 890/1170 | Loss: 0.2123 | Batch acc: 0.91\n",
      "  Step 900/1170 | Loss: 0.1945 | Batch acc: 0.91\n",
      "  Step 910/1170 | Loss: 0.0595 | Batch acc: 1.00\n",
      "  Step 920/1170 | Loss: 0.0716 | Batch acc: 1.00\n",
      "  Step 930/1170 | Loss: 0.1825 | Batch acc: 0.91\n",
      "  Step 940/1170 | Loss: 0.0758 | Batch acc: 0.97\n",
      "  Step 950/1170 | Loss: 0.0916 | Batch acc: 0.97\n",
      "  Step 960/1170 | Loss: 0.0396 | Batch acc: 1.00\n",
      "  Step 970/1170 | Loss: 0.0826 | Batch acc: 0.97\n",
      "  Step 980/1170 | Loss: 0.0236 | Batch acc: 1.00\n",
      "  Step 990/1170 | Loss: 0.0961 | Batch acc: 0.97\n",
      "  Step 1000/1170 | Loss: 0.3154 | Batch acc: 0.91\n",
      "  Step 1010/1170 | Loss: 0.2250 | Batch acc: 0.94\n",
      "  Step 1020/1170 | Loss: 0.0580 | Batch acc: 1.00\n",
      "  Step 1030/1170 | Loss: 0.1520 | Batch acc: 0.94\n",
      "  Step 1040/1170 | Loss: 0.2079 | Batch acc: 0.94\n",
      "  Step 1050/1170 | Loss: 0.1855 | Batch acc: 0.91\n",
      "  Step 1060/1170 | Loss: 0.0693 | Batch acc: 1.00\n",
      "  Step 1070/1170 | Loss: 0.0602 | Batch acc: 1.00\n",
      "  Step 1080/1170 | Loss: 0.0623 | Batch acc: 1.00\n",
      "  Step 1090/1170 | Loss: 0.0334 | Batch acc: 1.00\n",
      "  Step 1100/1170 | Loss: 0.2256 | Batch acc: 0.94\n",
      "  Step 1110/1170 | Loss: 0.3222 | Batch acc: 0.84\n",
      "  Step 1120/1170 | Loss: 0.5678 | Batch acc: 0.81\n",
      "  Step 1130/1170 | Loss: 0.1160 | Batch acc: 0.97\n",
      "  Step 1140/1170 | Loss: 0.1365 | Batch acc: 0.94\n",
      "  Step 1150/1170 | Loss: 0.1182 | Batch acc: 0.97\n",
      "  Step 1160/1170 | Loss: 0.2714 | Batch acc: 0.91\n",
      "  Step 1170/1170 | Loss: 0.2812 | Batch acc: 0.86\n",
      "Train: loss=0.3513, acc=0.893\n",
      "  Step 1/307 | Loss: 2.1775 | Batch acc: 0.34\n",
      "  Step 10/307 | Loss: 0.5127 | Batch acc: 0.75\n",
      "  Step 20/307 | Loss: 0.0545 | Batch acc: 1.00\n",
      "  Step 30/307 | Loss: 0.0000 | Batch acc: 1.00\n",
      "  Step 40/307 | Loss: 0.0007 | Batch acc: 1.00\n",
      "  Step 50/307 | Loss: 1.1852 | Batch acc: 0.12\n",
      "  Step 60/307 | Loss: 1.2073 | Batch acc: 0.22\n",
      "  Step 70/307 | Loss: 0.1274 | Batch acc: 0.94\n",
      "  Step 80/307 | Loss: 0.0000 | Batch acc: 1.00\n",
      "  Step 90/307 | Loss: 1.3156 | Batch acc: 0.38\n",
      "  Step 100/307 | Loss: 0.8684 | Batch acc: 0.66\n",
      "  Step 110/307 | Loss: 1.3657 | Batch acc: 0.22\n",
      "  Step 120/307 | Loss: 2.1352 | Batch acc: 0.31\n",
      "  Step 130/307 | Loss: 10.5238 | Batch acc: 0.06\n",
      "  Step 140/307 | Loss: 0.3774 | Batch acc: 0.91\n",
      "  Step 150/307 | Loss: 0.0160 | Batch acc: 1.00\n",
      "  Step 160/307 | Loss: 0.3693 | Batch acc: 0.91\n",
      "  Step 170/307 | Loss: 0.1077 | Batch acc: 1.00\n",
      "  Step 180/307 | Loss: 0.0807 | Batch acc: 1.00\n",
      "  Step 190/307 | Loss: 0.1949 | Batch acc: 0.94\n",
      "  Step 200/307 | Loss: 0.0066 | Batch acc: 1.00\n",
      "  Step 210/307 | Loss: 0.0001 | Batch acc: 1.00\n",
      "  Step 220/307 | Loss: 0.6276 | Batch acc: 0.59\n",
      "  Step 230/307 | Loss: 0.2586 | Batch acc: 0.97\n",
      "  Step 240/307 | Loss: 3.2949 | Batch acc: 0.03\n",
      "  Step 250/307 | Loss: 8.4422 | Batch acc: 0.16\n",
      "  Step 260/307 | Loss: 0.0340 | Batch acc: 1.00\n",
      "  Step 270/307 | Loss: 0.0120 | Batch acc: 1.00\n",
      "  Step 280/307 | Loss: 0.0065 | Batch acc: 1.00\n",
      "  Step 290/307 | Loss: 0.0001 | Batch acc: 1.00\n",
      "  Step 300/307 | Loss: 0.0006 | Batch acc: 1.00\n",
      "Val:   loss=0.7413, acc=0.763\n",
      "  ✅ New best model saved (acc=0.763)\n",
      "\n",
      "Epoch 2/2\n",
      "  Step 1/1170 | Loss: 0.2772 | Batch acc: 0.91\n",
      "  Step 10/1170 | Loss: 0.1243 | Batch acc: 0.94\n",
      "  Step 20/1170 | Loss: 0.1291 | Batch acc: 0.97\n",
      "  Step 30/1170 | Loss: 0.1804 | Batch acc: 0.97\n",
      "  Step 40/1170 | Loss: 0.1640 | Batch acc: 0.88\n",
      "  Step 50/1170 | Loss: 0.0968 | Batch acc: 1.00\n",
      "  Step 60/1170 | Loss: 0.0343 | Batch acc: 1.00\n",
      "  Step 70/1170 | Loss: 0.1438 | Batch acc: 0.97\n",
      "  Step 80/1170 | Loss: 0.1343 | Batch acc: 0.94\n",
      "  Step 90/1170 | Loss: 0.0458 | Batch acc: 1.00\n",
      "  Step 100/1170 | Loss: 0.0676 | Batch acc: 0.97\n",
      "  Step 110/1170 | Loss: 0.0789 | Batch acc: 0.97\n",
      "  Step 120/1170 | Loss: 0.0523 | Batch acc: 1.00\n",
      "  Step 130/1170 | Loss: 0.2447 | Batch acc: 0.88\n",
      "  Step 140/1170 | Loss: 0.1811 | Batch acc: 0.97\n",
      "  Step 150/1170 | Loss: 0.1683 | Batch acc: 0.94\n",
      "  Step 160/1170 | Loss: 0.0911 | Batch acc: 0.97\n",
      "  Step 170/1170 | Loss: 0.2470 | Batch acc: 0.94\n",
      "  Step 180/1170 | Loss: 0.2093 | Batch acc: 0.88\n",
      "  Step 190/1170 | Loss: 0.0623 | Batch acc: 0.97\n",
      "  Step 200/1170 | Loss: 0.0073 | Batch acc: 1.00\n",
      "  Step 210/1170 | Loss: 0.1928 | Batch acc: 0.94\n",
      "  Step 220/1170 | Loss: 0.2091 | Batch acc: 0.88\n",
      "  Step 230/1170 | Loss: 0.2281 | Batch acc: 0.91\n",
      "  Step 240/1170 | Loss: 0.0469 | Batch acc: 1.00\n",
      "  Step 250/1170 | Loss: 0.2818 | Batch acc: 0.94\n",
      "  Step 260/1170 | Loss: 0.2606 | Batch acc: 0.94\n",
      "  Step 270/1170 | Loss: 0.1717 | Batch acc: 0.91\n",
      "  Step 280/1170 | Loss: 0.1001 | Batch acc: 0.97\n",
      "  Step 290/1170 | Loss: 0.3420 | Batch acc: 0.88\n",
      "  Step 300/1170 | Loss: 0.0886 | Batch acc: 0.97\n",
      "  Step 310/1170 | Loss: 0.0377 | Batch acc: 0.97\n",
      "  Step 320/1170 | Loss: 0.0372 | Batch acc: 1.00\n",
      "  Step 330/1170 | Loss: 0.1668 | Batch acc: 0.94\n",
      "  Step 340/1170 | Loss: 0.4895 | Batch acc: 0.91\n",
      "  Step 350/1170 | Loss: 0.0616 | Batch acc: 0.97\n",
      "  Step 360/1170 | Loss: 0.0717 | Batch acc: 1.00\n",
      "  Step 370/1170 | Loss: 0.0444 | Batch acc: 1.00\n",
      "  Step 380/1170 | Loss: 0.1393 | Batch acc: 0.91\n",
      "  Step 390/1170 | Loss: 0.1778 | Batch acc: 0.97\n",
      "  Step 400/1170 | Loss: 0.0644 | Batch acc: 1.00\n",
      "  Step 410/1170 | Loss: 0.2175 | Batch acc: 0.94\n",
      "  Step 420/1170 | Loss: 0.1692 | Batch acc: 0.97\n",
      "  Step 430/1170 | Loss: 0.1073 | Batch acc: 0.94\n",
      "  Step 440/1170 | Loss: 0.0793 | Batch acc: 0.97\n",
      "  Step 450/1170 | Loss: 0.0657 | Batch acc: 0.97\n",
      "  Step 460/1170 | Loss: 0.0979 | Batch acc: 0.94\n",
      "  Step 470/1170 | Loss: 0.1018 | Batch acc: 0.97\n",
      "  Step 480/1170 | Loss: 0.0279 | Batch acc: 1.00\n",
      "  Step 490/1170 | Loss: 0.0794 | Batch acc: 0.97\n",
      "  Step 500/1170 | Loss: 0.0096 | Batch acc: 1.00\n",
      "  Step 510/1170 | Loss: 0.0670 | Batch acc: 0.97\n",
      "  Step 520/1170 | Loss: 0.0199 | Batch acc: 1.00\n",
      "  Step 530/1170 | Loss: 0.3532 | Batch acc: 0.88\n",
      "  Step 540/1170 | Loss: 0.0313 | Batch acc: 0.97\n",
      "  Step 550/1170 | Loss: 0.0468 | Batch acc: 1.00\n",
      "  Step 560/1170 | Loss: 0.2141 | Batch acc: 0.94\n",
      "  Step 570/1170 | Loss: 0.1454 | Batch acc: 0.94\n",
      "  Step 580/1170 | Loss: 0.1049 | Batch acc: 0.97\n",
      "  Step 590/1170 | Loss: 0.0371 | Batch acc: 1.00\n",
      "  Step 600/1170 | Loss: 0.0305 | Batch acc: 1.00\n",
      "  Step 610/1170 | Loss: 0.0163 | Batch acc: 1.00\n",
      "  Step 620/1170 | Loss: 0.0360 | Batch acc: 1.00\n",
      "  Step 630/1170 | Loss: 0.0633 | Batch acc: 0.97\n",
      "  Step 640/1170 | Loss: 0.0480 | Batch acc: 0.97\n",
      "  Step 650/1170 | Loss: 0.1358 | Batch acc: 0.97\n",
      "  Step 660/1170 | Loss: 0.5439 | Batch acc: 0.84\n",
      "  Step 670/1170 | Loss: 0.0926 | Batch acc: 0.97\n",
      "  Step 680/1170 | Loss: 0.1707 | Batch acc: 0.91\n",
      "  Step 690/1170 | Loss: 0.1565 | Batch acc: 0.97\n",
      "  Step 700/1170 | Loss: 0.0439 | Batch acc: 1.00\n",
      "  Step 710/1170 | Loss: 0.1239 | Batch acc: 0.94\n",
      "  Step 720/1170 | Loss: 0.0389 | Batch acc: 1.00\n",
      "  Step 730/1170 | Loss: 0.0685 | Batch acc: 0.97\n",
      "  Step 740/1170 | Loss: 0.3070 | Batch acc: 0.91\n",
      "  Step 750/1170 | Loss: 0.0116 | Batch acc: 1.00\n",
      "  Step 760/1170 | Loss: 0.1042 | Batch acc: 0.97\n",
      "  Step 770/1170 | Loss: 0.0937 | Batch acc: 0.97\n",
      "  Step 780/1170 | Loss: 0.3124 | Batch acc: 0.91\n",
      "  Step 790/1170 | Loss: 0.1240 | Batch acc: 0.94\n",
      "  Step 800/1170 | Loss: 0.1657 | Batch acc: 0.97\n",
      "  Step 810/1170 | Loss: 0.0694 | Batch acc: 0.97\n",
      "  Step 820/1170 | Loss: 0.2379 | Batch acc: 0.91\n",
      "  Step 830/1170 | Loss: 0.0871 | Batch acc: 0.97\n",
      "  Step 840/1170 | Loss: 0.0494 | Batch acc: 1.00\n",
      "  Step 850/1170 | Loss: 0.0624 | Batch acc: 1.00\n",
      "  Step 860/1170 | Loss: 0.0836 | Batch acc: 0.97\n",
      "  Step 870/1170 | Loss: 0.3403 | Batch acc: 0.91\n",
      "  Step 880/1170 | Loss: 0.0588 | Batch acc: 0.97\n",
      "  Step 890/1170 | Loss: 0.2916 | Batch acc: 0.91\n",
      "  Step 900/1170 | Loss: 0.1632 | Batch acc: 0.94\n",
      "  Step 910/1170 | Loss: 0.1125 | Batch acc: 0.97\n",
      "  Step 920/1170 | Loss: 0.1887 | Batch acc: 0.91\n",
      "  Step 930/1170 | Loss: 0.0432 | Batch acc: 1.00\n",
      "  Step 940/1170 | Loss: 0.1010 | Batch acc: 0.97\n",
      "  Step 950/1170 | Loss: 0.0363 | Batch acc: 1.00\n",
      "  Step 960/1170 | Loss: 0.0264 | Batch acc: 1.00\n",
      "  Step 970/1170 | Loss: 0.0238 | Batch acc: 1.00\n",
      "  Step 980/1170 | Loss: 0.1933 | Batch acc: 0.97\n",
      "  Step 990/1170 | Loss: 0.0567 | Batch acc: 1.00\n",
      "  Step 1000/1170 | Loss: 0.0148 | Batch acc: 1.00\n",
      "  Step 1010/1170 | Loss: 0.3080 | Batch acc: 0.94\n",
      "  Step 1020/1170 | Loss: 0.2011 | Batch acc: 0.91\n",
      "  Step 1030/1170 | Loss: 0.1345 | Batch acc: 0.94\n",
      "  Step 1040/1170 | Loss: 0.1371 | Batch acc: 0.97\n",
      "  Step 1050/1170 | Loss: 0.0788 | Batch acc: 1.00\n",
      "  Step 1060/1170 | Loss: 0.1700 | Batch acc: 0.94\n",
      "  Step 1070/1170 | Loss: 0.1999 | Batch acc: 0.94\n",
      "  Step 1080/1170 | Loss: 0.1010 | Batch acc: 0.97\n",
      "  Step 1090/1170 | Loss: 0.0837 | Batch acc: 0.97\n",
      "  Step 1100/1170 | Loss: 0.2506 | Batch acc: 0.94\n",
      "  Step 1110/1170 | Loss: 0.0497 | Batch acc: 1.00\n",
      "  Step 1120/1170 | Loss: 0.0383 | Batch acc: 1.00\n",
      "  Step 1130/1170 | Loss: 0.1251 | Batch acc: 0.94\n",
      "  Step 1140/1170 | Loss: 0.2429 | Batch acc: 0.91\n",
      "  Step 1150/1170 | Loss: 0.1197 | Batch acc: 0.97\n",
      "  Step 1160/1170 | Loss: 0.1550 | Batch acc: 0.94\n",
      "  Step 1170/1170 | Loss: 0.1394 | Batch acc: 0.95\n",
      "Train: loss=0.1413, acc=0.955\n",
      "  Step 1/307 | Loss: 2.3800 | Batch acc: 0.34\n",
      "  Step 10/307 | Loss: 0.2010 | Batch acc: 0.94\n",
      "  Step 20/307 | Loss: 0.0158 | Batch acc: 1.00\n",
      "  Step 30/307 | Loss: 0.0001 | Batch acc: 1.00\n",
      "  Step 40/307 | Loss: 0.0027 | Batch acc: 1.00\n",
      "  Step 50/307 | Loss: 5.7866 | Batch acc: 0.00\n",
      "  Step 60/307 | Loss: 4.8996 | Batch acc: 0.00\n",
      "  Step 70/307 | Loss: 0.1524 | Batch acc: 0.94\n",
      "  Step 80/307 | Loss: 0.0001 | Batch acc: 1.00\n",
      "  Step 90/307 | Loss: 3.0749 | Batch acc: 0.06\n",
      "  Step 100/307 | Loss: 0.6330 | Batch acc: 0.75\n",
      "  Step 110/307 | Loss: 3.9887 | Batch acc: 0.00\n",
      "  Step 120/307 | Loss: 1.8790 | Batch acc: 0.28\n",
      "  Step 130/307 | Loss: 8.3719 | Batch acc: 0.09\n",
      "  Step 140/307 | Loss: 0.9286 | Batch acc: 0.66\n",
      "  Step 150/307 | Loss: 0.1422 | Batch acc: 0.97\n",
      "  Step 160/307 | Loss: 0.3676 | Batch acc: 0.84\n",
      "  Step 170/307 | Loss: 0.4183 | Batch acc: 0.84\n",
      "  Step 180/307 | Loss: 0.1831 | Batch acc: 1.00\n",
      "  Step 190/307 | Loss: 0.5663 | Batch acc: 0.88\n",
      "  Step 200/307 | Loss: 0.0025 | Batch acc: 1.00\n",
      "  Step 210/307 | Loss: 0.0001 | Batch acc: 1.00\n",
      "  Step 220/307 | Loss: 0.1728 | Batch acc: 1.00\n",
      "  Step 230/307 | Loss: 0.1694 | Batch acc: 0.94\n",
      "  Step 240/307 | Loss: 9.8961 | Batch acc: 0.00\n",
      "  Step 250/307 | Loss: 4.9444 | Batch acc: 0.16\n",
      "  Step 260/307 | Loss: 0.0211 | Batch acc: 1.00\n",
      "  Step 270/307 | Loss: 0.0050 | Batch acc: 1.00\n",
      "  Step 280/307 | Loss: 0.0097 | Batch acc: 1.00\n",
      "  Step 290/307 | Loss: 0.0000 | Batch acc: 1.00\n",
      "  Step 300/307 | Loss: 0.0036 | Batch acc: 1.00\n",
      "Val:   loss=1.3308, acc=0.707\n",
      "\n",
      "Training finished!\n",
      "Best validation accuracy: 0.763\n",
      "Elapsed time: 403.7 sec\n"
     ]
    }
   ],
   "source": [
    "# ==== Tiny ResNet50 sanity trainer for Kvasir-Capsule ====\n",
    "# - Beginner-friendly, step-by-step\n",
    "# - Prints debug info along the way\n",
    "# - Optimized for Tesla V100 (mixed precision, pinned memory, etc.)\n",
    "# =========================================================\n",
    "\n",
    "import os, time\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models\n",
    "\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from collections import Counter\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "CSV_PATH = \"/bhome/fwag/ele670_project/data/processed/metadata_normalized.csv\"\n",
    "IMG_ROOT   = \"/bhome/fwag/ele670_project/data/raw/labelled_images\"\n",
    "IMG_SIZE   = 224\n",
    "BATCH_SIZE = 32     # safe for V100 (can go higher if memory allows)\n",
    "NUM_WORKERS = 4     # adjust depending on system\n",
    "EPOCHS     = 2      # keep small for quick sanity check\n",
    "LR         = 1e-3\n",
    "SEED       = 42\n",
    "\n",
    "# ----------------------------------------\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "if device == \"cuda\":\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print(\"Memory allocated:\", round(torch.cuda.memory_allocated(0)/1024**3, 2), \"GB\")\n",
    "\n",
    "# --------------- DATASET ----------------\n",
    "class KvasirDataset(Dataset):\n",
    "    \"\"\"Custom dataset to read image paths + labels from CSV.\"\"\"\n",
    "    def __init__(self, df, img_root, transform):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.img_root = img_root\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        # Join root with relative path from CSV\n",
    "        img_path = os.path.join(self.img_root, row[\"image_path\"])\n",
    "        try:\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            print(f\"[WARNING] Failed to open {img_path}: {e}\")\n",
    "            # use black image as fallback\n",
    "            img = Image.new(\"RGB\", (IMG_SIZE, IMG_SIZE), (0,0,0))\n",
    "        img = self.transform(img)\n",
    "        label = int(row[\"encoded_label\"])\n",
    "        return img, label\n",
    "\n",
    "# --------------- LOAD CSV ----------------\n",
    "df = pd.read_csv(CSV_PATH, sep=\",\")\n",
    "print(\"First rows of metadata_processed.csv:\")\n",
    "print(df.head())\n",
    "\n",
    "# How many unique classes?\n",
    "num_classes = df[\"encoded_label\"].nunique()\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "\n",
    "# --------------- TRAIN/VAL SPLIT ----------------\n",
    "# Keep videos separated (no leakage between train/val)\n",
    "gss = GroupShuffleSplit(n_splits=1, train_size=0.8, random_state=SEED)\n",
    "train_idx, val_idx = next(gss.split(df, groups=df[\"video_id\"]))\n",
    "df_train, df_val = df.iloc[train_idx], df.iloc[val_idx]\n",
    "\n",
    "print(f\"Train samples: {len(df_train)} | Val samples: {len(df_val)}\")\n",
    "print(f\"Train videos: {df_train['video_id'].nunique()} | Val videos: {df_val['video_id'].nunique()}\")\n",
    "\n",
    "# --------------- TRANSFORMS ----------------\n",
    "# Simple transforms for sanity check\n",
    "mean = (0.485, 0.456, 0.406)\n",
    "std  = (0.229, 0.224, 0.225)\n",
    "\n",
    "train_tf = T.Compose([\n",
    "    T.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    T.RandomHorizontalFlip(p=0.5),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean, std),\n",
    "])\n",
    "\n",
    "val_tf = T.Compose([\n",
    "    T.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean, std),\n",
    "])\n",
    "\n",
    "# --------------- DATA LOADERS ----------------\n",
    "train_ds = KvasirDataset(df_train, IMG_ROOT, train_tf)\n",
    "val_ds   = KvasirDataset(df_val,   IMG_ROOT, val_tf)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "print(\"One training batch (images, labels):\")\n",
    "sample_imgs, sample_labels = next(iter(train_loader))\n",
    "print(\"Image batch shape:\", sample_imgs.shape)\n",
    "print(\"Label batch shape:\", sample_labels.shape)\n",
    "\n",
    "# --------------- MODEL ----------------\n",
    "# Load ResNet50 with ImageNet weights (helps it train faster)\n",
    "model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)  # replace classifier\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss + optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "# Mixed precision (faster on V100)\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "# --------------- TRAINING LOOP ----------------\n",
    "def run_epoch(loader, train=True):\n",
    "    if train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    total_loss, total_correct, total_samples = 0.0, 0, 0\n",
    "\n",
    "    for step, (imgs, labels) in enumerate(loader, start=1):\n",
    "        imgs = imgs.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=True):\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        if train:\n",
    "            optimizer.zero_grad()\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "        # Accuracy calculation\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        correct = (preds == labels).sum().item()\n",
    "        total_correct += correct\n",
    "        total_loss += loss.item() * labels.size(0)\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "        # Print debug info every 10 steps\n",
    "        if step % 10 == 0 or step == 1:\n",
    "            print(f\"  Step {step}/{len(loader)} | Loss: {loss.item():.4f} | \"\n",
    "                  f\"Batch acc: {correct/labels.size(0):.2f}\")\n",
    "\n",
    "    avg_loss = total_loss / total_samples\n",
    "    avg_acc = total_correct / total_samples\n",
    "    return avg_loss, avg_acc\n",
    "\n",
    "# --------------- TRAIN FOR A FEW EPOCHS ----------------\n",
    "print(\"\\n==== START TRAINING ====\\n\")\n",
    "best_val_acc = 0.0\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    print(f\"\\nEpoch {epoch}/{EPOCHS}\")\n",
    "\n",
    "    train_loss, train_acc = run_epoch(train_loader, train=True)\n",
    "    print(f\"Train: loss={train_loss:.4f}, acc={train_acc:.3f}\")\n",
    "\n",
    "    val_loss, val_acc = run_epoch(val_loader, train=False)\n",
    "    print(f\"Val:   loss={val_loss:.4f}, acc={val_acc:.3f}\")\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), \"tiny_resnet50_best.pt\")\n",
    "        print(f\"  ✅ New best model saved (acc={best_val_acc:.3f})\")\n",
    "\n",
    "print(\"\\nTraining finished!\")\n",
    "print(\"Best validation accuracy:\", round(best_val_acc, 3))\n",
    "print(\"Elapsed time: %.1f sec\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd260f35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ele670)",
   "language": "python",
   "name": "ele670"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
