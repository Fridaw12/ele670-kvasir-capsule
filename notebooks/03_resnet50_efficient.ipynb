{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87334838",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a7c8d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Host: gorina6\n",
      "CUDA: True\n",
      "Fri Oct  3 14:24:36 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.144.03             Driver Version: 550.144.03     CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n"
     ]
    }
   ],
   "source": [
    "import platform, torch, subprocess\n",
    "print(\"Host:\", platform.node())               # gorina6\n",
    "print(\"CUDA:\", torch.cuda.is_available())     # True\n",
    "print(subprocess.getoutput(\"nvidia-smi | head -n 5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf0e1811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Host: gorina6\n",
      "Torch: 2.7.1+cu118\n",
      "CUDA (build): 11.8\n",
      "CUDA available: True\n",
      "cuDNN: 90100\n",
      "GPU count: 8\n",
      "0 Tesla V100-PCIE-32GB\n",
      "1 Tesla V100-PCIE-32GB\n",
      "2 Tesla V100-PCIE-32GB\n",
      "3 Tesla V100-PCIE-32GB\n",
      "4 Tesla V100-PCIE-32GB\n",
      "5 Tesla V100-PCIE-32GB\n",
      "6 Tesla V100-PCIE-32GB\n",
      "7 Tesla V100-PCIE-32GB\n"
     ]
    }
   ],
   "source": [
    "import torch, platform\n",
    "print(\"Host:\", platform.node())\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"CUDA (build):\", torch.version.cuda)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"cuDNN:\", torch.backends.cudnn.version())\n",
    "print(\"GPU count:\", torch.cuda.device_count())\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(i, torch.cuda.get_device_name(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "627da106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU mem free/total: 15.83 / 34.07 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "free, total = torch.cuda.mem_get_info()\n",
    "print(f\"GPU mem free/total: {free/1e9:.2f} / {total/1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c696d380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla V100-PCIE-32GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bfe05eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA_VISIBLE_DEVICES = None\n",
      "Visible count = 8\n",
      "Using: Tesla V100-PCIE-32GB\n"
     ]
    }
   ],
   "source": [
    "import os, torch\n",
    "print(\"CUDA_VISIBLE_DEVICES =\", os.environ.get(\"CUDA_VISIBLE_DEVICES\"))\n",
    "print(\"Visible count =\", torch.cuda.device_count())\n",
    "print(\"Using:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b723e309",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11289a7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.device_count()  # should be 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "580e99d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing: 786 / 47248\n",
      "                       filename          video_id  frame_number  \\\n",
      "6865  eb0203196e284797_1157.jpg  eb0203196e284797          1157   \n",
      "6866  eb0203196e284797_1158.jpg  eb0203196e284797          1158   \n",
      "6867  eb0203196e284797_1160.jpg  eb0203196e284797          1160   \n",
      "6868  eb0203196e284797_1167.jpg  eb0203196e284797          1167   \n",
      "6869  eb0203196e284797_1168.jpg  eb0203196e284797          1168   \n",
      "\n",
      "     finding_category     finding_class     x1     y1     x2     y2     x3  \\\n",
      "6865          Anatomy  Ampulla of Vater  238.0  196.0  327.0  196.0  327.0   \n",
      "6866          Anatomy  Ampulla of Vater  138.0    0.0  251.0    0.0  251.0   \n",
      "6867          Anatomy  Ampulla of Vater   69.0    0.0  153.0    0.0  153.0   \n",
      "6868          Anatomy  Ampulla of Vater    4.0  115.0   56.0  115.0   56.0   \n",
      "6869          Anatomy  Ampulla of Vater   57.0  182.0  137.0  182.0  137.0   \n",
      "\n",
      "         y3     x4     y4                                  image_path  \n",
      "6865  289.0  238.0  289.0  Ampulla of Vater/eb0203196e284797_1157.jpg  \n",
      "6866   66.0  138.0   66.0  Ampulla of Vater/eb0203196e284797_1158.jpg  \n",
      "6867   45.0   69.0   45.0  Ampulla of Vater/eb0203196e284797_1160.jpg  \n",
      "6868  190.0    4.0  190.0  Ampulla of Vater/eb0203196e284797_1167.jpg  \n",
      "6869  258.0   57.0  258.0  Ampulla of Vater/eb0203196e284797_1168.jpg  \n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "metadata_csv = Path(\"/home/stud/fwag/bhome/ele670_project/data/raw/metadata.csv\")\n",
    "dataset_root = Path(\"/home/stud/fwag/bhome/ele670_project/data/raw/labelled_images\")\n",
    "\n",
    "df = pd.read_csv(metadata_csv, sep=\";\")\n",
    "df[\"finding_class\"] = df[\"finding_class\"].astype(str).str.strip()\n",
    "df[\"filename\"] = df[\"filename\"].astype(str).str.strip()\n",
    "# Build relative image_path that matches your folder layout\n",
    "# labelled_images/<finding_class>/<filename>\n",
    "df[\"image_path\"] = df.apply(\n",
    "    lambda r: str(Path(r[\"finding_class\"]) / r[\"filename\"]), axis=1\n",
    ")\n",
    "\n",
    "bad = df[~df[\"image_path\"].apply(lambda p: (dataset_root / p).exists())]\n",
    "print(f\"Missing: {len(bad)} / {len(df)}\")\n",
    "print(bad.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18a1398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Class names with no folder match (case-insensitive): 0\n",
      "[]\n",
      "[CHECK] Missing after folder mapping: 0 / 47248\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>video_id</th>\n",
       "      <th>frame_number</th>\n",
       "      <th>finding_category</th>\n",
       "      <th>finding_class</th>\n",
       "      <th>x1</th>\n",
       "      <th>y1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y2</th>\n",
       "      <th>x3</th>\n",
       "      <th>y3</th>\n",
       "      <th>x4</th>\n",
       "      <th>y4</th>\n",
       "      <th>image_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [filename, video_id, frame_number, finding_category, finding_class, x1, y1, x2, y2, x3, y3, x4, y4, image_path]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SAVED] Processed DataFrame with image_path â†’ /home/stud/fwag/bhome/ele670_project/data/processed/metadata_cleaned.pkl\n"
     ]
    }
   ],
   "source": [
    "# CELL 1: SCAN + NORMALIZE + SAVE PROCESSED DF\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- config / paths ---\n",
    "dataset_root = Path(\"/home/stud/fwag/bhome/ele670_project/data/raw/labelled_images\")\n",
    "processed_df_pkl = Path(\"/home/stud/fwag/bhome/ele670_project/data/processed/metadata_cleaned.pkl\")\n",
    "metadata_csv = Path(\"/home/stud/fwag/bhome/ele670_project/data/raw/metadata.csv\")\n",
    "\n",
    "df = pd.read_csv(metadata_csv, sep=\";\")\n",
    "n_videos = df[\"video_id\"].nunique()\n",
    "print(f\"Total distinct videos: {n_videos}\")\n",
    "\n",
    "# --- preconditions ---\n",
    "if \"df\" not in globals():\n",
    "    raise NameError(\"df is not defined. Load your original CSV/DataFrame into `df` before running this cell.\")\n",
    "required_cols = {\"finding_class\", \"filename\"}\n",
    "missing = required_cols - set(df.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"`df` is missing required columns: {sorted(missing)}\")\n",
    "\n",
    "# --- build case-insensitive map of class folders on disk (SLOW) ---\n",
    "classes_on_disk = {p.name.lower().strip(): p.name for p in dataset_root.iterdir() if p.is_dir()}\n",
    "\n",
    "# --- check CSV classes vs folders ---\n",
    "csv_classes = sorted(df[\"finding_class\"].astype(str).str.strip().unique())\n",
    "no_match = [c for c in csv_classes if c.lower().strip() not in classes_on_disk]\n",
    "print(f\"[INFO] Class names with no folder match (case-insensitive): {len(no_match)}\")\n",
    "print(no_match[:20])\n",
    "\n",
    "# --- optional manual fixes for known variants ---\n",
    "manual_map = {\n",
    "    # \"CSV value\": \"Exact folder name on disk\"\n",
    "    # e.g., \"Reduced Mucosal View\": \"Reduced mucosal view\",\n",
    "}\n",
    "\n",
    "def resolve_class_folder(name: str) -> str:\n",
    "    name = str(name).strip()\n",
    "    if name in manual_map:\n",
    "        return manual_map[name]\n",
    "    return classes_on_disk.get(name.lower().strip(), name)  # fallback to original (will be flagged if missing)\n",
    "\n",
    "# --- rebuild image_path using resolved folder name ---\n",
    "df[\"image_path\"] = df.apply(\n",
    "    lambda r: str(Path(resolve_class_folder(r[\"finding_class\"])) / r[\"filename\"]),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# --- validate paths on disk ---\n",
    "bad = df[~df[\"image_path\"].apply(lambda p: (dataset_root / p).exists())]\n",
    "print(f\"[CHECK] Missing after folder mapping: {len(bad)} / {len(df)}\")\n",
    "display(bad.head(10))\n",
    "\n",
    "# --- persist the processed df so Cell 2 can run without rescanning ---\n",
    "processed_df_pkl.parent.mkdir(parents=True, exist_ok=True)\n",
    "df.to_pickle(processed_df_pkl)\n",
    "print(f\"[SAVED] Processed DataFrame with image_path â†’ {processed_df_pkl}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8954c230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SAVED] Normalized metadata CSV â†’ /home/stud/fwag/bhome/ele670_project/data/processed/metadata_cleaned.csv\n",
      "[SAVED] Label classes (.npy)     â†’ /home/stud/fwag/bhome/ele670_project/data/processed/label_classes_cleaned.npy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "      <th>finding_class</th>\n",
       "      <th>video_id</th>\n",
       "      <th>encoded_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Normal clean mucosa/0728084c8da942d9_22803.jpg</td>\n",
       "      <td>Normal clean mucosa</td>\n",
       "      <td>0728084c8da942d9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Normal clean mucosa/0728084c8da942d9_22804.jpg</td>\n",
       "      <td>Normal clean mucosa</td>\n",
       "      <td>0728084c8da942d9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Normal clean mucosa/0728084c8da942d9_22805.jpg</td>\n",
       "      <td>Normal clean mucosa</td>\n",
       "      <td>0728084c8da942d9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Normal clean mucosa/0728084c8da942d9_22806.jpg</td>\n",
       "      <td>Normal clean mucosa</td>\n",
       "      <td>0728084c8da942d9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Normal clean mucosa/0728084c8da942d9_22807.jpg</td>\n",
       "      <td>Normal clean mucosa</td>\n",
       "      <td>0728084c8da942d9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       image_path        finding_class  \\\n",
       "0  Normal clean mucosa/0728084c8da942d9_22803.jpg  Normal clean mucosa   \n",
       "1  Normal clean mucosa/0728084c8da942d9_22804.jpg  Normal clean mucosa   \n",
       "2  Normal clean mucosa/0728084c8da942d9_22805.jpg  Normal clean mucosa   \n",
       "3  Normal clean mucosa/0728084c8da942d9_22806.jpg  Normal clean mucosa   \n",
       "4  Normal clean mucosa/0728084c8da942d9_22807.jpg  Normal clean mucosa   \n",
       "\n",
       "           video_id  encoded_label  \n",
       "0  0728084c8da942d9              9  \n",
       "1  0728084c8da942d9              9  \n",
       "2  0728084c8da942d9              9  \n",
       "3  0728084c8da942d9              9  \n",
       "4  0728084c8da942d9              9  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CELL 2: EXPORT METADATA + LABELS (NO RESCAN)\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "processed_df_pkl = Path(\"/home/stud/fwag/bhome/ele670_project/data/processed/metadata_cleaned.pkl\")\n",
    "out_csv          = Path(\"/home/stud/fwag/bhome/ele670_project/data/processed/metadata_cleaned.csv\")\n",
    "out_classes      = Path(\"/home/stud/fwag/bhome/ele670_project/data/processed/label_classes_cleaned.npy\")\n",
    "\n",
    "# Prefer loading the persisted df (so we don't re-run the slow scan)\n",
    "if processed_df_pkl.exists():\n",
    "    df_proc = pd.read_pickle(processed_df_pkl)\n",
    "elif \"df\" in globals():\n",
    "    # Fallback: use in-memory df if it already has image_path (e.g., you just ran Cell 1)\n",
    "    if \"image_path\" not in df.columns:\n",
    "        raise ValueError(\"In-memory `df` has no 'image_path'. Run Cell 1 or load the processed df.\")\n",
    "    df_proc = df\n",
    "else:\n",
    "    raise FileNotFoundError(\n",
    "        f\"Processed df not found at {processed_df_pkl}. Run Cell 1 first to generate it.\"\n",
    "    )\n",
    "\n",
    "# Keep only columns needed for training\n",
    "keep_cols = [\"image_path\", \"finding_class\", \"video_id\"]\n",
    "missing_keep = [c for c in keep_cols if c not in df_proc.columns]\n",
    "if missing_keep:\n",
    "    raise ValueError(f\"Processed df missing required columns {missing_keep}. Ensure Cell 1 ran on the correct input.\")\n",
    "\n",
    "df_out = df_proc[keep_cols].copy()\n",
    "\n",
    "# Encode labels and persist artifacts\n",
    "lb = LabelEncoder()\n",
    "df_out[\"encoded_label\"] = lb.fit_transform(df_out[\"finding_class\"])\n",
    "np.save(\"/home/stud/fwag/bhome/ele670_project/data/processed/label_classes.npy\", lb.classes_)\n",
    "\n",
    "out_csv.parent.mkdir(parents=True, exist_ok=True)\n",
    "df_out.to_csv(out_csv, index=False)\n",
    "np.save(out_classes, lb.classes_)\n",
    "\n",
    "print(f\"[SAVED] Normalized metadata CSV â†’ {out_csv}\")\n",
    "print(f\"[SAVED] Label classes (.npy)     â†’ {out_classes}\")\n",
    "display(df_out.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "36d0fb07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STATS] Distinct videos and frame counts per class (before filtering):\n",
      "       finding_class  frames  videos\n",
      "    Ampulla of Vater      10       1\n",
      "     Blood - hematin      12       1\n",
      "               Polyp      55       1\n",
      "       Blood - fresh     446       2\n",
      "            Erythema     159       3\n",
      "    Lymphangiectasia     592       3\n",
      "               Ulcer     854       3\n",
      "        Foreign Body     776       4\n",
      "         Angiectasia     866       6\n",
      "Reduced Mucosal View    2906       7\n",
      "             Erosion     507       9\n",
      "             Pylorus    1538      32\n",
      "     Ileocecal valve    4189      34\n",
      " Normal clean mucosa   34338      37\n",
      "\n",
      "[INFO] How many classes each video has (top 10):\n",
      "video_id\n",
      "04a78ef00c5245e0    6\n",
      "48579eec79784294    6\n",
      "64440803f87b4843    6\n",
      "8ebf0e483cac48d6    6\n",
      "8885668afb844852    5\n",
      "5e59c7fdb16c4228    5\n",
      "eb0203196e284797    5\n",
      "3ada4222967f421d    4\n",
      "131368cc17e44240    4\n",
      "7a47e8eacea04e64    4\n",
      "\n",
      "[INFO] Distribution of 'number of classes per video':\n",
      "Classes_per_video | Num_videos\n",
      " Classes_per_video  Num_videos\n",
      "                 1           2\n",
      "                 2           8\n",
      "                 3          18\n",
      "                 4           8\n",
      "                 5           3\n",
      "                 6           4\n",
      "\n",
      "[INFO] Videos containing only ONE class: 2 found\n",
      "[INFO] Example single-class videos (up to 10 shown):\n",
      "['2fc3db471f9d44c0', '4aebc5cb2d4847aa']\n",
      "[SAVED] Full list of single-class videos â†’ /home/stud/fwag/bhome/ele670_project/data/processed/single_class_videos.csv\n",
      "\n",
      "[INFO] Classes found in video 04a78ef00c5245e0:\n",
      "['Angiectasia' 'Blood - fresh' 'Ileocecal valve' 'Lymphangiectasia'\n",
      " 'Normal clean mucosa' 'Pylorus']\n",
      "\n",
      "[CONFIG] min_videos_per_class = 2, min_frames_per_class = 20\n",
      "[FILTER] Too few videos (<2): 3 classes\n",
      "['Ampulla of Vater', 'Blood - hematin', 'Polyp']\n",
      "[FILTER] Too few frames (<20): 2 classes\n",
      "['Ampulla of Vater', 'Blood - hematin']\n",
      "[FILTER] Total unique classes removed: 3\n",
      "\n",
      "[SUMMARY] Frames before: 47,248 | after: 47,171\n",
      "[SUMMARY] Classes before: 14 | after: 11\n",
      "\n",
      "[SAVED] Filtered metadata CSV  â†’ /home/stud/fwag/bhome/ele670_project/data/processed/metadata_filtered.csv\n",
      "[SAVED] Filtered label classes â†’ /home/stud/fwag/bhome/ele670_project/data/processed/label_classes_filtered.npy\n",
      "[SAVED] Per-video class counts â†’ /home/stud/fwag/bhome/ele670_project/data/processed/per_video_class_counts.csv\n",
      "[SAVED] Per-video class-count distribution â†’ /home/stud/fwag/bhome/ele670_project/data/processed/per_video_class_count_distribution.csv\n",
      "\n",
      "[HEAD]\n",
      "                                    image_path       finding_class         video_id  encoded_label\n",
      "Normal clean mucosa/0728084c8da942d9_22803.jpg Normal clean mucosa 0728084c8da942d9              7\n",
      "Normal clean mucosa/0728084c8da942d9_22804.jpg Normal clean mucosa 0728084c8da942d9              7\n",
      "Normal clean mucosa/0728084c8da942d9_22805.jpg Normal clean mucosa 0728084c8da942d9              7\n",
      "Normal clean mucosa/0728084c8da942d9_22806.jpg Normal clean mucosa 0728084c8da942d9              7\n",
      "Normal clean mucosa/0728084c8da942d9_22807.jpg Normal clean mucosa 0728084c8da942d9              7\n"
     ]
    }
   ],
   "source": [
    "# CELL 3: FILTER SMALL / SINGLE-VIDEO CLASSES + PRINT STATS + EXPORT + PER-VIDEO INSIGHTS\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# --- config / thresholds ---\n",
    "min_videos_per_class = 2     # threshold 1: a class must appear in at least this many DISTINCT videos\n",
    "min_frames_per_class = 20    # threshold 2: a class must have at least this many frames overall\n",
    "\n",
    "# --- standardized file paths ---\n",
    "processed_df_pkl = Path(\"/home/stud/fwag/bhome/ele670_project/data/processed/metadata_cleaned.pkl\")  # cleaned df from Cell 1\n",
    "out_csv_filtered  = Path(\"/home/stud/fwag/bhome/ele670_project/data/processed/metadata_filtered.csv\")  # final filtered metadata\n",
    "out_classes       = Path(\"/home/stud/fwag/bhome/ele670_project/data/processed/label_classes_filtered.npy\")  # label names after filtering\n",
    "out_stats_csv     = Path(\"/home/stud/fwag/bhome/ele670_project/data/processed/class_stats_before_filter.csv\")  # class-level stats BEFORE filtering\n",
    "out_per_video_csv = Path(\"/home/stud/fwag/bhome/ele670_project/data/processed/per_video_class_counts.csv\")  # n_classes per video (pre-filter)\n",
    "out_per_video_dist_csv = Path(\"/home/stud/fwag/bhome/ele670_project/data/processed/per_video_class_count_distribution.csv\")  # distribution over n_classes per video (pre-filter)\n",
    "out_single_class_videos = Path(\"/home/stud/fwag/bhome/ele670_project/data/processed/single_class_videos.csv\")  # list of videos that contain exactly one class (pre-filter)\n",
    "\n",
    "# --- load cleaned DataFrame (output of Cell 1) ---\n",
    "if processed_df_pkl.exists():\n",
    "    # preferred path: load the persisted, cleaned dataframe so we don't rescan the filesystem\n",
    "    df_proc = pd.read_pickle(processed_df_pkl)\n",
    "elif \"df\" in globals():\n",
    "    # fallback: use in-memory df only if it already has 'image_path' (implies Cell 1 ran in this session)\n",
    "    if \"image_path\" not in df.columns:\n",
    "        raise ValueError(\"In-memory `df` has no 'image_path'. Run Cell 1 or load the cleaned df.\")\n",
    "    df_proc = df\n",
    "else:\n",
    "    # neither persisted nor in-memory cleaned df exists â†’ instruct user to run Cell 1\n",
    "    raise FileNotFoundError(f\"Cleaned df not found at {processed_df_pkl}. Run Cell 1 first.\")\n",
    "\n",
    "# --- sanity check ---\n",
    "for c in [\"finding_class\", \"video_id\", \"image_path\"]:\n",
    "    # ensure the key columns exist; otherwise the following stats/filters would fail\n",
    "    if c not in df_proc.columns:\n",
    "        raise ValueError(f\"Column '{c}' missing in df_proc â€” ensure Cell 1 ran correctly.\")\n",
    "\n",
    "# ===================================================\n",
    "# 1) PER-CLASS STATS\n",
    "# ===================================================\n",
    "# Build per-class aggregates:\n",
    "# - frames: total number of rows/images belonging to the class\n",
    "# - videos: number of DISTINCT video_id values where the class appears\n",
    "stats = (\n",
    "    df_proc.groupby(\"finding_class\")\n",
    "           .agg(frames=(\"image_path\", \"size\"),\n",
    "                videos=(\"video_id\", lambda s: s.nunique(dropna=True)))  # nunique over video_id counts distinct videos with that class\n",
    "           .reset_index()\n",
    "           .sort_values([\"videos\", \"frames\", \"finding_class\"])\n",
    ")\n",
    "\n",
    "# Persist these pre-filter stats for auditing/repro\n",
    "out_stats_csv.parent.mkdir(parents=True, exist_ok=True)\n",
    "stats.to_csv(out_stats_csv, index=False)\n",
    "\n",
    "print(\"[STATS] Distinct videos and frame counts per class (before filtering):\")\n",
    "print(stats.to_string(index=False, max_rows=40))  # print a readable table (truncate beyond 40 rows)\n",
    "\n",
    "# ===================================================\n",
    "# 2) PER-VIDEO CLASS DIVERSITY\n",
    "# ===================================================\n",
    "# For each video_id, count how many DISTINCT classes appear in that video\n",
    "per_video_classes = (\n",
    "    df_proc.groupby(\"video_id\")[\"finding_class\"]\n",
    "           .nunique(dropna=True)\n",
    "           .sort_values(ascending=False)\n",
    ")\n",
    "\n",
    "# Save per-video class counts and the distribution (how many videos have 1,2,3,... classes)\n",
    "per_video_classes.to_csv(out_per_video_csv, header=[\"n_classes\"])\n",
    "\n",
    "per_video_dist = per_video_classes.value_counts().sort_index()\n",
    "per_video_dist.to_csv(out_per_video_dist_csv, header=[\"n_videos\"])\n",
    "\n",
    "print(\"\\n[INFO] How many classes each video has (top 10):\")\n",
    "print(per_video_classes.head(10).to_string())  # show the 'busiest' videos by class diversity\n",
    "\n",
    "print(\"\\n[INFO] Distribution of 'number of classes per video':\")\n",
    "print(\"Classes_per_video | Num_videos\")\n",
    "print(per_video_dist.rename_axis(\"Classes_per_video\")\n",
    "                     .reset_index(name=\"Num_videos\")\n",
    "                     .to_string(index=False))\n",
    "\n",
    "# ===================================================\n",
    "# 3) NEW: IDENTIFY SINGLE-CLASS VIDEOS\n",
    "# ===================================================\n",
    "# Find videos where only ONE class appears (diagnostic only; not filtering them out here)\n",
    "single_class_videos = per_video_classes[per_video_classes == 1].index.tolist()\n",
    "print(f\"\\n[INFO] Videos containing only ONE class: {len(single_class_videos)} found\")\n",
    "if single_class_videos:\n",
    "    print(\"[INFO] Example single-class videos (up to 10 shown):\")\n",
    "    print(single_class_videos[:10])\n",
    "\n",
    "    # For those videos, also record WHICH class they contain (array per video)\n",
    "    single_class_info = (\n",
    "        df_proc[df_proc[\"video_id\"].isin(single_class_videos)]\n",
    "        .groupby(\"video_id\")[\"finding_class\"]\n",
    "        .unique()\n",
    "        .reset_index()\n",
    "    )\n",
    "    single_class_info.to_csv(out_single_class_videos, index=False)\n",
    "    print(f\"[SAVED] Full list of single-class videos â†’ {out_single_class_videos}\")\n",
    "\n",
    "# Pick one example video (the one with most classes) and list its classes (sanity check)\n",
    "example_video = per_video_classes.index[0]  # video with most distinct classes\n",
    "classes_in_example = (\n",
    "    df_proc.loc[df_proc[\"video_id\"] == example_video, \"finding_class\"]\n",
    "          .dropna().unique()\n",
    ")\n",
    "print(f\"\\n[INFO] Classes found in video {example_video}:\")\n",
    "print(np.sort(classes_in_example))\n",
    "\n",
    "# ===================================================\n",
    "# 4) FILTERING BY CLASS SIZE / VIDEO COUNT\n",
    "# ===================================================\n",
    "# Identify classes that FAIL either threshold:\n",
    "# - 'removed_by_videos': classes that appear in fewer than min_videos_per_class distinct videos\n",
    "# - 'removed_by_frames': classes that have fewer than min_frames_per_class total frames\n",
    "removed_by_videos = stats.loc[stats[\"videos\"] < min_videos_per_class, \"finding_class\"]\n",
    "removed_by_frames = stats.loc[stats[\"frames\"] < min_frames_per_class, \"finding_class\"]\n",
    "\n",
    "# Union of both failure sets â†’ final set of classes to remove\n",
    "to_remove = set(removed_by_videos) | set(removed_by_frames)\n",
    "\n",
    "# Log what will be removed and why\n",
    "print(f\"\\n[CONFIG] min_videos_per_class = {min_videos_per_class}, min_frames_per_class = {min_frames_per_class}\")\n",
    "print(f\"[FILTER] Too few videos (<{min_videos_per_class}): {len(removed_by_videos)} classes\")\n",
    "if len(removed_by_videos): print(sorted(removed_by_videos.tolist())[:40])\n",
    "print(f\"[FILTER] Too few frames (<{min_frames_per_class}): {len(removed_by_frames)} classes\")\n",
    "if len(removed_by_frames): print(sorted(removed_by_frames.tolist())[:40])\n",
    "print(f\"[FILTER] Total unique classes removed: {len(to_remove)}\")\n",
    "\n",
    "# Apply class-level filter: DROP ALL ROWS whose 'finding_class' is in 'to_remove'\n",
    "# (i.e., remove every frame belonging to low-support classes)\n",
    "df_filt = df_proc[~df_proc[\"finding_class\"].isin(to_remove)].copy()\n",
    "\n",
    "# Summaries pre vs post\n",
    "print(f\"\\n[SUMMARY] Frames before: {len(df_proc):,} | after: {len(df_filt):,}\")\n",
    "print(f\"[SUMMARY] Classes before: {stats.shape[0]} | after: {df_filt['finding_class'].nunique()}\")\n",
    "\n",
    "# ===================================================\n",
    "# 5) ENCODE + SAVE FILTERED DATA\n",
    "# ===================================================\n",
    "# Keep just the columns needed downstream and add an integer label encoding for 'finding_class'\n",
    "df_out = df_filt[[\"image_path\", \"finding_class\", \"video_id\"]].copy()\n",
    "lb = LabelEncoder()\n",
    "df_out[\"encoded_label\"] = lb.fit_transform(df_out[\"finding_class\"])\n",
    "\n",
    "# Save filtered metadata (CSV) and the label names (NPY)\n",
    "out_csv_filtered.parent.mkdir(parents=True, exist_ok=True)\n",
    "df_out.to_csv(out_csv_filtered, index=False)\n",
    "np.save(out_classes, lb.classes_)\n",
    "\n",
    "print(f\"\\n[SAVED] Filtered metadata CSV  â†’ {out_csv_filtered}\")\n",
    "print(f\"[SAVED] Filtered label classes â†’ {out_classes}\")\n",
    "print(\"[SAVED] Per-video class counts â†’\", out_per_video_csv)\n",
    "print(\"[SAVED] Per-video class-count distribution â†’\", out_per_video_dist_csv)\n",
    "print(\"\\n[HEAD]\")\n",
    "print(df_out.head().to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22296fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loaded 47248 rows from /bhome/fwag/ele670_project/data/processed/metadata_normalized.csv\n",
      "[INFO] Columns: ['image_path', 'finding_class', 'video_id', 'encoded_label']\n",
      "                                       image_path        finding_class  \\\n",
      "0  Normal clean mucosa/0728084c8da942d9_22803.jpg  Normal clean mucosa   \n",
      "1  Normal clean mucosa/0728084c8da942d9_22804.jpg  Normal clean mucosa   \n",
      "2  Normal clean mucosa/0728084c8da942d9_22805.jpg  Normal clean mucosa   \n",
      "3  Normal clean mucosa/0728084c8da942d9_22806.jpg  Normal clean mucosa   \n",
      "4  Normal clean mucosa/0728084c8da942d9_22807.jpg  Normal clean mucosa   \n",
      "\n",
      "           video_id  encoded_label  \n",
      "0  0728084c8da942d9              9  \n",
      "1  0728084c8da942d9              9  \n",
      "2  0728084c8da942d9              9  \n",
      "3  0728084c8da942d9              9  \n",
      "4  0728084c8da942d9              9  \n",
      "\n",
      "[INFO] Number of DISTINCT videos per class:\n",
      "finding_class\n",
      "Normal clean mucosa     37\n",
      "Ileocecal valve         34\n",
      "Pylorus                 32\n",
      "Erosion                  9\n",
      "Reduced Mucosal View     7\n",
      "Angiectasia              6\n",
      "Foreign Body             4\n",
      "Lymphangiectasia         3\n",
      "Ulcer                    3\n",
      "Erythema                 3\n",
      "Blood - fresh            2\n",
      "Ampulla of Vater         1\n",
      "Blood - hematin          1\n",
      "Polyp                    1\n",
      "Name: video_id, dtype: int64\n",
      "\n",
      "[INFO] Per-class stats (frames, distinct videos, single-video flag):\n",
      "                      n_frames  n_videos  only_one_video\n",
      "finding_class                                           \n",
      "Normal clean mucosa      34338        37           False\n",
      "Ileocecal valve           4189        34           False\n",
      "Pylorus                   1538        32           False\n",
      "Erosion                    507         9           False\n",
      "Reduced Mucosal View      2906         7           False\n",
      "Angiectasia                866         6           False\n",
      "Foreign Body               776         4           False\n",
      "Ulcer                      854         3           False\n",
      "Lymphangiectasia           592         3           False\n",
      "Erythema                   159         3           False\n",
      "Blood - fresh              446         2           False\n",
      "Polyp                       55         1            True\n",
      "Blood - hematin             12         1            True\n",
      "Ampulla of Vater            10         1            True\n",
      "\n",
      "[INFO] Classes with frames coming from ONLY ONE video:\n",
      "                  n_frames  n_videos  only_one_video\n",
      "finding_class                                       \n",
      "Polyp                   55         1            True\n",
      "Blood - hematin         12         1            True\n",
      "Ampulla of Vater        10         1            True\n",
      "\n",
      "[INFO] Distinct video_ids for ALL classes:\n",
      "- Normal clean mucosa: 37 video(s) â†’ ['04a78ef00c5245e0', '0531325b64674948', '0728084c8da942d9', '07c1fa15a20a4398', '131368cc17e44240', '39960e5e099a45ca', '3ada4222967f421d', '3c8d5f0b90d7475d', '4560e83f9afc4685', '48579eec79784294', '495f16498db34d3c', '5bb1d3cc7dc64cec', '5e59c7fdb16c4228', '5e9beaf4e66142c8', '64440803f87b4843', '6cb700585c4f4070', '7ad22d50ebaf4596', '8885668afb844852', '89cdd41258c542c5', '8a00709108cd4e2b', '8ebf0e483cac48d6', 'b2134f4a6f864613', 'bc84479c66fe4da6', 'bca26705313a4644', 'c11b28a8b2344716', 'c7084b3556e34619', 'd369e4f163df4aba', 'd7a271f233ba4a40', 'dac1e27f7e4d4ef5', 'dc221ccc65d34010', 'dca1377dec974312', 'df6b47bafe5143f5', 'eb0203196e284797', 'ed02f27ef36f483e', 'fb86bc87d3874cd7', 'fc32def0e7194981', 'fe5d372e43f94f68']\n",
      "- Ileocecal valve: 34 video(s) â†’ ['04a78ef00c5245e0', '0728084c8da942d9', '07c1fa15a20a4398', '131368cc17e44240', '39960e5e099a45ca', '3ada4222967f421d', '3c8d5f0b90d7475d', '4560e83f9afc4685', '48579eec79784294', '495f16498db34d3c', '4aebc5cb2d4847aa', '5bb1d3cc7dc64cec', '5e59c7fdb16c4228', '5e9beaf4e66142c8', '64440803f87b4843', '6cb700585c4f4070', '7a47e8eacea04e64', '7ad22d50ebaf4596', '8885668afb844852', '8a00709108cd4e2b', '8ebf0e483cac48d6', 'ad91cf7ca91440aa', 'af9bd7d0e43741e3', 'bc84479c66fe4da6', 'bca26705313a4644', 'c7084b3556e34619', 'd7a271f233ba4a40', 'dac1e27f7e4d4ef5', 'dc221ccc65d34010', 'dca1377dec974312', 'df6b47bafe5143f5', 'ed02f27ef36f483e', 'fc32def0e7194981', 'fe5d372e43f94f68']\n",
      "- Pylorus: 32 video(s) â†’ ['04a78ef00c5245e0', '0531325b64674948', '0728084c8da942d9', '131368cc17e44240', '39960e5e099a45ca', '3ada4222967f421d', '4560e83f9afc4685', '48579eec79784294', '5e59c7fdb16c4228', '64440803f87b4843', '7a47e8eacea04e64', '7ad22d50ebaf4596', '8885668afb844852', '89cdd41258c542c5', '8ebf0e483cac48d6', 'ad91cf7ca91440aa', 'af9bd7d0e43741e3', 'b2134f4a6f864613', 'bc84479c66fe4da6', 'bca26705313a4644', 'c11b28a8b2344716', 'c7084b3556e34619', 'd369e4f163df4aba', 'd626f4f4a5ac4785', 'd7a271f233ba4a40', 'dac1e27f7e4d4ef5', 'dca1377dec974312', 'df6b47bafe5143f5', 'eb0203196e284797', 'ed02f27ef36f483e', 'fb86bc87d3874cd7', 'fe5d372e43f94f68']\n",
      "- Erosion: 9 video(s) â†’ ['0531325b64674948', '0728084c8da942d9', '5e59c7fdb16c4228', '8ebf0e483cac48d6', 'bca26705313a4644', 'd626f4f4a5ac4785', 'dac1e27f7e4d4ef5', 'eb0203196e284797', 'fb86bc87d3874cd7']\n",
      "- Reduced Mucosal View: 7 video(s) â†’ ['48579eec79784294', '5bb1d3cc7dc64cec', '8885668afb844852', '8ebf0e483cac48d6', 'af9bd7d0e43741e3', 'c11b28a8b2344716', 'dc221ccc65d34010']\n",
      "- Angiectasia: 6 video(s) â†’ ['04a78ef00c5245e0', '64440803f87b4843', 'ad91cf7ca91440aa', 'b2134f4a6f864613', 'd369e4f163df4aba', 'eb0203196e284797']\n",
      "- Foreign Body: 4 video(s) â†’ ['3ada4222967f421d', '48579eec79784294', '7a47e8eacea04e64', '8885668afb844852']\n",
      "- Ulcer: 3 video(s) â†’ ['2fc3db471f9d44c0', '7a47e8eacea04e64', 'd626f4f4a5ac4785']\n",
      "- Lymphangiectasia: 3 video(s) â†’ ['04a78ef00c5245e0', '64440803f87b4843', '7ad22d50ebaf4596']\n",
      "- Erythema: 3 video(s) â†’ ['48579eec79784294', '5e59c7fdb16c4228', '64440803f87b4843']\n",
      "- Blood - fresh: 2 video(s) â†’ ['04a78ef00c5245e0', 'd369e4f163df4aba']\n",
      "- Polyp: 1 video(s) â†’ ['131368cc17e44240']\n",
      "- Blood - hematin: 1 video(s) â†’ ['8ebf0e483cac48d6']\n",
      "- Ampulla of Vater: 1 video(s) â†’ ['eb0203196e284797']\n",
      "\n",
      "[INFO] Breakdown for single-video classes:\n",
      "\n",
      "Class: Polyp\n",
      "        video_id finding_class  n_frames\n",
      "131368cc17e44240         Polyp        55\n",
      "\n",
      "Class: Blood - hematin\n",
      "        video_id   finding_class  n_frames\n",
      "8ebf0e483cac48d6 Blood - hematin        12\n",
      "\n",
      "Class: Ampulla of Vater\n",
      "        video_id    finding_class  n_frames\n",
      "eb0203196e284797 Ampulla of Vater        10\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- Load the dataset ---\n",
    "csv_path = \"/bhome/fwag/ele670_project/data/processed/metadata_normalized.csv\"  # <-- change this to your file\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "print(f\"[INFO] Loaded {len(df)} rows from {csv_path}\")\n",
    "print(\"[INFO] Columns:\", list(df.columns))\n",
    "print(df.head())\n",
    "\n",
    "# --- Safety check ---\n",
    "required_cols = {\"video_id\", \"finding_class\"}\n",
    "missing = required_cols - set(df.columns)\n",
    "assert not missing, f\"DataFrame is missing required columns: {missing}\"\n",
    "\n",
    "# --- Per-class â†” videos stats ---\n",
    "class_to_n_videos = (\n",
    "    df.groupby(\"finding_class\")[\"video_id\"]\n",
    "      .nunique()\n",
    "      .sort_values(ascending=False)\n",
    ")\n",
    "\n",
    "print(\"\\n[INFO] Number of DISTINCT videos per class:\")\n",
    "print(class_to_n_videos)\n",
    "\n",
    "# Full stats table per class\n",
    "class_stats = (\n",
    "    df.groupby(\"finding_class\")\n",
    "      .agg(\n",
    "          n_frames=(\"finding_class\", \"size\"),\n",
    "          n_videos=(\"video_id\", \"nunique\"),\n",
    "      )\n",
    "      .sort_values([\"n_videos\", \"n_frames\"], ascending=[False, False])\n",
    ")\n",
    "class_stats[\"only_one_video\"] = class_stats[\"n_videos\"] == 1\n",
    "\n",
    "print(\"\\n[INFO] Per-class stats (frames, distinct videos, single-video flag):\")\n",
    "print(class_stats)\n",
    "\n",
    "# Which classes are only in a single video?\n",
    "single_video_classes = class_stats[class_stats[\"only_one_video\"]].copy()\n",
    "print(\"\\n[INFO] Classes with frames coming from ONLY ONE video:\")\n",
    "if single_video_classes.empty:\n",
    "    print(\"None ðŸŽ‰\")\n",
    "else:\n",
    "    print(single_video_classes)\n",
    "\n",
    "# Map each class -> list of distinct video_ids\n",
    "videos_list_per_class = (\n",
    "    df.groupby(\"finding_class\")[\"video_id\"]\n",
    "      .apply(lambda s: sorted(s.unique()))\n",
    "      .to_dict()\n",
    ")\n",
    "\n",
    "# ðŸ”¹ NEW: print ALL classes, not just top-10\n",
    "print(\"\\n[INFO] Distinct video_ids for ALL classes:\")\n",
    "for cls in class_stats.index:  # <-- removed .head(10)\n",
    "    vids = videos_list_per_class.get(cls, [])\n",
    "    print(f\"- {cls}: {len(vids)} video(s) â†’ {vids}\")\n",
    "\n",
    "# --- Optional: per (video_id, class) breakdown ---\n",
    "per_video_class_counts = (\n",
    "    df.groupby([\"video_id\", \"finding_class\"])\n",
    "      .size()\n",
    "      .rename(\"n_frames\")\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "if not single_video_classes.empty:\n",
    "    print(\"\\n[INFO] Breakdown for single-video classes:\")\n",
    "    for cls in single_video_classes.index:\n",
    "        rows = per_video_class_counts[per_video_class_counts[\"finding_class\"] == cls]\n",
    "        print(f\"\\nClass: {cls}\")\n",
    "        print(rows.sort_values(\"n_frames\", ascending=False).head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96f36011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA visible devices: 1\n",
      "Using GPU: Tesla V100-PCIE-32GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "\n",
    "import torch\n",
    "print(\"CUDA visible devices:\", torch.cuda.device_count())\n",
    "print(\"Using GPU:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9787aab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity check done âœ… | Batch size: 8, Loss: 3.3002\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# ==== CONFIG ====\n",
    "CSV_PATH = \"/bhome/fwag/ele670_project/data/processed/metadata_normalized.csv\"\n",
    "IMG_ROOT = \"/bhome/fwag/ele670_project/data/raw/labelled_images\"\n",
    "BATCH_SIZE = 8\n",
    "NUM_WORKERS = 4\n",
    "\n",
    "# ==== DATASET ====\n",
    "class KvasirDataset(Dataset):\n",
    "    def __init__(self, csv_path, img_root, transform=None):\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.img_root = img_root\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = os.path.join(self.img_root, row[\"image_path\"])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        label = int(row[\"encoded_label\"])\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "# Simple transforms for sanity check\n",
    "transform = T.Compose([\n",
    "    T.Resize((224,224)),\n",
    "    T.ToTensor()\n",
    "])\n",
    "\n",
    "dataset = KvasirDataset(CSV_PATH, IMG_ROOT, transform)\n",
    "loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "\n",
    "# ==== MODEL ====\n",
    "num_classes = dataset.df[\"encoded_label\"].nunique()\n",
    "model = models.resnet50(weights=None)   # no pretrained, just sanity check\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "\n",
    "# ==== SANITY TRAIN LOOP (1 batch only) ====\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "\n",
    "images, labels = next(iter(loader))\n",
    "images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# forward\n",
    "outputs = model(images)\n",
    "loss = criterion(outputs, labels)\n",
    "\n",
    "# backward\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "print(f\"Sanity check done âœ… | Batch size: {images.size(0)}, Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6402707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Tesla V100-PCIE-32GB\n",
      "Memory allocated: 0.38 GB\n",
      "First rows of metadata.csv:\n",
      "                                       image_path        finding_class  \\\n",
      "0  Normal clean mucosa/0728084c8da942d9_22803.jpg  Normal clean mucosa   \n",
      "1  Normal clean mucosa/0728084c8da942d9_22804.jpg  Normal clean mucosa   \n",
      "2  Normal clean mucosa/0728084c8da942d9_22805.jpg  Normal clean mucosa   \n",
      "3  Normal clean mucosa/0728084c8da942d9_22806.jpg  Normal clean mucosa   \n",
      "4  Normal clean mucosa/0728084c8da942d9_22807.jpg  Normal clean mucosa   \n",
      "\n",
      "           video_id  encoded_label  \n",
      "0  0728084c8da942d9              9  \n",
      "1  0728084c8da942d9              9  \n",
      "2  0728084c8da942d9              9  \n",
      "3  0728084c8da942d9              9  \n",
      "4  0728084c8da942d9              9  \n",
      "Number of classes: 14\n",
      "Train samples: 37430 | Val samples: 9818\n",
      "Train videos: 34 | Val videos: 9\n",
      "One training batch (images, labels):\n",
      "Image batch shape: torch.Size([32, 3, 224, 224])\n",
      "Label batch shape: torch.Size([32])\n",
      "\n",
      "==== START TRAINING ====\n",
      "\n",
      "\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3190418/2432252680.py:126: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n",
      "/tmp/ipykernel_3190418/2432252680.py:141: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=True):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 1/1170 | Loss: 2.7401 | Batch acc: 0.00\n",
      "  Step 10/1170 | Loss: 0.9073 | Batch acc: 0.75\n",
      "  Step 20/1170 | Loss: 0.5375 | Batch acc: 0.84\n",
      "  Step 30/1170 | Loss: 1.3675 | Batch acc: 0.69\n",
      "  Step 40/1170 | Loss: 1.3706 | Batch acc: 0.62\n",
      "  Step 50/1170 | Loss: 0.9294 | Batch acc: 0.75\n",
      "  Step 60/1170 | Loss: 0.6208 | Batch acc: 0.78\n",
      "  Step 70/1170 | Loss: 1.0341 | Batch acc: 0.59\n",
      "  Step 80/1170 | Loss: 0.5050 | Batch acc: 0.88\n",
      "  Step 90/1170 | Loss: 0.6382 | Batch acc: 0.84\n",
      "  Step 100/1170 | Loss: 0.4670 | Batch acc: 0.84\n",
      "  Step 110/1170 | Loss: 0.3199 | Batch acc: 0.94\n",
      "  Step 120/1170 | Loss: 0.6109 | Batch acc: 0.78\n",
      "  Step 130/1170 | Loss: 0.3193 | Batch acc: 0.94\n",
      "  Step 140/1170 | Loss: 0.5663 | Batch acc: 0.81\n",
      "  Step 150/1170 | Loss: 0.4441 | Batch acc: 0.84\n",
      "  Step 160/1170 | Loss: 0.6226 | Batch acc: 0.81\n",
      "  Step 170/1170 | Loss: 0.3706 | Batch acc: 0.94\n",
      "  Step 180/1170 | Loss: 0.4049 | Batch acc: 0.97\n",
      "  Step 190/1170 | Loss: 0.4931 | Batch acc: 0.84\n",
      "  Step 200/1170 | Loss: 0.1632 | Batch acc: 0.97\n",
      "  Step 210/1170 | Loss: 0.4126 | Batch acc: 0.78\n",
      "  Step 220/1170 | Loss: 0.6373 | Batch acc: 0.78\n",
      "  Step 230/1170 | Loss: 0.5492 | Batch acc: 0.84\n",
      "  Step 240/1170 | Loss: 0.5981 | Batch acc: 0.81\n",
      "  Step 250/1170 | Loss: 0.6370 | Batch acc: 0.84\n",
      "  Step 260/1170 | Loss: 0.6786 | Batch acc: 0.78\n",
      "  Step 270/1170 | Loss: 0.4745 | Batch acc: 0.88\n",
      "  Step 280/1170 | Loss: 0.4842 | Batch acc: 0.94\n",
      "  Step 290/1170 | Loss: 0.4380 | Batch acc: 0.91\n",
      "  Step 300/1170 | Loss: 0.1687 | Batch acc: 0.97\n",
      "  Step 310/1170 | Loss: 0.3805 | Batch acc: 0.91\n",
      "  Step 320/1170 | Loss: 0.2664 | Batch acc: 0.94\n",
      "  Step 330/1170 | Loss: 0.2141 | Batch acc: 0.94\n",
      "  Step 340/1170 | Loss: 0.2777 | Batch acc: 0.97\n",
      "  Step 350/1170 | Loss: 0.3091 | Batch acc: 0.88\n",
      "  Step 360/1170 | Loss: 0.2249 | Batch acc: 0.94\n",
      "  Step 370/1170 | Loss: 0.4003 | Batch acc: 0.84\n",
      "  Step 380/1170 | Loss: 0.4517 | Batch acc: 0.88\n",
      "  Step 390/1170 | Loss: 0.3130 | Batch acc: 0.91\n",
      "  Step 400/1170 | Loss: 0.8656 | Batch acc: 0.84\n",
      "  Step 410/1170 | Loss: 0.3821 | Batch acc: 0.91\n",
      "  Step 420/1170 | Loss: 0.3958 | Batch acc: 0.81\n",
      "  Step 430/1170 | Loss: 0.2929 | Batch acc: 0.84\n",
      "  Step 440/1170 | Loss: 0.5165 | Batch acc: 0.88\n",
      "  Step 450/1170 | Loss: 0.3159 | Batch acc: 0.88\n",
      "  Step 460/1170 | Loss: 0.3719 | Batch acc: 0.97\n",
      "  Step 470/1170 | Loss: 0.4839 | Batch acc: 0.88\n",
      "  Step 480/1170 | Loss: 0.1268 | Batch acc: 0.97\n",
      "  Step 490/1170 | Loss: 0.0888 | Batch acc: 1.00\n",
      "  Step 500/1170 | Loss: 0.2672 | Batch acc: 0.91\n",
      "  Step 510/1170 | Loss: 0.2020 | Batch acc: 0.94\n",
      "  Step 520/1170 | Loss: 0.2766 | Batch acc: 0.91\n",
      "  Step 530/1170 | Loss: 0.0954 | Batch acc: 0.97\n",
      "  Step 540/1170 | Loss: 0.3133 | Batch acc: 0.84\n",
      "  Step 550/1170 | Loss: 0.1501 | Batch acc: 0.94\n",
      "  Step 560/1170 | Loss: 0.4750 | Batch acc: 0.81\n",
      "  Step 570/1170 | Loss: 0.4003 | Batch acc: 0.91\n",
      "  Step 580/1170 | Loss: 0.0754 | Batch acc: 1.00\n",
      "  Step 590/1170 | Loss: 0.2187 | Batch acc: 0.91\n",
      "  Step 600/1170 | Loss: 0.3067 | Batch acc: 0.91\n",
      "  Step 610/1170 | Loss: 0.3444 | Batch acc: 0.88\n",
      "  Step 620/1170 | Loss: 0.2338 | Batch acc: 0.97\n",
      "  Step 630/1170 | Loss: 0.2647 | Batch acc: 0.94\n",
      "  Step 640/1170 | Loss: 0.0409 | Batch acc: 1.00\n",
      "  Step 650/1170 | Loss: 0.1695 | Batch acc: 0.97\n",
      "  Step 660/1170 | Loss: 0.1780 | Batch acc: 0.94\n",
      "  Step 670/1170 | Loss: 0.0670 | Batch acc: 1.00\n",
      "  Step 680/1170 | Loss: 0.1702 | Batch acc: 0.94\n",
      "  Step 690/1170 | Loss: 0.2318 | Batch acc: 0.91\n",
      "  Step 700/1170 | Loss: 0.3792 | Batch acc: 0.88\n",
      "  Step 710/1170 | Loss: 0.1884 | Batch acc: 0.91\n",
      "  Step 720/1170 | Loss: 0.4390 | Batch acc: 0.88\n",
      "  Step 730/1170 | Loss: 0.2949 | Batch acc: 0.91\n",
      "  Step 740/1170 | Loss: 0.3494 | Batch acc: 0.88\n",
      "  Step 750/1170 | Loss: 0.1610 | Batch acc: 0.94\n",
      "  Step 760/1170 | Loss: 0.1003 | Batch acc: 0.97\n",
      "  Step 770/1170 | Loss: 0.0838 | Batch acc: 0.97\n",
      "  Step 780/1170 | Loss: 0.1739 | Batch acc: 0.94\n",
      "  Step 790/1170 | Loss: 0.4317 | Batch acc: 0.84\n",
      "  Step 800/1170 | Loss: 0.4819 | Batch acc: 0.84\n",
      "  Step 810/1170 | Loss: 0.1751 | Batch acc: 0.94\n",
      "  Step 820/1170 | Loss: 0.1797 | Batch acc: 0.94\n",
      "  Step 830/1170 | Loss: 0.2215 | Batch acc: 0.91\n",
      "  Step 840/1170 | Loss: 0.3197 | Batch acc: 0.91\n",
      "  Step 850/1170 | Loss: 0.1629 | Batch acc: 0.97\n",
      "  Step 860/1170 | Loss: 0.0991 | Batch acc: 1.00\n",
      "  Step 870/1170 | Loss: 0.3195 | Batch acc: 0.91\n",
      "  Step 880/1170 | Loss: 0.2964 | Batch acc: 0.88\n",
      "  Step 890/1170 | Loss: 0.2123 | Batch acc: 0.91\n",
      "  Step 900/1170 | Loss: 0.1945 | Batch acc: 0.91\n",
      "  Step 910/1170 | Loss: 0.0595 | Batch acc: 1.00\n",
      "  Step 920/1170 | Loss: 0.0716 | Batch acc: 1.00\n",
      "  Step 930/1170 | Loss: 0.1825 | Batch acc: 0.91\n",
      "  Step 940/1170 | Loss: 0.0758 | Batch acc: 0.97\n",
      "  Step 950/1170 | Loss: 0.0916 | Batch acc: 0.97\n",
      "  Step 960/1170 | Loss: 0.0396 | Batch acc: 1.00\n",
      "  Step 970/1170 | Loss: 0.0826 | Batch acc: 0.97\n",
      "  Step 980/1170 | Loss: 0.0236 | Batch acc: 1.00\n",
      "  Step 990/1170 | Loss: 0.0961 | Batch acc: 0.97\n",
      "  Step 1000/1170 | Loss: 0.3154 | Batch acc: 0.91\n",
      "  Step 1010/1170 | Loss: 0.2250 | Batch acc: 0.94\n",
      "  Step 1020/1170 | Loss: 0.0580 | Batch acc: 1.00\n",
      "  Step 1030/1170 | Loss: 0.1520 | Batch acc: 0.94\n",
      "  Step 1040/1170 | Loss: 0.2079 | Batch acc: 0.94\n",
      "  Step 1050/1170 | Loss: 0.1855 | Batch acc: 0.91\n",
      "  Step 1060/1170 | Loss: 0.0693 | Batch acc: 1.00\n",
      "  Step 1070/1170 | Loss: 0.0602 | Batch acc: 1.00\n",
      "  Step 1080/1170 | Loss: 0.0623 | Batch acc: 1.00\n",
      "  Step 1090/1170 | Loss: 0.0334 | Batch acc: 1.00\n",
      "  Step 1100/1170 | Loss: 0.2256 | Batch acc: 0.94\n",
      "  Step 1110/1170 | Loss: 0.3222 | Batch acc: 0.84\n",
      "  Step 1120/1170 | Loss: 0.5678 | Batch acc: 0.81\n",
      "  Step 1130/1170 | Loss: 0.1160 | Batch acc: 0.97\n",
      "  Step 1140/1170 | Loss: 0.1365 | Batch acc: 0.94\n",
      "  Step 1150/1170 | Loss: 0.1182 | Batch acc: 0.97\n",
      "  Step 1160/1170 | Loss: 0.2714 | Batch acc: 0.91\n",
      "  Step 1170/1170 | Loss: 0.2812 | Batch acc: 0.86\n",
      "Train: loss=0.3513, acc=0.893\n",
      "  Step 1/307 | Loss: 2.1775 | Batch acc: 0.34\n",
      "  Step 10/307 | Loss: 0.5127 | Batch acc: 0.75\n",
      "  Step 20/307 | Loss: 0.0545 | Batch acc: 1.00\n",
      "  Step 30/307 | Loss: 0.0000 | Batch acc: 1.00\n",
      "  Step 40/307 | Loss: 0.0007 | Batch acc: 1.00\n",
      "  Step 50/307 | Loss: 1.1852 | Batch acc: 0.12\n",
      "  Step 60/307 | Loss: 1.2073 | Batch acc: 0.22\n",
      "  Step 70/307 | Loss: 0.1274 | Batch acc: 0.94\n",
      "  Step 80/307 | Loss: 0.0000 | Batch acc: 1.00\n",
      "  Step 90/307 | Loss: 1.3156 | Batch acc: 0.38\n",
      "  Step 100/307 | Loss: 0.8684 | Batch acc: 0.66\n",
      "  Step 110/307 | Loss: 1.3657 | Batch acc: 0.22\n",
      "  Step 120/307 | Loss: 2.1352 | Batch acc: 0.31\n",
      "  Step 130/307 | Loss: 10.5238 | Batch acc: 0.06\n",
      "  Step 140/307 | Loss: 0.3774 | Batch acc: 0.91\n",
      "  Step 150/307 | Loss: 0.0160 | Batch acc: 1.00\n",
      "  Step 160/307 | Loss: 0.3693 | Batch acc: 0.91\n",
      "  Step 170/307 | Loss: 0.1077 | Batch acc: 1.00\n",
      "  Step 180/307 | Loss: 0.0807 | Batch acc: 1.00\n",
      "  Step 190/307 | Loss: 0.1949 | Batch acc: 0.94\n",
      "  Step 200/307 | Loss: 0.0066 | Batch acc: 1.00\n",
      "  Step 210/307 | Loss: 0.0001 | Batch acc: 1.00\n",
      "  Step 220/307 | Loss: 0.6276 | Batch acc: 0.59\n",
      "  Step 230/307 | Loss: 0.2586 | Batch acc: 0.97\n",
      "  Step 240/307 | Loss: 3.2949 | Batch acc: 0.03\n",
      "  Step 250/307 | Loss: 8.4422 | Batch acc: 0.16\n",
      "  Step 260/307 | Loss: 0.0340 | Batch acc: 1.00\n",
      "  Step 270/307 | Loss: 0.0120 | Batch acc: 1.00\n",
      "  Step 280/307 | Loss: 0.0065 | Batch acc: 1.00\n",
      "  Step 290/307 | Loss: 0.0001 | Batch acc: 1.00\n",
      "  Step 300/307 | Loss: 0.0006 | Batch acc: 1.00\n",
      "Val:   loss=0.7413, acc=0.763\n",
      "  âœ… New best model saved (acc=0.763)\n",
      "\n",
      "Epoch 2/2\n",
      "  Step 1/1170 | Loss: 0.2772 | Batch acc: 0.91\n",
      "  Step 10/1170 | Loss: 0.1243 | Batch acc: 0.94\n",
      "  Step 20/1170 | Loss: 0.1291 | Batch acc: 0.97\n",
      "  Step 30/1170 | Loss: 0.1804 | Batch acc: 0.97\n",
      "  Step 40/1170 | Loss: 0.1640 | Batch acc: 0.88\n",
      "  Step 50/1170 | Loss: 0.0968 | Batch acc: 1.00\n",
      "  Step 60/1170 | Loss: 0.0343 | Batch acc: 1.00\n",
      "  Step 70/1170 | Loss: 0.1438 | Batch acc: 0.97\n",
      "  Step 80/1170 | Loss: 0.1343 | Batch acc: 0.94\n",
      "  Step 90/1170 | Loss: 0.0458 | Batch acc: 1.00\n",
      "  Step 100/1170 | Loss: 0.0676 | Batch acc: 0.97\n",
      "  Step 110/1170 | Loss: 0.0789 | Batch acc: 0.97\n",
      "  Step 120/1170 | Loss: 0.0523 | Batch acc: 1.00\n",
      "  Step 130/1170 | Loss: 0.2447 | Batch acc: 0.88\n",
      "  Step 140/1170 | Loss: 0.1811 | Batch acc: 0.97\n",
      "  Step 150/1170 | Loss: 0.1683 | Batch acc: 0.94\n",
      "  Step 160/1170 | Loss: 0.0911 | Batch acc: 0.97\n",
      "  Step 170/1170 | Loss: 0.2470 | Batch acc: 0.94\n",
      "  Step 180/1170 | Loss: 0.2093 | Batch acc: 0.88\n",
      "  Step 190/1170 | Loss: 0.0623 | Batch acc: 0.97\n",
      "  Step 200/1170 | Loss: 0.0073 | Batch acc: 1.00\n",
      "  Step 210/1170 | Loss: 0.1928 | Batch acc: 0.94\n",
      "  Step 220/1170 | Loss: 0.2091 | Batch acc: 0.88\n",
      "  Step 230/1170 | Loss: 0.2281 | Batch acc: 0.91\n",
      "  Step 240/1170 | Loss: 0.0469 | Batch acc: 1.00\n",
      "  Step 250/1170 | Loss: 0.2818 | Batch acc: 0.94\n",
      "  Step 260/1170 | Loss: 0.2606 | Batch acc: 0.94\n",
      "  Step 270/1170 | Loss: 0.1717 | Batch acc: 0.91\n",
      "  Step 280/1170 | Loss: 0.1001 | Batch acc: 0.97\n",
      "  Step 290/1170 | Loss: 0.3420 | Batch acc: 0.88\n",
      "  Step 300/1170 | Loss: 0.0886 | Batch acc: 0.97\n",
      "  Step 310/1170 | Loss: 0.0377 | Batch acc: 0.97\n",
      "  Step 320/1170 | Loss: 0.0372 | Batch acc: 1.00\n",
      "  Step 330/1170 | Loss: 0.1668 | Batch acc: 0.94\n",
      "  Step 340/1170 | Loss: 0.4895 | Batch acc: 0.91\n",
      "  Step 350/1170 | Loss: 0.0616 | Batch acc: 0.97\n",
      "  Step 360/1170 | Loss: 0.0717 | Batch acc: 1.00\n",
      "  Step 370/1170 | Loss: 0.0444 | Batch acc: 1.00\n",
      "  Step 380/1170 | Loss: 0.1393 | Batch acc: 0.91\n",
      "  Step 390/1170 | Loss: 0.1778 | Batch acc: 0.97\n",
      "  Step 400/1170 | Loss: 0.0644 | Batch acc: 1.00\n",
      "  Step 410/1170 | Loss: 0.2175 | Batch acc: 0.94\n",
      "  Step 420/1170 | Loss: 0.1692 | Batch acc: 0.97\n",
      "  Step 430/1170 | Loss: 0.1073 | Batch acc: 0.94\n",
      "  Step 440/1170 | Loss: 0.0793 | Batch acc: 0.97\n",
      "  Step 450/1170 | Loss: 0.0657 | Batch acc: 0.97\n",
      "  Step 460/1170 | Loss: 0.0979 | Batch acc: 0.94\n",
      "  Step 470/1170 | Loss: 0.1018 | Batch acc: 0.97\n",
      "  Step 480/1170 | Loss: 0.0279 | Batch acc: 1.00\n",
      "  Step 490/1170 | Loss: 0.0794 | Batch acc: 0.97\n",
      "  Step 500/1170 | Loss: 0.0096 | Batch acc: 1.00\n",
      "  Step 510/1170 | Loss: 0.0670 | Batch acc: 0.97\n",
      "  Step 520/1170 | Loss: 0.0199 | Batch acc: 1.00\n",
      "  Step 530/1170 | Loss: 0.3532 | Batch acc: 0.88\n",
      "  Step 540/1170 | Loss: 0.0313 | Batch acc: 0.97\n",
      "  Step 550/1170 | Loss: 0.0468 | Batch acc: 1.00\n",
      "  Step 560/1170 | Loss: 0.2141 | Batch acc: 0.94\n",
      "  Step 570/1170 | Loss: 0.1454 | Batch acc: 0.94\n",
      "  Step 580/1170 | Loss: 0.1049 | Batch acc: 0.97\n",
      "  Step 590/1170 | Loss: 0.0371 | Batch acc: 1.00\n",
      "  Step 600/1170 | Loss: 0.0305 | Batch acc: 1.00\n",
      "  Step 610/1170 | Loss: 0.0163 | Batch acc: 1.00\n",
      "  Step 620/1170 | Loss: 0.0360 | Batch acc: 1.00\n",
      "  Step 630/1170 | Loss: 0.0633 | Batch acc: 0.97\n",
      "  Step 640/1170 | Loss: 0.0480 | Batch acc: 0.97\n",
      "  Step 650/1170 | Loss: 0.1358 | Batch acc: 0.97\n",
      "  Step 660/1170 | Loss: 0.5439 | Batch acc: 0.84\n",
      "  Step 670/1170 | Loss: 0.0926 | Batch acc: 0.97\n",
      "  Step 680/1170 | Loss: 0.1707 | Batch acc: 0.91\n",
      "  Step 690/1170 | Loss: 0.1565 | Batch acc: 0.97\n",
      "  Step 700/1170 | Loss: 0.0439 | Batch acc: 1.00\n",
      "  Step 710/1170 | Loss: 0.1239 | Batch acc: 0.94\n",
      "  Step 720/1170 | Loss: 0.0389 | Batch acc: 1.00\n",
      "  Step 730/1170 | Loss: 0.0685 | Batch acc: 0.97\n",
      "  Step 740/1170 | Loss: 0.3070 | Batch acc: 0.91\n",
      "  Step 750/1170 | Loss: 0.0116 | Batch acc: 1.00\n",
      "  Step 760/1170 | Loss: 0.1042 | Batch acc: 0.97\n",
      "  Step 770/1170 | Loss: 0.0937 | Batch acc: 0.97\n",
      "  Step 780/1170 | Loss: 0.3124 | Batch acc: 0.91\n",
      "  Step 790/1170 | Loss: 0.1240 | Batch acc: 0.94\n",
      "  Step 800/1170 | Loss: 0.1657 | Batch acc: 0.97\n",
      "  Step 810/1170 | Loss: 0.0694 | Batch acc: 0.97\n",
      "  Step 820/1170 | Loss: 0.2379 | Batch acc: 0.91\n",
      "  Step 830/1170 | Loss: 0.0871 | Batch acc: 0.97\n",
      "  Step 840/1170 | Loss: 0.0494 | Batch acc: 1.00\n",
      "  Step 850/1170 | Loss: 0.0624 | Batch acc: 1.00\n",
      "  Step 860/1170 | Loss: 0.0836 | Batch acc: 0.97\n",
      "  Step 870/1170 | Loss: 0.3403 | Batch acc: 0.91\n",
      "  Step 880/1170 | Loss: 0.0588 | Batch acc: 0.97\n",
      "  Step 890/1170 | Loss: 0.2916 | Batch acc: 0.91\n",
      "  Step 900/1170 | Loss: 0.1632 | Batch acc: 0.94\n",
      "  Step 910/1170 | Loss: 0.1125 | Batch acc: 0.97\n",
      "  Step 920/1170 | Loss: 0.1887 | Batch acc: 0.91\n",
      "  Step 930/1170 | Loss: 0.0432 | Batch acc: 1.00\n",
      "  Step 940/1170 | Loss: 0.1010 | Batch acc: 0.97\n",
      "  Step 950/1170 | Loss: 0.0363 | Batch acc: 1.00\n",
      "  Step 960/1170 | Loss: 0.0264 | Batch acc: 1.00\n",
      "  Step 970/1170 | Loss: 0.0238 | Batch acc: 1.00\n",
      "  Step 980/1170 | Loss: 0.1933 | Batch acc: 0.97\n",
      "  Step 990/1170 | Loss: 0.0567 | Batch acc: 1.00\n",
      "  Step 1000/1170 | Loss: 0.0148 | Batch acc: 1.00\n",
      "  Step 1010/1170 | Loss: 0.3080 | Batch acc: 0.94\n",
      "  Step 1020/1170 | Loss: 0.2011 | Batch acc: 0.91\n",
      "  Step 1030/1170 | Loss: 0.1345 | Batch acc: 0.94\n",
      "  Step 1040/1170 | Loss: 0.1371 | Batch acc: 0.97\n",
      "  Step 1050/1170 | Loss: 0.0788 | Batch acc: 1.00\n",
      "  Step 1060/1170 | Loss: 0.1700 | Batch acc: 0.94\n",
      "  Step 1070/1170 | Loss: 0.1999 | Batch acc: 0.94\n",
      "  Step 1080/1170 | Loss: 0.1010 | Batch acc: 0.97\n",
      "  Step 1090/1170 | Loss: 0.0837 | Batch acc: 0.97\n",
      "  Step 1100/1170 | Loss: 0.2506 | Batch acc: 0.94\n",
      "  Step 1110/1170 | Loss: 0.0497 | Batch acc: 1.00\n",
      "  Step 1120/1170 | Loss: 0.0383 | Batch acc: 1.00\n",
      "  Step 1130/1170 | Loss: 0.1251 | Batch acc: 0.94\n",
      "  Step 1140/1170 | Loss: 0.2429 | Batch acc: 0.91\n",
      "  Step 1150/1170 | Loss: 0.1197 | Batch acc: 0.97\n",
      "  Step 1160/1170 | Loss: 0.1550 | Batch acc: 0.94\n",
      "  Step 1170/1170 | Loss: 0.1394 | Batch acc: 0.95\n",
      "Train: loss=0.1413, acc=0.955\n",
      "  Step 1/307 | Loss: 2.3800 | Batch acc: 0.34\n",
      "  Step 10/307 | Loss: 0.2010 | Batch acc: 0.94\n",
      "  Step 20/307 | Loss: 0.0158 | Batch acc: 1.00\n",
      "  Step 30/307 | Loss: 0.0001 | Batch acc: 1.00\n",
      "  Step 40/307 | Loss: 0.0027 | Batch acc: 1.00\n",
      "  Step 50/307 | Loss: 5.7866 | Batch acc: 0.00\n",
      "  Step 60/307 | Loss: 4.8996 | Batch acc: 0.00\n",
      "  Step 70/307 | Loss: 0.1524 | Batch acc: 0.94\n",
      "  Step 80/307 | Loss: 0.0001 | Batch acc: 1.00\n",
      "  Step 90/307 | Loss: 3.0749 | Batch acc: 0.06\n",
      "  Step 100/307 | Loss: 0.6330 | Batch acc: 0.75\n",
      "  Step 110/307 | Loss: 3.9887 | Batch acc: 0.00\n",
      "  Step 120/307 | Loss: 1.8790 | Batch acc: 0.28\n",
      "  Step 130/307 | Loss: 8.3719 | Batch acc: 0.09\n",
      "  Step 140/307 | Loss: 0.9286 | Batch acc: 0.66\n",
      "  Step 150/307 | Loss: 0.1422 | Batch acc: 0.97\n",
      "  Step 160/307 | Loss: 0.3676 | Batch acc: 0.84\n",
      "  Step 170/307 | Loss: 0.4183 | Batch acc: 0.84\n",
      "  Step 180/307 | Loss: 0.1831 | Batch acc: 1.00\n",
      "  Step 190/307 | Loss: 0.5663 | Batch acc: 0.88\n",
      "  Step 200/307 | Loss: 0.0025 | Batch acc: 1.00\n",
      "  Step 210/307 | Loss: 0.0001 | Batch acc: 1.00\n",
      "  Step 220/307 | Loss: 0.1728 | Batch acc: 1.00\n",
      "  Step 230/307 | Loss: 0.1694 | Batch acc: 0.94\n",
      "  Step 240/307 | Loss: 9.8961 | Batch acc: 0.00\n",
      "  Step 250/307 | Loss: 4.9444 | Batch acc: 0.16\n",
      "  Step 260/307 | Loss: 0.0211 | Batch acc: 1.00\n",
      "  Step 270/307 | Loss: 0.0050 | Batch acc: 1.00\n",
      "  Step 280/307 | Loss: 0.0097 | Batch acc: 1.00\n",
      "  Step 290/307 | Loss: 0.0000 | Batch acc: 1.00\n",
      "  Step 300/307 | Loss: 0.0036 | Batch acc: 1.00\n",
      "Val:   loss=1.3308, acc=0.707\n",
      "\n",
      "Training finished!\n",
      "Best validation accuracy: 0.763\n",
      "Elapsed time: 403.7 sec\n"
     ]
    }
   ],
   "source": [
    "# ==== Tiny ResNet50 sanity trainer for Kvasir-Capsule ====\n",
    "# - Beginner-friendly, step-by-step\n",
    "# - Prints debug info along the way\n",
    "# - Optimized for Tesla V100 (mixed precision, pinned memory, etc.)\n",
    "# =========================================================\n",
    "\n",
    "import os, time\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models\n",
    "\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from collections import Counter\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "CSV_PATH = \"/bhome/fwag/ele670_project/data/processed/metadata_normalized.csv\"\n",
    "IMG_ROOT   = \"/bhome/fwag/ele670_project/data/raw/labelled_images\"\n",
    "IMG_SIZE   = 224\n",
    "BATCH_SIZE = 32     # safe for V100 (can go higher if memory allows)\n",
    "NUM_WORKERS = 4     # adjust depending on system\n",
    "EPOCHS     = 2      # keep small for quick sanity check\n",
    "LR         = 1e-3\n",
    "SEED       = 42\n",
    "\n",
    "# ----------------------------------------\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "if device == \"cuda\":\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print(\"Memory allocated:\", round(torch.cuda.memory_allocated(0)/1024**3, 2), \"GB\")\n",
    "\n",
    "# --------------- DATASET ----------------\n",
    "class KvasirDataset(Dataset):\n",
    "    \"\"\"Custom dataset to read image paths + labels from CSV.\"\"\"\n",
    "    def __init__(self, df, img_root, transform):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.img_root = img_root\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        # Join root with relative path from CSV\n",
    "        img_path = os.path.join(self.img_root, row[\"image_path\"])\n",
    "        try:\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            print(f\"[WARNING] Failed to open {img_path}: {e}\")\n",
    "            # use black image as fallback\n",
    "            img = Image.new(\"RGB\", (IMG_SIZE, IMG_SIZE), (0,0,0))\n",
    "        img = self.transform(img)\n",
    "        label = int(row[\"encoded_label\"])\n",
    "        return img, label\n",
    "\n",
    "# --------------- LOAD CSV ----------------\n",
    "df = pd.read_csv(CSV_PATH, sep=\",\")\n",
    "print(\"First rows of metadata_processed.csv:\")\n",
    "print(df.head())\n",
    "\n",
    "# How many unique classes?\n",
    "num_classes = df[\"encoded_label\"].nunique()\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "\n",
    "# --------------- TRAIN/VAL SPLIT ----------------\n",
    "# Keep videos separated (no leakage between train/val)\n",
    "gss = GroupShuffleSplit(n_splits=1, train_size=0.8, random_state=SEED)\n",
    "train_idx, val_idx = next(gss.split(df, groups=df[\"video_id\"]))\n",
    "df_train, df_val = df.iloc[train_idx], df.iloc[val_idx]\n",
    "\n",
    "print(f\"Train samples: {len(df_train)} | Val samples: {len(df_val)}\")\n",
    "print(f\"Train videos: {df_train['video_id'].nunique()} | Val videos: {df_val['video_id'].nunique()}\")\n",
    "\n",
    "# --------------- TRANSFORMS ----------------\n",
    "# Simple transforms for sanity check\n",
    "mean = (0.485, 0.456, 0.406)\n",
    "std  = (0.229, 0.224, 0.225)\n",
    "\n",
    "train_tf = T.Compose([\n",
    "    T.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    T.RandomHorizontalFlip(p=0.5),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean, std),\n",
    "])\n",
    "\n",
    "val_tf = T.Compose([\n",
    "    T.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean, std),\n",
    "])\n",
    "\n",
    "# --------------- DATA LOADERS ----------------\n",
    "train_ds = KvasirDataset(df_train, IMG_ROOT, train_tf)\n",
    "val_ds   = KvasirDataset(df_val,   IMG_ROOT, val_tf)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "print(\"One training batch (images, labels):\")\n",
    "sample_imgs, sample_labels = next(iter(train_loader))\n",
    "print(\"Image batch shape:\", sample_imgs.shape)\n",
    "print(\"Label batch shape:\", sample_labels.shape)\n",
    "\n",
    "# --------------- MODEL ----------------\n",
    "# Load ResNet50 with ImageNet weights (helps it train faster)\n",
    "model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)  # replace classifier\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss + optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "# Mixed precision (faster on V100)\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "# --------------- TRAINING LOOP ----------------\n",
    "def run_epoch(loader, train=True):\n",
    "    if train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    total_loss, total_correct, total_samples = 0.0, 0, 0\n",
    "\n",
    "    for step, (imgs, labels) in enumerate(loader, start=1):\n",
    "        imgs = imgs.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=True):\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        if train:\n",
    "            optimizer.zero_grad()\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "        # Accuracy calculation\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        correct = (preds == labels).sum().item()\n",
    "        total_correct += correct\n",
    "        total_loss += loss.item() * labels.size(0)\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "        # Print debug info every 10 steps\n",
    "        if step % 10 == 0 or step == 1:\n",
    "            print(f\"  Step {step}/{len(loader)} | Loss: {loss.item():.4f} | \"\n",
    "                  f\"Batch acc: {correct/labels.size(0):.2f}\")\n",
    "\n",
    "    avg_loss = total_loss / total_samples\n",
    "    avg_acc = total_correct / total_samples\n",
    "    return avg_loss, avg_acc\n",
    "\n",
    "# --------------- TRAIN FOR A FEW EPOCHS ----------------\n",
    "print(\"\\n==== START TRAINING ====\\n\")\n",
    "best_val_acc = 0.0\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    print(f\"\\nEpoch {epoch}/{EPOCHS}\")\n",
    "\n",
    "    train_loss, train_acc = run_epoch(train_loader, train=True)\n",
    "    print(f\"Train: loss={train_loss:.4f}, acc={train_acc:.3f}\")\n",
    "\n",
    "    val_loss, val_acc = run_epoch(val_loader, train=False)\n",
    "    print(f\"Val:   loss={val_loss:.4f}, acc={val_acc:.3f}\")\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), \"tiny_resnet50_best.pt\")\n",
    "        print(f\"  âœ… New best model saved (acc={best_val_acc:.3f})\")\n",
    "\n",
    "print(\"\\nTraining finished!\")\n",
    "print(\"Best validation accuracy:\", round(best_val_acc, 3))\n",
    "print(\"Elapsed time: %.1f sec\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd260f35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce100be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Tesla V100-PCIE-32GB\n",
      "Allocated: 0.56 GB\n",
      "Reserved:  3.44 GB\n",
      "First rows of metadata_normalized.csv:\n",
      "                                       image_path        finding_class  \\\n",
      "0  Normal clean mucosa/0728084c8da942d9_22803.jpg  Normal clean mucosa   \n",
      "1  Normal clean mucosa/0728084c8da942d9_22804.jpg  Normal clean mucosa   \n",
      "2  Normal clean mucosa/0728084c8da942d9_22805.jpg  Normal clean mucosa   \n",
      "3  Normal clean mucosa/0728084c8da942d9_22806.jpg  Normal clean mucosa   \n",
      "4  Normal clean mucosa/0728084c8da942d9_22807.jpg  Normal clean mucosa   \n",
      "\n",
      "           video_id  encoded_label  \n",
      "0  0728084c8da942d9              9  \n",
      "1  0728084c8da942d9              9  \n",
      "2  0728084c8da942d9              9  \n",
      "3  0728084c8da942d9              9  \n",
      "4  0728084c8da942d9              9  \n",
      "Number of classes: 14\n",
      "Train samples: 37430 | Val samples: 9818\n",
      "Train videos: 34 | Val videos: 9\n",
      "One training batch (images, labels):\n",
      "Image batch shape: torch.Size([32, 3, 224, 224])\n",
      "Label batch shape: torch.Size([32])\n",
      "\n",
      "==== START TRAINING ====\n",
      "\n",
      "\n",
      "Epoch 1/2\n",
      "  Step 1/1169 | Loss: 2.7060 | Batch acc: 0.03\n",
      "  Step 10/1169 | Loss: 1.1464 | Batch acc: 0.78\n",
      "  Step 20/1169 | Loss: 2.0793 | Batch acc: 0.56\n",
      "  Step 30/1169 | Loss: 1.2360 | Batch acc: 0.69\n",
      "  Step 40/1169 | Loss: 1.3910 | Batch acc: 0.62\n",
      "  Step 50/1169 | Loss: 1.2361 | Batch acc: 0.72\n",
      "  Step 60/1169 | Loss: 1.1189 | Batch acc: 0.75\n",
      "  Step 70/1169 | Loss: 0.9773 | Batch acc: 0.81\n",
      "  Step 80/1169 | Loss: 0.9523 | Batch acc: 0.78\n",
      "  Step 90/1169 | Loss: 1.3639 | Batch acc: 0.59\n",
      "  Step 100/1169 | Loss: 1.0276 | Batch acc: 0.75\n",
      "  Step 110/1169 | Loss: 0.8167 | Batch acc: 0.88\n",
      "  Step 120/1169 | Loss: 0.9579 | Batch acc: 0.75\n",
      "  Step 130/1169 | Loss: 0.9642 | Batch acc: 0.84\n",
      "  Step 140/1169 | Loss: 1.1073 | Batch acc: 0.78\n",
      "  Step 150/1169 | Loss: 0.7302 | Batch acc: 0.88\n",
      "  Step 160/1169 | Loss: 1.3628 | Batch acc: 0.59\n",
      "  Step 170/1169 | Loss: 0.9289 | Batch acc: 0.81\n",
      "  Step 180/1169 | Loss: 0.7257 | Batch acc: 0.88\n",
      "  Step 190/1169 | Loss: 0.7274 | Batch acc: 0.84\n",
      "  Step 200/1169 | Loss: 0.9095 | Batch acc: 0.78\n",
      "  Step 210/1169 | Loss: 0.8077 | Batch acc: 0.78\n",
      "  Step 220/1169 | Loss: 0.9365 | Batch acc: 0.78\n",
      "  Step 230/1169 | Loss: 0.9867 | Batch acc: 0.75\n",
      "  Step 240/1169 | Loss: 0.8396 | Batch acc: 0.78\n",
      "  Step 250/1169 | Loss: 0.8777 | Batch acc: 0.84\n",
      "  Step 260/1169 | Loss: 0.7974 | Batch acc: 0.88\n",
      "  Step 270/1169 | Loss: 0.7858 | Batch acc: 0.88\n",
      "  Step 280/1169 | Loss: 0.8213 | Batch acc: 0.84\n",
      "  Step 290/1169 | Loss: 0.9496 | Batch acc: 0.81\n",
      "  Step 300/1169 | Loss: 1.1069 | Batch acc: 0.66\n",
      "  Step 310/1169 | Loss: 0.6433 | Batch acc: 0.88\n",
      "  Step 320/1169 | Loss: 0.5208 | Batch acc: 1.00\n",
      "  Step 330/1169 | Loss: 0.6260 | Batch acc: 0.91\n",
      "  Step 340/1169 | Loss: 1.1711 | Batch acc: 0.72\n",
      "  Step 350/1169 | Loss: 0.7984 | Batch acc: 0.78\n",
      "  Step 360/1169 | Loss: 0.6885 | Batch acc: 0.94\n",
      "  Step 370/1169 | Loss: 0.9485 | Batch acc: 0.81\n",
      "  Step 380/1169 | Loss: 0.7774 | Batch acc: 0.84\n",
      "  Step 390/1169 | Loss: 0.8924 | Batch acc: 0.75\n",
      "  Step 400/1169 | Loss: 0.6733 | Batch acc: 0.88\n",
      "  Step 410/1169 | Loss: 0.8423 | Batch acc: 0.78\n",
      "  Step 420/1169 | Loss: 0.9955 | Batch acc: 0.72\n",
      "  Step 430/1169 | Loss: 0.6317 | Batch acc: 0.91\n",
      "  Step 440/1169 | Loss: 0.6166 | Batch acc: 0.94\n",
      "  Step 450/1169 | Loss: 0.5852 | Batch acc: 0.91\n",
      "  Step 460/1169 | Loss: 0.7134 | Batch acc: 0.81\n",
      "  Step 470/1169 | Loss: 0.9394 | Batch acc: 0.78\n",
      "  Step 480/1169 | Loss: 0.9142 | Batch acc: 0.72\n",
      "  Step 490/1169 | Loss: 0.8555 | Batch acc: 0.84\n",
      "  Step 500/1169 | Loss: 0.8311 | Batch acc: 0.88\n",
      "  Step 510/1169 | Loss: 0.6724 | Batch acc: 0.88\n",
      "  Step 520/1169 | Loss: 0.6830 | Batch acc: 0.88\n",
      "  Step 530/1169 | Loss: 0.7150 | Batch acc: 0.84\n",
      "  Step 540/1169 | Loss: 0.5653 | Batch acc: 0.91\n",
      "  Step 550/1169 | Loss: 0.8096 | Batch acc: 0.81\n",
      "  Step 560/1169 | Loss: 0.8922 | Batch acc: 0.78\n",
      "  Step 570/1169 | Loss: 0.7692 | Batch acc: 0.88\n",
      "  Step 580/1169 | Loss: 0.7491 | Batch acc: 0.84\n",
      "  Step 590/1169 | Loss: 0.7248 | Batch acc: 0.88\n",
      "  Step 600/1169 | Loss: 0.9536 | Batch acc: 0.81\n",
      "  Step 610/1169 | Loss: 0.7266 | Batch acc: 0.88\n",
      "  Step 620/1169 | Loss: 0.9706 | Batch acc: 0.78\n",
      "  Step 630/1169 | Loss: 0.5183 | Batch acc: 0.94\n",
      "  Step 640/1169 | Loss: 0.5797 | Batch acc: 0.94\n",
      "  Step 650/1169 | Loss: 0.6028 | Batch acc: 0.88\n",
      "  Step 660/1169 | Loss: 0.6686 | Batch acc: 0.88\n",
      "  Step 670/1169 | Loss: 0.7816 | Batch acc: 0.91\n",
      "  Step 680/1169 | Loss: 0.5842 | Batch acc: 0.97\n",
      "  Step 690/1169 | Loss: 0.7380 | Batch acc: 0.88\n",
      "  Step 700/1169 | Loss: 0.7544 | Batch acc: 0.88\n",
      "  Step 710/1169 | Loss: 0.7953 | Batch acc: 0.91\n",
      "  Step 720/1169 | Loss: 0.7423 | Batch acc: 0.88\n",
      "  Step 730/1169 | Loss: 0.8360 | Batch acc: 0.78\n",
      "  Step 740/1169 | Loss: 0.7925 | Batch acc: 0.84\n",
      "  Step 750/1169 | Loss: 0.6496 | Batch acc: 0.88\n",
      "  Step 760/1169 | Loss: 0.7625 | Batch acc: 0.88\n",
      "  Step 770/1169 | Loss: 0.6289 | Batch acc: 0.91\n",
      "  Step 780/1169 | Loss: 1.0365 | Batch acc: 0.78\n",
      "  Step 790/1169 | Loss: 0.7365 | Batch acc: 0.84\n",
      "  Step 800/1169 | Loss: 0.7272 | Batch acc: 0.84\n",
      "  Step 810/1169 | Loss: 0.7772 | Batch acc: 0.81\n",
      "  Step 820/1169 | Loss: 0.6202 | Batch acc: 0.84\n",
      "  Step 830/1169 | Loss: 0.6488 | Batch acc: 0.91\n",
      "  Step 840/1169 | Loss: 0.6319 | Batch acc: 0.91\n",
      "  Step 850/1169 | Loss: 0.7612 | Batch acc: 0.88\n",
      "  Step 860/1169 | Loss: 0.5988 | Batch acc: 0.97\n",
      "  Step 870/1169 | Loss: 0.7321 | Batch acc: 0.88\n",
      "  Step 880/1169 | Loss: 0.9290 | Batch acc: 0.78\n",
      "  Step 890/1169 | Loss: 0.4831 | Batch acc: 0.97\n",
      "  Step 900/1169 | Loss: 0.8097 | Batch acc: 0.78\n",
      "  Step 910/1169 | Loss: 0.7557 | Batch acc: 0.88\n",
      "  Step 920/1169 | Loss: 0.6286 | Batch acc: 0.88\n",
      "  Step 930/1169 | Loss: 0.5861 | Batch acc: 0.91\n",
      "  Step 940/1169 | Loss: 0.7810 | Batch acc: 0.84\n",
      "  Step 950/1169 | Loss: 0.5232 | Batch acc: 0.94\n",
      "  Step 960/1169 | Loss: 0.6420 | Batch acc: 0.88\n",
      "  Step 970/1169 | Loss: 0.7110 | Batch acc: 0.88\n",
      "  Step 980/1169 | Loss: 0.8085 | Batch acc: 0.88\n",
      "  Step 990/1169 | Loss: 0.8574 | Batch acc: 0.78\n",
      "  Step 1000/1169 | Loss: 0.6370 | Batch acc: 0.88\n",
      "  Step 1010/1169 | Loss: 0.6104 | Batch acc: 0.94\n",
      "  Step 1020/1169 | Loss: 0.5862 | Batch acc: 0.91\n",
      "  Step 1030/1169 | Loss: 0.7270 | Batch acc: 0.88\n",
      "  Step 1040/1169 | Loss: 0.5953 | Batch acc: 0.94\n",
      "  Step 1050/1169 | Loss: 0.5379 | Batch acc: 0.94\n",
      "  Step 1060/1169 | Loss: 0.9549 | Batch acc: 0.78\n",
      "  Step 1070/1169 | Loss: 0.6003 | Batch acc: 0.91\n",
      "  Step 1080/1169 | Loss: 0.7011 | Batch acc: 0.88\n",
      "  Step 1090/1169 | Loss: 0.5173 | Batch acc: 0.94\n",
      "  Step 1100/1169 | Loss: 0.5076 | Batch acc: 0.97\n",
      "  Step 1110/1169 | Loss: 0.8430 | Batch acc: 0.84\n",
      "  Step 1120/1169 | Loss: 0.7982 | Batch acc: 0.84\n",
      "  Step 1130/1169 | Loss: 0.5252 | Batch acc: 0.91\n",
      "  Step 1140/1169 | Loss: 0.5714 | Batch acc: 0.94\n",
      "  Step 1150/1169 | Loss: 0.8356 | Batch acc: 0.84\n",
      "  Step 1160/1169 | Loss: 0.5616 | Batch acc: 0.91\n",
      "Train: loss=0.7997, acc=0.839\n",
      "  Step 1/307 | Loss: 1.7948 | Batch acc: 0.34\n",
      "  Step 10/307 | Loss: 0.7074 | Batch acc: 0.84\n",
      "  Step 20/307 | Loss: 0.3601 | Batch acc: 1.00\n",
      "  Step 30/307 | Loss: 0.3456 | Batch acc: 1.00\n",
      "  Step 40/307 | Loss: 0.3537 | Batch acc: 1.00\n",
      "  Step 50/307 | Loss: 1.1481 | Batch acc: 0.38\n",
      "  Step 60/307 | Loss: 1.1434 | Batch acc: 0.44\n",
      "  Step 70/307 | Loss: 0.4093 | Batch acc: 1.00\n",
      "  Step 80/307 | Loss: 0.3646 | Batch acc: 1.00\n",
      "  Step 90/307 | Loss: 2.2921 | Batch acc: 0.09\n",
      "  Step 100/307 | Loss: 1.3469 | Batch acc: 0.50\n",
      "  Step 110/307 | Loss: 2.1229 | Batch acc: 0.16\n",
      "  Step 120/307 | Loss: 1.7761 | Batch acc: 0.41\n",
      "  Step 130/307 | Loss: 5.1214 | Batch acc: 0.09\n",
      "  Step 140/307 | Loss: 0.4234 | Batch acc: 1.00\n",
      "  Step 150/307 | Loss: 0.3863 | Batch acc: 1.00\n",
      "  Step 160/307 | Loss: 0.3927 | Batch acc: 1.00\n",
      "  Step 170/307 | Loss: 0.8214 | Batch acc: 0.78\n",
      "  Step 180/307 | Loss: 0.5415 | Batch acc: 1.00\n",
      "  Step 190/307 | Loss: 0.9559 | Batch acc: 0.75\n",
      "  Step 200/307 | Loss: 0.4228 | Batch acc: 0.97\n",
      "  Step 210/307 | Loss: 0.3493 | Batch acc: 1.00\n",
      "  Step 220/307 | Loss: 0.7700 | Batch acc: 0.81\n",
      "  Step 230/307 | Loss: 0.4999 | Batch acc: 0.97\n",
      "  Step 240/307 | Loss: 2.8191 | Batch acc: 0.09\n",
      "  Step 250/307 | Loss: 3.7511 | Batch acc: 0.12\n",
      "  Step 260/307 | Loss: 0.4461 | Batch acc: 1.00\n",
      "  Step 270/307 | Loss: 0.3504 | Batch acc: 1.00\n",
      "  Step 280/307 | Loss: 0.3573 | Batch acc: 1.00\n",
      "  Step 290/307 | Loss: 0.3799 | Batch acc: 1.00\n",
      "  Step 300/307 | Loss: 0.3740 | Batch acc: 1.00\n",
      "Val:   loss=0.9487, acc=0.746\n",
      "  âœ… New best model saved â†’ /bhome/fwag/ele670_project/results/tiny_resnet50_best.pt (acc=0.746)\n",
      "\n",
      "Validation Confusion Matrix (rows=true, cols=pred):\n",
      "[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0   11    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    3    0   15    0   18    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    2    8    0    3    0  410  460    1  600    0    2   76   85]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0   30    0    0    1    0   23  114   12 6769    0  537   58   23]\n",
      " [   0    1    0    0    0    0   18    1    0   34    0    0    0    1]\n",
      " [   0    3    1    0    0    0   21    7    0  127    0   81   17    6]\n",
      " [   0    0    0    0    0    0    2  191    0   19    0    0   17   10]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0]]\n",
      "\n",
      "Per-class metrics (validation):\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "    Ampulla of Vater      0.000     0.000     0.000         0\n",
      "         Angiectasia      0.000     0.000     0.000        11\n",
      "       Blood - fresh      0.000     0.000     0.000         0\n",
      "     Blood - hematin      0.000     0.000     0.000         0\n",
      "             Erosion      0.000     0.000     0.000        36\n",
      "            Erythema      0.000     0.000     0.000         0\n",
      "        Foreign Body      0.000     0.000     0.000         0\n",
      "     Ileocecal valve      0.593     0.279     0.380      1647\n",
      "    Lymphangiectasia      0.000     0.000     0.000         0\n",
      " Normal clean mucosa      0.894     0.895     0.894      7567\n",
      "            class_10      0.000     0.000     0.000        55\n",
      "             Pylorus      0.127     0.308     0.180       263\n",
      "Reduced Mucosal View      0.101     0.071     0.084       239\n",
      "               Ulcer      0.000     0.000     0.000         0\n",
      "\n",
      "            accuracy                          0.746      9818\n",
      "           macro avg      0.122     0.111     0.110      9818\n",
      "        weighted avg      0.794     0.746     0.760      9818\n",
      "\n",
      "\n",
      "Epoch 2/2\n",
      "  Step 1/1169 | Loss: 0.8468 | Batch acc: 0.81\n",
      "  Step 10/1169 | Loss: 0.5508 | Batch acc: 0.94\n",
      "  Step 20/1169 | Loss: 0.7293 | Batch acc: 0.81\n",
      "  Step 30/1169 | Loss: 0.6812 | Batch acc: 0.88\n",
      "  Step 40/1169 | Loss: 0.6169 | Batch acc: 0.84\n",
      "  Step 50/1169 | Loss: 0.6556 | Batch acc: 0.94\n",
      "  Step 60/1169 | Loss: 0.6241 | Batch acc: 0.97\n",
      "  Step 70/1169 | Loss: 0.5211 | Batch acc: 0.94\n",
      "  Step 80/1169 | Loss: 0.6190 | Batch acc: 0.91\n",
      "  Step 90/1169 | Loss: 0.5234 | Batch acc: 0.94\n",
      "  Step 100/1169 | Loss: 0.6207 | Batch acc: 0.97\n",
      "  Step 110/1169 | Loss: 0.5154 | Batch acc: 0.91\n",
      "  Step 120/1169 | Loss: 0.5046 | Batch acc: 0.94\n",
      "  Step 130/1169 | Loss: 0.4785 | Batch acc: 1.00\n",
      "  Step 140/1169 | Loss: 0.5123 | Batch acc: 0.94\n",
      "  Step 150/1169 | Loss: 0.7085 | Batch acc: 0.91\n",
      "  Step 160/1169 | Loss: 0.6429 | Batch acc: 0.88\n",
      "  Step 170/1169 | Loss: 0.8848 | Batch acc: 0.72\n",
      "  Step 180/1169 | Loss: 0.7075 | Batch acc: 0.81\n",
      "  Step 190/1169 | Loss: 0.5458 | Batch acc: 0.94\n",
      "  Step 200/1169 | Loss: 0.5814 | Batch acc: 0.91\n",
      "  Step 210/1169 | Loss: 0.6697 | Batch acc: 0.84\n",
      "  Step 220/1169 | Loss: 0.4838 | Batch acc: 1.00\n",
      "  Step 230/1169 | Loss: 0.4734 | Batch acc: 0.94\n",
      "  Step 240/1169 | Loss: 0.5837 | Batch acc: 0.88\n",
      "  Step 250/1169 | Loss: 0.7906 | Batch acc: 0.84\n",
      "  Step 260/1169 | Loss: 0.5058 | Batch acc: 0.97\n",
      "  Step 270/1169 | Loss: 0.6165 | Batch acc: 0.88\n",
      "  Step 280/1169 | Loss: 0.5212 | Batch acc: 0.94\n",
      "  Step 290/1169 | Loss: 0.4751 | Batch acc: 1.00\n",
      "  Step 300/1169 | Loss: 0.5083 | Batch acc: 0.97\n",
      "  Step 310/1169 | Loss: 0.4795 | Batch acc: 0.97\n",
      "  Step 320/1169 | Loss: 0.4793 | Batch acc: 0.97\n",
      "  Step 330/1169 | Loss: 0.5279 | Batch acc: 0.94\n",
      "  Step 340/1169 | Loss: 0.6518 | Batch acc: 0.91\n",
      "  Step 350/1169 | Loss: 0.4640 | Batch acc: 0.97\n",
      "  Step 360/1169 | Loss: 0.6794 | Batch acc: 0.94\n",
      "  Step 370/1169 | Loss: 0.5424 | Batch acc: 0.94\n",
      "  Step 380/1169 | Loss: 0.6615 | Batch acc: 0.84\n",
      "  Step 390/1169 | Loss: 0.5845 | Batch acc: 0.94\n",
      "  Step 400/1169 | Loss: 0.5316 | Batch acc: 0.91\n",
      "  Step 410/1169 | Loss: 0.6867 | Batch acc: 0.88\n",
      "  Step 420/1169 | Loss: 0.6426 | Batch acc: 0.94\n",
      "  Step 430/1169 | Loss: 0.4620 | Batch acc: 0.97\n",
      "  Step 440/1169 | Loss: 0.6392 | Batch acc: 0.94\n",
      "  Step 450/1169 | Loss: 0.8226 | Batch acc: 0.75\n",
      "  Step 460/1169 | Loss: 0.5799 | Batch acc: 0.94\n",
      "  Step 470/1169 | Loss: 0.4300 | Batch acc: 1.00\n",
      "  Step 480/1169 | Loss: 0.4946 | Batch acc: 0.94\n",
      "  Step 490/1169 | Loss: 0.4982 | Batch acc: 0.97\n",
      "  Step 500/1169 | Loss: 0.4830 | Batch acc: 1.00\n",
      "  Step 510/1169 | Loss: 0.5428 | Batch acc: 0.94\n",
      "  Step 520/1169 | Loss: 0.6766 | Batch acc: 0.91\n",
      "  Step 530/1169 | Loss: 0.6072 | Batch acc: 0.84\n",
      "  Step 540/1169 | Loss: 0.5875 | Batch acc: 0.94\n",
      "  Step 550/1169 | Loss: 0.7346 | Batch acc: 0.88\n",
      "  Step 560/1169 | Loss: 0.5606 | Batch acc: 0.94\n",
      "  Step 570/1169 | Loss: 0.5901 | Batch acc: 0.94\n",
      "  Step 580/1169 | Loss: 0.5896 | Batch acc: 0.94\n",
      "  Step 590/1169 | Loss: 0.6095 | Batch acc: 0.88\n",
      "  Step 600/1169 | Loss: 0.5521 | Batch acc: 0.91\n",
      "  Step 610/1169 | Loss: 0.4517 | Batch acc: 0.94\n",
      "  Step 620/1169 | Loss: 0.5998 | Batch acc: 0.94\n",
      "  Step 630/1169 | Loss: 0.6607 | Batch acc: 0.84\n",
      "  Step 640/1169 | Loss: 0.5206 | Batch acc: 0.91\n",
      "  Step 650/1169 | Loss: 0.4532 | Batch acc: 1.00\n",
      "  Step 660/1169 | Loss: 0.5985 | Batch acc: 0.88\n",
      "  Step 670/1169 | Loss: 0.6059 | Batch acc: 0.91\n",
      "  Step 680/1169 | Loss: 0.4285 | Batch acc: 0.97\n",
      "  Step 690/1169 | Loss: 0.5716 | Batch acc: 0.94\n",
      "  Step 700/1169 | Loss: 0.5669 | Batch acc: 0.91\n",
      "  Step 710/1169 | Loss: 0.4611 | Batch acc: 0.97\n",
      "  Step 720/1169 | Loss: 0.4730 | Batch acc: 0.97\n",
      "  Step 730/1169 | Loss: 0.5763 | Batch acc: 0.91\n",
      "  Step 740/1169 | Loss: 0.6177 | Batch acc: 0.91\n",
      "  Step 750/1169 | Loss: 0.7120 | Batch acc: 0.84\n",
      "  Step 760/1169 | Loss: 0.4820 | Batch acc: 1.00\n",
      "  Step 770/1169 | Loss: 0.4644 | Batch acc: 0.97\n",
      "  Step 780/1169 | Loss: 0.5423 | Batch acc: 0.94\n",
      "  Step 790/1169 | Loss: 0.7398 | Batch acc: 0.84\n",
      "  Step 800/1169 | Loss: 0.5299 | Batch acc: 0.94\n",
      "  Step 810/1169 | Loss: 0.6315 | Batch acc: 0.81\n",
      "  Step 820/1169 | Loss: 0.5785 | Batch acc: 0.94\n",
      "  Step 830/1169 | Loss: 0.6543 | Batch acc: 0.88\n",
      "  Step 840/1169 | Loss: 0.5485 | Batch acc: 0.91\n",
      "  Step 850/1169 | Loss: 0.5871 | Batch acc: 0.91\n",
      "  Step 860/1169 | Loss: 0.6900 | Batch acc: 0.88\n",
      "  Step 870/1169 | Loss: 0.5113 | Batch acc: 0.94\n",
      "  Step 880/1169 | Loss: 0.5825 | Batch acc: 0.94\n",
      "  Step 890/1169 | Loss: 0.4847 | Batch acc: 1.00\n",
      "  Step 900/1169 | Loss: 0.5325 | Batch acc: 0.97\n",
      "  Step 910/1169 | Loss: 0.6525 | Batch acc: 0.91\n",
      "  Step 920/1169 | Loss: 0.4729 | Batch acc: 0.97\n",
      "  Step 930/1169 | Loss: 0.6878 | Batch acc: 0.84\n",
      "  Step 940/1169 | Loss: 0.6039 | Batch acc: 0.91\n",
      "  Step 950/1169 | Loss: 0.4333 | Batch acc: 1.00\n",
      "  Step 960/1169 | Loss: 0.4892 | Batch acc: 0.94\n",
      "  Step 970/1169 | Loss: 0.5612 | Batch acc: 0.91\n",
      "  Step 980/1169 | Loss: 0.4785 | Batch acc: 0.97\n",
      "  Step 990/1169 | Loss: 0.7986 | Batch acc: 0.84\n",
      "  Step 1000/1169 | Loss: 0.6397 | Batch acc: 0.91\n",
      "  Step 1010/1169 | Loss: 0.6831 | Batch acc: 0.88\n",
      "  Step 1020/1169 | Loss: 0.7763 | Batch acc: 0.78\n",
      "  Step 1030/1169 | Loss: 0.4706 | Batch acc: 0.97\n",
      "  Step 1040/1169 | Loss: 0.6343 | Batch acc: 0.88\n",
      "  Step 1050/1169 | Loss: 0.5761 | Batch acc: 0.94\n",
      "  Step 1060/1169 | Loss: 0.5023 | Batch acc: 0.97\n",
      "  Step 1070/1169 | Loss: 0.6283 | Batch acc: 0.91\n",
      "  Step 1080/1169 | Loss: 0.5793 | Batch acc: 0.88\n",
      "  Step 1090/1169 | Loss: 0.5876 | Batch acc: 0.94\n",
      "  Step 1100/1169 | Loss: 0.5444 | Batch acc: 0.91\n",
      "  Step 1110/1169 | Loss: 0.4679 | Batch acc: 1.00\n",
      "  Step 1120/1169 | Loss: 0.7051 | Batch acc: 0.84\n",
      "  Step 1130/1169 | Loss: 0.8100 | Batch acc: 0.84\n",
      "  Step 1140/1169 | Loss: 0.8569 | Batch acc: 0.75\n",
      "  Step 1150/1169 | Loss: 0.5919 | Batch acc: 0.91\n",
      "  Step 1160/1169 | Loss: 0.4696 | Batch acc: 0.97\n",
      "Train: loss=0.6030, acc=0.906\n",
      "  Step 1/307 | Loss: 1.7031 | Batch acc: 0.38\n",
      "  Step 10/307 | Loss: 0.6332 | Batch acc: 0.84\n",
      "  Step 20/307 | Loss: 0.3557 | Batch acc: 1.00\n",
      "  Step 30/307 | Loss: 0.3472 | Batch acc: 1.00\n",
      "  Step 40/307 | Loss: 0.3492 | Batch acc: 1.00\n",
      "  Step 50/307 | Loss: 1.5667 | Batch acc: 0.06\n",
      "  Step 60/307 | Loss: 1.3082 | Batch acc: 0.31\n",
      "  Step 70/307 | Loss: 0.3913 | Batch acc: 1.00\n",
      "  Step 80/307 | Loss: 0.3494 | Batch acc: 1.00\n",
      "  Step 90/307 | Loss: 1.8215 | Batch acc: 0.28\n",
      "  Step 100/307 | Loss: 1.2676 | Batch acc: 0.56\n",
      "  Step 110/307 | Loss: 1.9532 | Batch acc: 0.16\n",
      "  Step 120/307 | Loss: 1.4482 | Batch acc: 0.62\n",
      "  Step 130/307 | Loss: 5.1213 | Batch acc: 0.09\n",
      "  Step 140/307 | Loss: 0.3966 | Batch acc: 1.00\n",
      "  Step 150/307 | Loss: 0.3787 | Batch acc: 1.00\n",
      "  Step 160/307 | Loss: 0.3878 | Batch acc: 1.00\n",
      "  Step 170/307 | Loss: 0.6906 | Batch acc: 0.81\n",
      "  Step 180/307 | Loss: 0.4399 | Batch acc: 1.00\n",
      "  Step 190/307 | Loss: 1.0456 | Batch acc: 0.81\n",
      "  Step 200/307 | Loss: 0.4394 | Batch acc: 0.97\n",
      "  Step 210/307 | Loss: 0.3401 | Batch acc: 1.00\n",
      "  Step 220/307 | Loss: 0.7325 | Batch acc: 0.84\n",
      "  Step 230/307 | Loss: 0.4288 | Batch acc: 0.97\n",
      "  Step 240/307 | Loss: 3.2030 | Batch acc: 0.06\n",
      "  Step 250/307 | Loss: 3.8944 | Batch acc: 0.16\n",
      "  Step 260/307 | Loss: 0.4510 | Batch acc: 1.00\n",
      "  Step 270/307 | Loss: 0.3637 | Batch acc: 1.00\n",
      "  Step 280/307 | Loss: 0.3551 | Batch acc: 1.00\n",
      "  Step 290/307 | Loss: 0.3597 | Batch acc: 1.00\n",
      "  Step 300/307 | Loss: 0.3585 | Batch acc: 1.00\n",
      "Val:   loss=0.9173, acc=0.761\n",
      "  âœ… New best model saved â†’ /bhome/fwag/ele670_project/results/tiny_resnet50_best.pt (acc=0.761)\n",
      "\n",
      "Validation Confusion Matrix (rows=true, cols=pred):\n",
      "[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0   11    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    3    0    7    0   26    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    2    3    0    2    0  220  629    3  701    0    9   63   15]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0   18    0    0    0    0    8  131   11 6712    0  624   50   13]\n",
      " [   0    0    0    0    0    0   12    8    0   35    0    0    0    0]\n",
      " [   0    2    0    0    0    0    3   13    0  107    0  122   15    1]\n",
      " [   0    0    0    0    0    0    1  211    0   14    0    0   12    1]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0]]\n",
      "\n",
      "Per-class metrics (validation):\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "    Ampulla of Vater      0.000     0.000     0.000         0\n",
      "         Angiectasia      0.000     0.000     0.000        11\n",
      "       Blood - fresh      0.000     0.000     0.000         0\n",
      "     Blood - hematin      0.000     0.000     0.000         0\n",
      "             Erosion      0.000     0.000     0.000        36\n",
      "            Erythema      0.000     0.000     0.000         0\n",
      "        Foreign Body      0.000     0.000     0.000         0\n",
      "     Ileocecal valve      0.632     0.382     0.476      1647\n",
      "    Lymphangiectasia      0.000     0.000     0.000         0\n",
      " Normal clean mucosa      0.885     0.887     0.886      7567\n",
      "            class_10      0.000     0.000     0.000        55\n",
      "             Pylorus      0.156     0.464     0.234       263\n",
      "Reduced Mucosal View      0.086     0.050     0.063       239\n",
      "               Ulcer      0.000     0.000     0.000         0\n",
      "\n",
      "            accuracy                          0.761      9818\n",
      "           macro avg      0.126     0.127     0.119      9818\n",
      "        weighted avg      0.794     0.761     0.770      9818\n",
      "\n",
      "\n",
      "Training finished!\n",
      "Best validation accuracy: 0.761\n",
      "Elapsed time: 604.6 sec\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # Tiny ResNet50 sanity trainer for Kvasir-Capsule (debug-friendly)\n",
    "# \n",
    "# * Beginner-friendly, heavily commented\n",
    "# * Safe, small training run (2 epochs) to check end-to-end wiring\n",
    "# * Works well on Tesla V100 (uses mixed precision)\n",
    "# * Video-independent split (by `video_id`) to avoid leakage\n",
    "# * Extra debug prints (device, memory, batch shapes, step logs)\n",
    "# * Optional: freeze backbone for very fast convergence of the classifier head\n",
    "# * Validation summary includes confusion matrix + per-class metrics\n",
    "# \n",
    "# âž• Changes vs earlier cell:\n",
    "# - Uses **new AMP API**: `from torch import amp; amp.autocast('cuda')` to avoid deprecation warnings\n",
    "# - Adds **label smoothing** + optional **weight decay** for stability\n",
    "# - Adds **GPU-2 pinning** (can be disabled by setting GPU_INDEX=None)\n",
    "# - Adds **confusion matrix** + **classification report** after validation\n",
    "# \n",
    "# ---\n",
    "\n",
    "# %%\n",
    "import os, time, math, warnings\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models\n",
    "from torch import amp\n",
    "\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "CSV_PATH      = \"/bhome/fwag/ele670_project/data/processed/metadata_normalized.csv\"\n",
    "IMG_ROOT      = \"/bhome/fwag/ele670_project/data/raw/labelled_images\"\n",
    "RESULTS_DIR   = \"/bhome/fwag/ele670_project/results\"\n",
    "SAVE_PATH     = os.path.join(RESULTS_DIR, \"tiny_resnet50_best.pt\")\n",
    "\n",
    "IMG_SIZE      = 224\n",
    "BATCH_SIZE    = 32          # Try 64 or 96 if you want to use more VRAM\n",
    "NUM_WORKERS   = 6           # Tune for your server (4-12 typically fine)\n",
    "EPOCHS        = 2           # Keep it short for sanity checks\n",
    "LR            = 1e-3\n",
    "WEIGHT_DECAY  = 1e-4        # small regularization\n",
    "LABEL_SMOOTH  = 0.05        # improves stability a bit\n",
    "FREEZE_BACKBONE = True     # set True for super-fast head-only training\n",
    "USE_PRETRAINED = True       # use ImageNet weights to converge fast\n",
    "SEED          = 42\n",
    "\n",
    "# If you want to lock to a specific GPU (e.g., GPU 2), set this to an int.\n",
    "# Set to None to use the default CUDA device.\n",
    "GPU_INDEX     = 2\n",
    "\n",
    "# Log every N steps for train/val loops\n",
    "LOG_EVERY     = 10\n",
    "\n",
    "# ----------------------------------------\n",
    "# (1) Reproducibility\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# (2) Pin to a specific GPU before creating tensors/models\n",
    "if torch.cuda.is_available() and GPU_INDEX is not None:\n",
    "    # Option A (recommended for notebooks): hide other GPUs so chosen one is cuda:0\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(GPU_INDEX)\n",
    "\n",
    "# After potentially setting CUDA_VISIBLE_DEVICES, re-check availability\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "if DEVICE == \"cuda\":\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print(\"Allocated:\", round(torch.cuda.memory_allocated()/1024**3, 2), \"GB\")\n",
    "    print(\"Reserved: \", round(torch.cuda.memory_reserved()/1024**3, 2), \"GB\")\n",
    "\n",
    "# Create results directory if missing\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "# Enable cuDNN heuristics (faster for fixed shapes)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "try:\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# --------------- DATASET ----------------\n",
    "class KvasirDataset(Dataset):\n",
    "    \"\"\"Reads image paths + encoded labels from a dataframe.\n",
    "    \n",
    "    Returns (image_tensor, label_int).\n",
    "    If an image fails to load, returns a black placeholder and prints a warning.\n",
    "    \"\"\"\n",
    "    def __init__(self, df: pd.DataFrame, img_root: str, transform):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.img_root = img_root\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        rel = row[\"image_path\"]\n",
    "        path = os.path.join(self.img_root, rel)\n",
    "        try:\n",
    "            img = Image.open(path).convert(\"RGB\")\n",
    "        except (FileNotFoundError, UnidentifiedImageError) as e:\n",
    "            print(f\"[WARNING] Failed to open {path}: {e}\")\n",
    "            img = Image.new(\"RGB\", (IMG_SIZE, IMG_SIZE), (0, 0, 0))\n",
    "        x = self.transform(img)\n",
    "        y = int(row[\"encoded_label\"])  # already numeric\n",
    "        return x, y\n",
    "\n",
    "# --------------- LOAD CSV ----------------\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "print(\"First rows of metadata_normalized.csv:\")\n",
    "print(df.head())\n",
    "\n",
    "# Basic validations\n",
    "required_cols = {\"image_path\", \"encoded_label\", \"video_id\"}\n",
    "missing = required_cols - set(df.columns)\n",
    "assert not missing, f\"CSV is missing required columns: {missing}\"\n",
    "\n",
    "num_classes = df[\"encoded_label\"].nunique()\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "\n",
    "# Try to recover (optional) human-readable names per label for metrics display\n",
    "# If multiple names map to same label, we pick the most frequent one in train split later.\n",
    "if \"finding_class\" in df.columns:\n",
    "    # Placeholder; will finalize names after split so we use train frequency\n",
    "    label_to_names = None\n",
    "else:\n",
    "    label_to_names = None\n",
    "\n",
    "# --------------- TRAIN/VAL SPLIT (video-independent) ----------------\n",
    "gss = GroupShuffleSplit(n_splits=1, train_size=0.8, random_state=SEED)\n",
    "train_idx, val_idx = next(gss.split(df, groups=df[\"video_id\"]))\n",
    "df_train, df_val = df.iloc[train_idx].copy(), df.iloc[val_idx].copy()\n",
    "\n",
    "print(f\"Train samples: {len(df_train)} | Val samples: {len(df_val)}\")\n",
    "print(f\"Train videos: {df_train['video_id'].nunique()} | Val videos: {df_val['video_id'].nunique()}\")\n",
    "\n",
    "# Now finalize readable class names if available\n",
    "if \"finding_class\" in df.columns:\n",
    "    # Map each encoded_label to its most common finding_class in the TRAIN set\n",
    "    names = (\n",
    "        df_train.groupby([\"encoded_label\", \"finding_class\"]).size()\n",
    "        .reset_index(name=\"count\")\n",
    "        .sort_values([\"encoded_label\", \"count\"], ascending=[True, False])\n",
    "        .drop_duplicates(subset=[\"encoded_label\"])\n",
    "    )\n",
    "    label_to_names = dict(zip(names[\"encoded_label\"].astype(int), names[\"finding_class\"].astype(str)))\n",
    "else:\n",
    "    label_to_names = {i: f\"class_{i}\" for i in range(num_classes)}\n",
    "\n",
    "# --------------- TRANSFORMS ----------------\n",
    "mean = (0.485, 0.456, 0.406)\n",
    "std  = (0.229, 0.224, 0.225)\n",
    "\n",
    "train_tf = T.Compose([\n",
    "    T.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    T.RandomHorizontalFlip(p=0.5),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean, std),\n",
    "])\n",
    "\n",
    "val_tf = T.Compose([\n",
    "    T.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean, std),\n",
    "])\n",
    "\n",
    "# --------------- DATA LOADERS ----------------\n",
    "train_ds = KvasirDataset(df_train, IMG_ROOT, train_tf)\n",
    "val_ds   = KvasirDataset(df_val,   IMG_ROOT, val_tf)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "    num_workers=NUM_WORKERS, pin_memory=True,\n",
    "    persistent_workers=True, prefetch_factor=4, drop_last=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "    num_workers=NUM_WORKERS, pin_memory=True,\n",
    "    persistent_workers=True, prefetch_factor=4, drop_last=False\n",
    ")\n",
    "\n",
    "print(\"One training batch (images, labels):\")\n",
    "sample_imgs, sample_labels = next(iter(train_loader))\n",
    "print(\"Image batch shape:\", sample_imgs.shape)   # [B, 3, H, W]\n",
    "print(\"Label batch shape:\", sample_labels.shape) # [B]\n",
    "\n",
    "# --------------- MODEL ----------------\n",
    "if USE_PRETRAINED:\n",
    "    try:\n",
    "        weights = models.ResNet50_Weights.IMAGENET1K_V2\n",
    "    except AttributeError:\n",
    "        weights = \"IMAGENET1K_V2\"\n",
    "    model = models.resnet50(weights=weights)\n",
    "else:\n",
    "    model = models.resnet50(weights=None)\n",
    "\n",
    "# Replace the classifier head to match your dataset\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "\n",
    "# Optionally freeze the backbone (train only the final classifier layer)\n",
    "if FREEZE_BACKBONE:\n",
    "    for name, p in model.named_parameters():\n",
    "        if not name.startswith(\"fc.\"):\n",
    "            p.requires_grad_(False)\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "# --------------- LOSS & OPTIMIZER ---------------\n",
    "# Label smoothing helps generalization slightly and prevents over-confident spikes\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=LABEL_SMOOTH)\n",
    "\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.Adam(params, lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# Mixed precision scaler (new API)\n",
    "scaler = amp.GradScaler('cuda') if DEVICE == 'cuda' else None\n",
    "\n",
    "# --------------- TRAIN / EVAL HELPERS ---------------\n",
    "@torch.no_grad()\n",
    "def _accuracy(logits: torch.Tensor, labels: torch.Tensor) -> float:\n",
    "    return (logits.argmax(dim=1) == labels).float().mean().item()\n",
    "\n",
    "\n",
    "def run_epoch(loader, train: bool = True, desc: str = \"train\"):\n",
    "    model.train(train)\n",
    "\n",
    "    total_loss, total_correct, total_seen = 0.0, 0, 0\n",
    "\n",
    "    for step, (images, labels) in enumerate(loader, 1):\n",
    "        images = images.to(DEVICE, non_blocking=True)\n",
    "        labels = labels.to(DEVICE, non_blocking=True)\n",
    "\n",
    "        # Forward (mixed precision if on CUDA)\n",
    "        if DEVICE == 'cuda':\n",
    "            autocast_ctx = amp.autocast('cuda', enabled=True)\n",
    "        else:\n",
    "            # No AMP on CPU\n",
    "            from contextlib import nullcontext\n",
    "            autocast_ctx = nullcontext()\n",
    "\n",
    "        with autocast_ctx:\n",
    "            logits = model(images)\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "        if train:\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            if scaler is not None:\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        # Statistics (computed in full precision)\n",
    "        with torch.no_grad():\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct = (preds == labels).sum().item()\n",
    "            batch_sz = labels.size(0)\n",
    "            total_correct += correct\n",
    "            total_seen += batch_sz\n",
    "            total_loss += float(loss.item()) * batch_sz\n",
    "\n",
    "        # Debug print\n",
    "        if step % LOG_EVERY == 0 or step == 1:\n",
    "            batch_acc = correct / batch_sz\n",
    "            print(f\"  Step {step}/{len(loader)} | Loss: {loss.item():.4f} | Batch acc: {batch_acc:.2f}\")\n",
    "\n",
    "    avg_loss = total_loss / max(1, total_seen)\n",
    "    avg_acc  = total_correct / max(1, total_seen)\n",
    "    return avg_loss, avg_acc\n",
    "\n",
    "\n",
    "def collect_predictions(loader):\n",
    "    \"\"\"Run model in eval mode and collect logits/labels for metrics.\n",
    "    Returns: y_true (np.array), y_pred (np.array)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds  = []\n",
    "\n",
    "    for images, labels in loader:\n",
    "        images = images.to(DEVICE, non_blocking=True)\n",
    "        labels = labels.to(DEVICE, non_blocking=True)\n",
    "        with torch.no_grad():\n",
    "            if DEVICE == 'cuda':\n",
    "                with amp.autocast('cuda', enabled=True):\n",
    "                    logits = model(images)\n",
    "            else:\n",
    "                logits = model(images)\n",
    "            preds = logits.argmax(dim=1)\n",
    "        all_labels.append(labels.cpu().numpy())\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "\n",
    "    y_true = np.concatenate(all_labels)\n",
    "    y_pred = np.concatenate(all_preds)\n",
    "    return y_true, y_pred\n",
    "\n",
    "# --------------- TRAIN FOR A FEW EPOCHS ---------------\n",
    "print(\"\\n==== START TRAINING ====\\n\")\n",
    "best_val_acc = 0.0\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    print(f\"\\nEpoch {epoch}/{EPOCHS}\")\n",
    "\n",
    "    tr_loss, tr_acc = run_epoch(train_loader, train=True, desc=\"train\")\n",
    "    print(f\"Train: loss={tr_loss:.4f}, acc={tr_acc:.3f}\")\n",
    "\n",
    "    vl_loss, vl_acc = run_epoch(val_loader, train=False, desc=\"val\")\n",
    "    print(f\"Val:   loss={vl_loss:.4f}, acc={vl_acc:.3f}\")\n",
    "\n",
    "    # Save best checkpoint\n",
    "    if vl_acc > best_val_acc:\n",
    "        best_val_acc = vl_acc\n",
    "        torch.save(model.state_dict(), SAVE_PATH)\n",
    "        print(f\"  âœ… New best model saved â†’ {SAVE_PATH} (acc={best_val_acc:.3f})\")\n",
    "\n",
    "    # After each epoch, print confusion matrix + classification report\n",
    "    y_true, y_pred = collect_predictions(val_loader)\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=list(range(num_classes)))\n",
    "    print(\"\\nValidation Confusion Matrix (rows=true, cols=pred):\")\n",
    "    # Pretty-print small CM; for large, you might want to save to file\n",
    "    with np.printoptions(threshold=200, linewidth=200):\n",
    "        print(cm)\n",
    "\n",
    "    target_names = [label_to_names.get(i, f\"class_{i}\") for i in range(num_classes)]\n",
    "    print(\"\\nPer-class metrics (validation):\")\n",
    "    print(classification_report(y_true, y_pred, labels=list(range(num_classes)), target_names=target_names, digits=3))\n",
    "\n",
    "print(\"\\nTraining finished!\")\n",
    "print(\"Best validation accuracy:\", round(best_val_acc, 3))\n",
    "print(\"Elapsed time: %.1f sec\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4ab3012b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Tesla V100-PCIE-32GB\n",
      "Allocated: 0.1 GB\n",
      "Reserved:  0.41 GB\n",
      "First rows of metadata_normalized.csv:\n",
      "                                       image_path        finding_class  \\\n",
      "0  Normal clean mucosa/0728084c8da942d9_22803.jpg  Normal clean mucosa   \n",
      "1  Normal clean mucosa/0728084c8da942d9_22804.jpg  Normal clean mucosa   \n",
      "2  Normal clean mucosa/0728084c8da942d9_22805.jpg  Normal clean mucosa   \n",
      "3  Normal clean mucosa/0728084c8da942d9_22806.jpg  Normal clean mucosa   \n",
      "4  Normal clean mucosa/0728084c8da942d9_22807.jpg  Normal clean mucosa   \n",
      "\n",
      "           video_id  encoded_label  \n",
      "0  0728084c8da942d9              7  \n",
      "1  0728084c8da942d9              7  \n",
      "2  0728084c8da942d9              7  \n",
      "3  0728084c8da942d9              7  \n",
      "4  0728084c8da942d9              7  \n",
      "Number of classes: 11\n",
      "Train samples: 37408 | Val samples: 9763\n",
      "Train videos: 34 | Val videos: 9\n",
      "Loaded 11 class names from /home/stud/fwag/bhome/ele670_project/data/processed/label_classes_filtered.npy\n",
      "One training batch (images, labels):\n",
      "Image batch shape: torch.Size([32, 3, 224, 224])\n",
      "Label batch shape: torch.Size([32])\n",
      "\n",
      "==== START TRAINING ====\n",
      "\n",
      "\n",
      "Epoch 1/2\n",
      "  Step 1/1169 | Loss: 2.5986 | Batch acc: 0.03\n",
      "  Step 10/1169 | Loss: 0.9513 | Batch acc: 0.84\n",
      "  Step 20/1169 | Loss: 0.9631 | Batch acc: 0.84\n",
      "  Step 30/1169 | Loss: 1.2371 | Batch acc: 0.75\n",
      "  Step 40/1169 | Loss: 0.8889 | Batch acc: 0.88\n",
      "  Step 50/1169 | Loss: 1.3224 | Batch acc: 0.69\n",
      "  Step 60/1169 | Loss: 1.2462 | Batch acc: 0.69\n",
      "  Step 70/1169 | Loss: 1.1595 | Batch acc: 0.72\n",
      "  Step 80/1169 | Loss: 1.4152 | Batch acc: 0.66\n",
      "  Step 90/1169 | Loss: 1.3908 | Batch acc: 0.62\n",
      "  Step 100/1169 | Loss: 0.7551 | Batch acc: 0.91\n",
      "  Step 110/1169 | Loss: 0.8768 | Batch acc: 0.81\n",
      "  Step 120/1169 | Loss: 0.9027 | Batch acc: 0.81\n",
      "  Step 130/1169 | Loss: 0.9441 | Batch acc: 0.72\n",
      "  Step 140/1169 | Loss: 0.8187 | Batch acc: 0.75\n",
      "  Step 150/1169 | Loss: 0.8277 | Batch acc: 0.88\n",
      "  Step 160/1169 | Loss: 0.9141 | Batch acc: 0.78\n",
      "  Step 170/1169 | Loss: 0.7697 | Batch acc: 0.84\n",
      "  Step 180/1169 | Loss: 0.7732 | Batch acc: 0.78\n",
      "  Step 190/1169 | Loss: 0.7971 | Batch acc: 0.81\n",
      "  Step 200/1169 | Loss: 0.8423 | Batch acc: 0.84\n",
      "  Step 210/1169 | Loss: 0.8377 | Batch acc: 0.78\n",
      "  Step 220/1169 | Loss: 1.2076 | Batch acc: 0.56\n",
      "  Step 230/1169 | Loss: 0.9440 | Batch acc: 0.69\n",
      "  Step 240/1169 | Loss: 1.1873 | Batch acc: 0.56\n",
      "  Step 250/1169 | Loss: 0.7519 | Batch acc: 0.91\n",
      "  Step 260/1169 | Loss: 0.6536 | Batch acc: 0.88\n",
      "  Step 270/1169 | Loss: 0.8539 | Batch acc: 0.81\n",
      "  Step 280/1169 | Loss: 0.7006 | Batch acc: 0.88\n",
      "  Step 290/1169 | Loss: 0.7458 | Batch acc: 0.91\n",
      "  Step 300/1169 | Loss: 0.6896 | Batch acc: 0.81\n",
      "  Step 310/1169 | Loss: 0.8741 | Batch acc: 0.81\n",
      "  Step 320/1169 | Loss: 0.9013 | Batch acc: 0.72\n",
      "  Step 330/1169 | Loss: 0.7289 | Batch acc: 0.84\n",
      "  Step 340/1169 | Loss: 0.7062 | Batch acc: 0.88\n",
      "  Step 350/1169 | Loss: 0.6512 | Batch acc: 0.91\n",
      "  Step 360/1169 | Loss: 0.6537 | Batch acc: 0.88\n",
      "  Step 370/1169 | Loss: 0.7245 | Batch acc: 0.81\n",
      "  Step 380/1169 | Loss: 0.7499 | Batch acc: 0.81\n",
      "  Step 390/1169 | Loss: 0.7732 | Batch acc: 0.81\n",
      "  Step 400/1169 | Loss: 0.5116 | Batch acc: 0.94\n",
      "  Step 410/1169 | Loss: 0.6908 | Batch acc: 0.88\n",
      "  Step 420/1169 | Loss: 0.6338 | Batch acc: 0.88\n",
      "  Step 430/1169 | Loss: 0.5842 | Batch acc: 0.94\n",
      "  Step 440/1169 | Loss: 0.5884 | Batch acc: 0.91\n",
      "  Step 450/1169 | Loss: 0.8397 | Batch acc: 0.78\n",
      "  Step 460/1169 | Loss: 0.6986 | Batch acc: 0.88\n",
      "  Step 470/1169 | Loss: 0.6048 | Batch acc: 0.91\n",
      "  Step 480/1169 | Loss: 0.7068 | Batch acc: 0.84\n",
      "  Step 490/1169 | Loss: 0.7571 | Batch acc: 0.81\n",
      "  Step 500/1169 | Loss: 0.6742 | Batch acc: 0.84\n",
      "  Step 510/1169 | Loss: 0.5759 | Batch acc: 0.91\n",
      "  Step 520/1169 | Loss: 0.7327 | Batch acc: 0.88\n",
      "  Step 530/1169 | Loss: 0.9768 | Batch acc: 0.69\n",
      "  Step 540/1169 | Loss: 0.7171 | Batch acc: 0.91\n",
      "  Step 550/1169 | Loss: 0.6504 | Batch acc: 0.84\n",
      "  Step 560/1169 | Loss: 0.6615 | Batch acc: 0.88\n",
      "  Step 570/1169 | Loss: 0.7840 | Batch acc: 0.75\n",
      "  Step 580/1169 | Loss: 0.7041 | Batch acc: 0.84\n",
      "  Step 590/1169 | Loss: 0.8416 | Batch acc: 0.84\n",
      "  Step 600/1169 | Loss: 0.7519 | Batch acc: 0.81\n",
      "  Step 610/1169 | Loss: 0.7894 | Batch acc: 0.78\n",
      "  Step 620/1169 | Loss: 0.7400 | Batch acc: 0.84\n",
      "  Step 630/1169 | Loss: 0.8401 | Batch acc: 0.78\n",
      "  Step 640/1169 | Loss: 1.0388 | Batch acc: 0.69\n",
      "  Step 650/1169 | Loss: 0.5251 | Batch acc: 0.91\n",
      "  Step 660/1169 | Loss: 0.6007 | Batch acc: 0.84\n",
      "  Step 670/1169 | Loss: 0.9898 | Batch acc: 0.78\n",
      "  Step 680/1169 | Loss: 0.6974 | Batch acc: 0.81\n",
      "  Step 690/1169 | Loss: 0.8627 | Batch acc: 0.78\n",
      "  Step 700/1169 | Loss: 0.6515 | Batch acc: 0.94\n",
      "  Step 710/1169 | Loss: 0.7650 | Batch acc: 0.78\n",
      "  Step 720/1169 | Loss: 0.9319 | Batch acc: 0.84\n",
      "  Step 730/1169 | Loss: 0.5297 | Batch acc: 0.97\n",
      "  Step 740/1169 | Loss: 0.9044 | Batch acc: 0.72\n",
      "  Step 750/1169 | Loss: 0.8072 | Batch acc: 0.78\n",
      "  Step 760/1169 | Loss: 0.6709 | Batch acc: 0.84\n",
      "  Step 770/1169 | Loss: 0.5710 | Batch acc: 0.94\n",
      "  Step 780/1169 | Loss: 0.8377 | Batch acc: 0.72\n",
      "  Step 790/1169 | Loss: 0.6338 | Batch acc: 0.84\n",
      "  Step 800/1169 | Loss: 0.7187 | Batch acc: 0.81\n",
      "  Step 810/1169 | Loss: 0.5693 | Batch acc: 0.97\n",
      "  Step 820/1169 | Loss: 0.6458 | Batch acc: 0.91\n",
      "  Step 830/1169 | Loss: 0.6442 | Batch acc: 0.81\n",
      "  Step 840/1169 | Loss: 0.6510 | Batch acc: 0.88\n",
      "  Step 850/1169 | Loss: 0.6727 | Batch acc: 0.84\n",
      "  Step 860/1169 | Loss: 0.7114 | Batch acc: 0.91\n",
      "  Step 870/1169 | Loss: 0.6673 | Batch acc: 0.88\n",
      "  Step 880/1169 | Loss: 0.6362 | Batch acc: 0.88\n",
      "  Step 890/1169 | Loss: 0.5638 | Batch acc: 0.91\n",
      "  Step 900/1169 | Loss: 0.5410 | Batch acc: 0.94\n",
      "  Step 910/1169 | Loss: 0.4856 | Batch acc: 1.00\n",
      "  Step 920/1169 | Loss: 0.5251 | Batch acc: 0.94\n",
      "  Step 930/1169 | Loss: 0.8100 | Batch acc: 0.81\n",
      "  Step 940/1169 | Loss: 0.6323 | Batch acc: 0.91\n",
      "  Step 950/1169 | Loss: 0.4792 | Batch acc: 0.94\n",
      "  Step 960/1169 | Loss: 0.6123 | Batch acc: 0.88\n",
      "  Step 970/1169 | Loss: 0.6523 | Batch acc: 0.91\n",
      "  Step 980/1169 | Loss: 0.6004 | Batch acc: 0.81\n",
      "  Step 990/1169 | Loss: 0.8642 | Batch acc: 0.78\n",
      "  Step 1000/1169 | Loss: 0.5774 | Batch acc: 0.91\n",
      "  Step 1010/1169 | Loss: 0.6340 | Batch acc: 0.88\n",
      "  Step 1020/1169 | Loss: 0.7606 | Batch acc: 0.84\n",
      "  Step 1030/1169 | Loss: 0.6052 | Batch acc: 0.94\n",
      "  Step 1040/1169 | Loss: 0.6914 | Batch acc: 0.84\n",
      "  Step 1050/1169 | Loss: 0.4899 | Batch acc: 0.94\n",
      "  Step 1060/1169 | Loss: 0.5378 | Batch acc: 0.88\n",
      "  Step 1070/1169 | Loss: 0.7250 | Batch acc: 0.88\n",
      "  Step 1080/1169 | Loss: 0.5717 | Batch acc: 0.91\n",
      "  Step 1090/1169 | Loss: 0.5584 | Batch acc: 0.91\n",
      "  Step 1100/1169 | Loss: 0.6720 | Batch acc: 0.94\n",
      "  Step 1110/1169 | Loss: 0.7366 | Batch acc: 0.84\n",
      "  Step 1120/1169 | Loss: 0.4440 | Batch acc: 0.97\n",
      "  Step 1130/1169 | Loss: 0.5727 | Batch acc: 0.88\n",
      "  Step 1140/1169 | Loss: 0.7850 | Batch acc: 0.78\n",
      "  Step 1150/1169 | Loss: 0.5372 | Batch acc: 0.97\n",
      "  Step 1160/1169 | Loss: 0.5854 | Batch acc: 0.91\n",
      "Train: loss=0.7678, acc=0.839\n",
      "  Step 1/306 | Loss: 1.7083 | Batch acc: 0.38\n",
      "  Step 10/306 | Loss: 0.6412 | Batch acc: 0.88\n",
      "  Step 20/306 | Loss: 0.3348 | Batch acc: 1.00\n",
      "  Step 30/306 | Loss: 0.3313 | Batch acc: 1.00\n",
      "  Step 40/306 | Loss: 0.3389 | Batch acc: 1.00\n",
      "  Step 50/306 | Loss: 0.9675 | Batch acc: 0.66\n",
      "  Step 60/306 | Loss: 1.0873 | Batch acc: 0.50\n",
      "  Step 70/306 | Loss: 0.3867 | Batch acc: 1.00\n",
      "  Step 80/306 | Loss: 0.3319 | Batch acc: 1.00\n",
      "  Step 90/306 | Loss: 2.1648 | Batch acc: 0.06\n",
      "  Step 100/306 | Loss: 1.2979 | Batch acc: 0.66\n",
      "  Step 110/306 | Loss: 1.9222 | Batch acc: 0.22\n",
      "  Step 120/306 | Loss: 1.6266 | Batch acc: 0.44\n",
      "  Step 130/306 | Loss: 1.0588 | Batch acc: 0.69\n",
      "  Step 140/306 | Loss: 0.5774 | Batch acc: 0.94\n",
      "  Step 150/306 | Loss: 0.4190 | Batch acc: 0.97\n",
      "  Step 160/306 | Loss: 0.3691 | Batch acc: 1.00\n",
      "  Step 170/306 | Loss: 0.7915 | Batch acc: 0.75\n",
      "  Step 180/306 | Loss: 0.4716 | Batch acc: 1.00\n",
      "  Step 190/306 | Loss: 0.8984 | Batch acc: 0.75\n",
      "  Step 200/306 | Loss: 0.4285 | Batch acc: 1.00\n",
      "  Step 210/306 | Loss: 0.3419 | Batch acc: 1.00\n",
      "  Step 220/306 | Loss: 0.7129 | Batch acc: 0.88\n",
      "  Step 230/306 | Loss: 0.5579 | Batch acc: 0.97\n",
      "  Step 240/306 | Loss: 2.6023 | Batch acc: 0.03\n",
      "  Step 250/306 | Loss: 3.0413 | Batch acc: 0.00\n",
      "  Step 260/306 | Loss: 0.3955 | Batch acc: 1.00\n",
      "  Step 270/306 | Loss: 0.3785 | Batch acc: 1.00\n",
      "  Step 280/306 | Loss: 0.3643 | Batch acc: 1.00\n",
      "  Step 290/306 | Loss: 0.3324 | Batch acc: 1.00\n",
      "  Step 300/306 | Loss: 0.3393 | Batch acc: 1.00\n",
      "Val:   loss=0.8789, acc=0.757\n",
      "  âœ… New best model saved â†’ /home/stud/fwag/bhome/ele670_project/results/tiny_resnet50_best.pt (acc=0.757)\n",
      "\n",
      "Validation Confusion Matrix (rows=true, cols=pred):\n",
      "[[   0    0    0    0    0    0    0   11    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    1    4    0   18   12    0    1]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   3    1    0    0  376  497    3  662    1   59   45]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [  15    0    0    0   35  116   23 6821  489   54   14]\n",
      " [   1    0    0    0   36   14    0  125   67   17    3]\n",
      " [   0    0    0    0    5  203    0   17    0   10    4]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]]\n",
      "\n",
      "Per-class metrics (validation):\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "         Angiectasia      0.000     0.000     0.000        11\n",
      "       Blood - fresh      0.000     0.000     0.000         0\n",
      "             Erosion      0.000     0.000     0.000        36\n",
      "            Erythema      0.000     0.000     0.000         0\n",
      "        Foreign Body      0.000     0.000     0.000         0\n",
      "     Ileocecal valve      0.596     0.302     0.401      1647\n",
      "    Lymphangiectasia      0.000     0.000     0.000         0\n",
      " Normal clean mucosa      0.891     0.901     0.896      7567\n",
      "             Pylorus      0.118     0.255     0.161       263\n",
      "Reduced Mucosal View      0.071     0.042     0.053       239\n",
      "               Ulcer      0.000     0.000     0.000         0\n",
      "\n",
      "            accuracy                          0.757      9763\n",
      "           macro avg      0.152     0.136     0.137      9763\n",
      "        weighted avg      0.796     0.757     0.768      9763\n",
      "\n",
      "\n",
      "Epoch 2/2\n",
      "  Step 1/1169 | Loss: 0.5882 | Batch acc: 0.84\n",
      "  Step 10/1169 | Loss: 0.6664 | Batch acc: 0.88\n",
      "  Step 20/1169 | Loss: 0.5070 | Batch acc: 0.94\n",
      "  Step 30/1169 | Loss: 0.6282 | Batch acc: 0.84\n",
      "  Step 40/1169 | Loss: 0.4660 | Batch acc: 0.97\n",
      "  Step 50/1169 | Loss: 0.4440 | Batch acc: 0.94\n",
      "  Step 60/1169 | Loss: 0.5836 | Batch acc: 0.94\n",
      "  Step 70/1169 | Loss: 0.6036 | Batch acc: 0.84\n",
      "  Step 80/1169 | Loss: 0.6775 | Batch acc: 0.88\n",
      "  Step 90/1169 | Loss: 0.5401 | Batch acc: 0.94\n",
      "  Step 100/1169 | Loss: 0.5312 | Batch acc: 0.94\n",
      "  Step 110/1169 | Loss: 0.6764 | Batch acc: 0.81\n",
      "  Step 120/1169 | Loss: 0.6598 | Batch acc: 0.91\n",
      "  Step 130/1169 | Loss: 0.4753 | Batch acc: 0.94\n",
      "  Step 140/1169 | Loss: 0.4117 | Batch acc: 1.00\n",
      "  Step 150/1169 | Loss: 0.7853 | Batch acc: 0.81\n",
      "  Step 160/1169 | Loss: 0.5883 | Batch acc: 0.94\n",
      "  Step 170/1169 | Loss: 0.5502 | Batch acc: 0.94\n",
      "  Step 180/1169 | Loss: 0.5407 | Batch acc: 0.94\n",
      "  Step 190/1169 | Loss: 0.8533 | Batch acc: 0.75\n",
      "  Step 200/1169 | Loss: 0.6057 | Batch acc: 0.91\n",
      "  Step 210/1169 | Loss: 0.4489 | Batch acc: 0.97\n",
      "  Step 220/1169 | Loss: 0.8366 | Batch acc: 0.78\n",
      "  Step 230/1169 | Loss: 0.4263 | Batch acc: 1.00\n",
      "  Step 240/1169 | Loss: 0.5613 | Batch acc: 0.94\n",
      "  Step 250/1169 | Loss: 0.4104 | Batch acc: 1.00\n",
      "  Step 260/1169 | Loss: 0.6429 | Batch acc: 0.88\n",
      "  Step 270/1169 | Loss: 0.6247 | Batch acc: 0.88\n",
      "  Step 280/1169 | Loss: 0.7493 | Batch acc: 0.84\n",
      "  Step 290/1169 | Loss: 0.4911 | Batch acc: 0.97\n",
      "  Step 300/1169 | Loss: 0.5740 | Batch acc: 0.94\n",
      "  Step 310/1169 | Loss: 0.5902 | Batch acc: 0.91\n",
      "  Step 320/1169 | Loss: 0.6423 | Batch acc: 0.88\n",
      "  Step 330/1169 | Loss: 0.5474 | Batch acc: 0.91\n",
      "  Step 340/1169 | Loss: 0.4817 | Batch acc: 0.97\n",
      "  Step 350/1169 | Loss: 0.7197 | Batch acc: 0.81\n",
      "  Step 360/1169 | Loss: 0.4764 | Batch acc: 0.97\n",
      "  Step 370/1169 | Loss: 0.5957 | Batch acc: 0.94\n",
      "  Step 380/1169 | Loss: 0.5896 | Batch acc: 0.88\n",
      "  Step 390/1169 | Loss: 0.6374 | Batch acc: 0.94\n",
      "  Step 400/1169 | Loss: 0.5162 | Batch acc: 0.94\n",
      "  Step 410/1169 | Loss: 0.4485 | Batch acc: 0.97\n",
      "  Step 420/1169 | Loss: 0.6269 | Batch acc: 0.94\n",
      "  Step 430/1169 | Loss: 0.6128 | Batch acc: 0.84\n",
      "  Step 440/1169 | Loss: 0.4823 | Batch acc: 0.94\n",
      "  Step 450/1169 | Loss: 0.5861 | Batch acc: 0.84\n",
      "  Step 460/1169 | Loss: 0.6243 | Batch acc: 0.94\n",
      "  Step 470/1169 | Loss: 0.4537 | Batch acc: 1.00\n",
      "  Step 480/1169 | Loss: 0.6066 | Batch acc: 0.94\n",
      "  Step 490/1169 | Loss: 0.7908 | Batch acc: 0.81\n",
      "  Step 500/1169 | Loss: 0.6387 | Batch acc: 0.91\n",
      "  Step 510/1169 | Loss: 0.6184 | Batch acc: 0.88\n",
      "  Step 520/1169 | Loss: 0.5520 | Batch acc: 0.88\n",
      "  Step 530/1169 | Loss: 0.5844 | Batch acc: 0.94\n",
      "  Step 540/1169 | Loss: 0.6126 | Batch acc: 0.91\n",
      "  Step 550/1169 | Loss: 0.5103 | Batch acc: 0.94\n",
      "  Step 560/1169 | Loss: 0.5306 | Batch acc: 0.91\n",
      "  Step 570/1169 | Loss: 0.3796 | Batch acc: 1.00\n",
      "  Step 580/1169 | Loss: 0.6947 | Batch acc: 0.91\n",
      "  Step 590/1169 | Loss: 0.8222 | Batch acc: 0.78\n",
      "  Step 600/1169 | Loss: 0.4649 | Batch acc: 0.97\n",
      "  Step 610/1169 | Loss: 0.5971 | Batch acc: 0.91\n",
      "  Step 620/1169 | Loss: 0.5188 | Batch acc: 0.88\n",
      "  Step 630/1169 | Loss: 0.6279 | Batch acc: 0.94\n",
      "  Step 640/1169 | Loss: 0.5727 | Batch acc: 0.94\n",
      "  Step 650/1169 | Loss: 0.6689 | Batch acc: 0.88\n",
      "  Step 660/1169 | Loss: 0.4321 | Batch acc: 0.97\n",
      "  Step 670/1169 | Loss: 0.4769 | Batch acc: 0.94\n",
      "  Step 680/1169 | Loss: 0.7218 | Batch acc: 0.84\n",
      "  Step 690/1169 | Loss: 0.5393 | Batch acc: 0.97\n",
      "  Step 700/1169 | Loss: 0.7575 | Batch acc: 0.84\n",
      "  Step 710/1169 | Loss: 0.6458 | Batch acc: 0.88\n",
      "  Step 720/1169 | Loss: 0.6610 | Batch acc: 0.91\n",
      "  Step 730/1169 | Loss: 0.4490 | Batch acc: 0.94\n",
      "  Step 740/1169 | Loss: 0.6691 | Batch acc: 0.91\n",
      "  Step 750/1169 | Loss: 0.5592 | Batch acc: 0.88\n",
      "  Step 760/1169 | Loss: 0.5584 | Batch acc: 0.88\n",
      "  Step 770/1169 | Loss: 0.4654 | Batch acc: 0.97\n",
      "  Step 780/1169 | Loss: 0.5216 | Batch acc: 0.91\n",
      "  Step 790/1169 | Loss: 0.5831 | Batch acc: 0.91\n",
      "  Step 800/1169 | Loss: 0.4759 | Batch acc: 0.94\n",
      "  Step 810/1169 | Loss: 0.5688 | Batch acc: 0.84\n",
      "  Step 820/1169 | Loss: 0.6439 | Batch acc: 0.88\n",
      "  Step 830/1169 | Loss: 0.5034 | Batch acc: 0.94\n",
      "  Step 840/1169 | Loss: 0.5002 | Batch acc: 0.97\n",
      "  Step 850/1169 | Loss: 0.6226 | Batch acc: 0.91\n",
      "  Step 860/1169 | Loss: 0.5203 | Batch acc: 0.97\n",
      "  Step 870/1169 | Loss: 0.8092 | Batch acc: 0.84\n",
      "  Step 880/1169 | Loss: 0.6685 | Batch acc: 0.81\n",
      "  Step 890/1169 | Loss: 0.5385 | Batch acc: 0.97\n",
      "  Step 900/1169 | Loss: 0.6021 | Batch acc: 0.88\n",
      "  Step 910/1169 | Loss: 0.4931 | Batch acc: 1.00\n",
      "  Step 920/1169 | Loss: 0.4792 | Batch acc: 0.94\n",
      "  Step 930/1169 | Loss: 0.5382 | Batch acc: 0.94\n",
      "  Step 940/1169 | Loss: 0.5244 | Batch acc: 0.97\n",
      "  Step 950/1169 | Loss: 0.7272 | Batch acc: 0.84\n",
      "  Step 960/1169 | Loss: 0.5158 | Batch acc: 0.91\n",
      "  Step 970/1169 | Loss: 0.5482 | Batch acc: 0.94\n",
      "  Step 980/1169 | Loss: 0.5401 | Batch acc: 0.91\n",
      "  Step 990/1169 | Loss: 0.5883 | Batch acc: 0.88\n",
      "  Step 1000/1169 | Loss: 0.5443 | Batch acc: 0.88\n",
      "  Step 1010/1169 | Loss: 0.5068 | Batch acc: 0.94\n",
      "  Step 1020/1169 | Loss: 0.5379 | Batch acc: 0.94\n",
      "  Step 1030/1169 | Loss: 0.6081 | Batch acc: 0.88\n",
      "  Step 1040/1169 | Loss: 0.7161 | Batch acc: 0.84\n",
      "  Step 1050/1169 | Loss: 0.5745 | Batch acc: 0.91\n",
      "  Step 1060/1169 | Loss: 0.6414 | Batch acc: 0.91\n",
      "  Step 1070/1169 | Loss: 0.5715 | Batch acc: 0.94\n",
      "  Step 1080/1169 | Loss: 0.6346 | Batch acc: 0.88\n",
      "  Step 1090/1169 | Loss: 0.5694 | Batch acc: 0.88\n",
      "  Step 1100/1169 | Loss: 0.7057 | Batch acc: 0.78\n",
      "  Step 1110/1169 | Loss: 0.5964 | Batch acc: 0.81\n",
      "  Step 1120/1169 | Loss: 0.4858 | Batch acc: 0.97\n",
      "  Step 1130/1169 | Loss: 0.5143 | Batch acc: 0.91\n",
      "  Step 1140/1169 | Loss: 0.5165 | Batch acc: 0.94\n",
      "  Step 1150/1169 | Loss: 0.4360 | Batch acc: 0.97\n",
      "  Step 1160/1169 | Loss: 0.4221 | Batch acc: 0.97\n",
      "Train: loss=0.5802, acc=0.907\n",
      "  Step 1/306 | Loss: 1.6239 | Batch acc: 0.38\n",
      "  Step 10/306 | Loss: 0.6200 | Batch acc: 0.84\n",
      "  Step 20/306 | Loss: 0.3363 | Batch acc: 1.00\n",
      "  Step 30/306 | Loss: 0.3360 | Batch acc: 1.00\n",
      "  Step 40/306 | Loss: 0.3276 | Batch acc: 1.00\n",
      "  Step 50/306 | Loss: 1.3168 | Batch acc: 0.25\n",
      "  Step 60/306 | Loss: 1.0293 | Batch acc: 0.62\n",
      "  Step 70/306 | Loss: 0.3533 | Batch acc: 1.00\n",
      "  Step 80/306 | Loss: 0.3386 | Batch acc: 1.00\n",
      "  Step 90/306 | Loss: 1.7870 | Batch acc: 0.25\n",
      "  Step 100/306 | Loss: 1.1185 | Batch acc: 0.69\n",
      "  Step 110/306 | Loss: 1.9040 | Batch acc: 0.16\n",
      "  Step 120/306 | Loss: 1.3882 | Batch acc: 0.59\n",
      "  Step 130/306 | Loss: 0.8352 | Batch acc: 0.88\n",
      "  Step 140/306 | Loss: 0.4987 | Batch acc: 0.97\n",
      "  Step 150/306 | Loss: 0.4680 | Batch acc: 1.00\n",
      "  Step 160/306 | Loss: 0.3402 | Batch acc: 1.00\n",
      "  Step 170/306 | Loss: 0.8514 | Batch acc: 0.72\n",
      "  Step 180/306 | Loss: 0.5261 | Batch acc: 0.94\n",
      "  Step 190/306 | Loss: 0.9376 | Batch acc: 0.81\n",
      "  Step 200/306 | Loss: 0.3989 | Batch acc: 1.00\n",
      "  Step 210/306 | Loss: 0.3404 | Batch acc: 1.00\n",
      "  Step 220/306 | Loss: 0.4513 | Batch acc: 0.97\n",
      "  Step 230/306 | Loss: 0.3665 | Batch acc: 1.00\n",
      "  Step 240/306 | Loss: 2.8398 | Batch acc: 0.00\n",
      "  Step 250/306 | Loss: 2.5936 | Batch acc: 0.03\n",
      "  Step 260/306 | Loss: 0.4109 | Batch acc: 1.00\n",
      "  Step 270/306 | Loss: 0.3619 | Batch acc: 1.00\n",
      "  Step 280/306 | Loss: 0.3437 | Batch acc: 1.00\n",
      "  Step 290/306 | Loss: 0.3273 | Batch acc: 1.00\n",
      "  Step 300/306 | Loss: 0.3523 | Batch acc: 1.00\n",
      "Val:   loss=0.8511, acc=0.773\n",
      "  âœ… New best model saved â†’ /home/stud/fwag/bhome/ele670_project/results/tiny_resnet50_best.pt (acc=0.773)\n",
      "\n",
      "Validation Confusion Matrix (rows=true, cols=pred):\n",
      "[[   0    0    0    0    0    0    0   11    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    4    0   21   11    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   0    1    2    0  213  634    1  692    2   87   15]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]\n",
      " [   4    0    0    0   13  146    1 6806  528   54   15]\n",
      " [   0    0    0    0    8   17    0  124   98   15    1]\n",
      " [   0    0    0    0    1  219    0   12    0    7    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0]]\n",
      "\n",
      "Per-class metrics (validation):\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "         Angiectasia      0.000     0.000     0.000        11\n",
      "       Blood - fresh      0.000     0.000     0.000         0\n",
      "             Erosion      0.000     0.000     0.000        36\n",
      "            Erythema      0.000     0.000     0.000         0\n",
      "        Foreign Body      0.000     0.000     0.000         0\n",
      "     Ileocecal valve      0.622     0.385     0.475      1647\n",
      "    Lymphangiectasia      0.000     0.000     0.000         0\n",
      " Normal clean mucosa      0.888     0.899     0.894      7567\n",
      "             Pylorus      0.153     0.373     0.217       263\n",
      "Reduced Mucosal View      0.043     0.029     0.035       239\n",
      "               Ulcer      0.000     0.000     0.000         0\n",
      "\n",
      "            accuracy                          0.773      9763\n",
      "           macro avg      0.155     0.153     0.147      9763\n",
      "        weighted avg      0.798     0.773     0.780      9763\n",
      "\n",
      "\n",
      "Training finished!\n",
      "Best validation accuracy: 0.773\n",
      "Elapsed time: 146.1 sec\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # Tiny ResNet50 sanity trainer for Kvasir-Capsule (debug-friendly)\n",
    "# \n",
    "# * Beginner-friendly, **now extremely commented**\n",
    "# * Safe, small training run (2 epochs) to check end-to-end wiring\n",
    "# * Works well on Tesla V100 (uses mixed precision)\n",
    "# * Video-independent split (by `video_id`) to avoid leakage\n",
    "# * Extra debug prints (device, memory, batch shapes, step logs)\n",
    "# * Optional: freeze backbone for very fast convergence of the classifier head\n",
    "# * Validation summary includes confusion matrix + per-class metrics\n",
    "# \n",
    "# âž• Changes vs earlier cell:\n",
    "# - Uses **new AMP API**: `from torch import amp; amp.autocast('cuda')` to avoid deprecation warnings\n",
    "# - Adds **label smoothing** + optional **weight decay** for stability\n",
    "# - Adds **GPU-2 pinning** (can be disabled by setting GPU_INDEX=None)\n",
    "# - Adds **confusion matrix** + **classification report** after validation\n",
    "# \n",
    "# NOTE: The logic below is *unchanged*; only comments were added for clarity.\n",
    "\n",
    "# %%\n",
    "# -------------------------\n",
    "# Standard library imports\n",
    "# -------------------------\n",
    "# os       : filesystem paths, environment variables (e.g., CUDA_VISIBLE_DEVICES)\n",
    "# time     : simple wall-clock timing of the whole train run\n",
    "# math     : not strictly needed here, but often handy for schedulers, etc.\n",
    "# warnings : to silence benign warnings for a cleaner notebook output\n",
    "import os, time, math, warnings\n",
    "from collections import Counter  # (not used below, but often useful for label histograms)\n",
    "\n",
    "# -------------------------\n",
    "# Third-party imports\n",
    "# -------------------------\n",
    "# pandas      : reading the metadata CSV with image paths / labels / groups\n",
    "# numpy       : CPU-side numeric ops, array concatenation for metrics\n",
    "# PIL.Image   : robust image loading; handles various formats\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "\n",
    "# -------------------------\n",
    "# PyTorch / TorchVision\n",
    "# -------------------------\n",
    "# torch             : tensors, autograd, CUDA utils\n",
    "# torch.nn          : neural network layers + loss functions\n",
    "# DataLoader        : mini-batching, shuffling, prefetching with workers\n",
    "# torchvision       : common transforms and pretrained CNN backbones\n",
    "# amp               : Automatic Mixed Precision for speed + memory savings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models\n",
    "from torch import amp\n",
    "\n",
    "# -------------------------\n",
    "# Scikit-learn utilities\n",
    "# -------------------------\n",
    "# GroupShuffleSplit : ensures train/val split is **video-independent** (group = video_id)\n",
    "# Metrics           : confusion matrix + per-class precision/recall/F1 report\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Silence overly chatty user warnings (e.g., PIL/torchvision), keeps output readable.\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "\n",
    "# ========================\n",
    "# CONFIGURATION CONSTANTS\n",
    "# ========================\n",
    "# Paths to data and outputs. Adjust these to your environment.\n",
    "# Paths to data and outputs. Adjust these to your environment.\n",
    "CSV_PATH      = \"/home/stud/fwag/bhome/ele670_project/data/processed/metadata_filtered.csv\"\n",
    "IMG_ROOT      = \"/home/stud/fwag/bhome/ele670_project/data/raw/labelled_images\"\n",
    "RESULTS_DIR   = \"/home/stud/fwag/bhome/ele670_project/results\"\n",
    "SAVE_PATH     = os.path.join(RESULTS_DIR, \"tiny_resnet50_best.pt\")\n",
    "\n",
    "# Optional: path to class-name mapping saved by LabelEncoder during preprocessing\n",
    "LABELS_NPY    = \"/home/stud/fwag/bhome/ele670_project/data/processed/label_classes_filtered.npy\"\n",
    "\n",
    "# Core training hyperparameters\n",
    "IMG_SIZE      = 224           # input resolution expected by ResNet50\n",
    "BATCH_SIZE    = 32            # try 64/96 if VRAM allows; affects throughput + stability\n",
    "NUM_WORKERS   = 6             # dataloader subprocesses; tune for server CPU\n",
    "EPOCHS        = 2             # keep short for sanity runs; increase when confident\n",
    "LR            = 1e-3          # Adam learning rate\n",
    "WEIGHT_DECAY  = 1e-4          # small L2 regularization for generalization\n",
    "LABEL_SMOOTH  = 0.05          # reduces overconfidence; helps class-imbalance a bit\n",
    "FREEZE_BACKBONE = True        # train only final FC layer for a *very* fast sanity pass\n",
    "USE_PRETRAINED = True         # start from ImageNet weights for quick convergence\n",
    "SEED          = 42            # reproducibility (affects split/shuffle/initialization)\n",
    "\n",
    "# Optional: pin training to a specific GPU index (e.g., GPU 2 on a multi-GPU server).\n",
    "# If None, uses default CUDA device selection.\n",
    "GPU_INDEX     = 2\n",
    "\n",
    "# How often to print batch-level logs during training/validation loops\n",
    "LOG_EVERY     = 10\n",
    "\n",
    "# ----------------------------\n",
    "# (1) Reproducibility seeding\n",
    "# ----------------------------\n",
    "torch.manual_seed(SEED)  # torch RNG seed (CPU + CUDA if available)\n",
    "np.random.seed(SEED)     # numpy RNG seed\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# (2) Pin to a specific GPU *before* constructing CUDA objects\n",
    "# -----------------------------------------------------------\n",
    "# Best practice in notebooks: mask other GPUs so the chosen one looks like cuda:0\n",
    "if torch.cuda.is_available() and GPU_INDEX is not None:\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(GPU_INDEX)\n",
    "\n",
    "# Now, after potentially masking devices, re-detect CUDA availability\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "if DEVICE == \"cuda\":\n",
    "    # Report which GPU and current memory state; helps verify the right GPU is in use.\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print(\"Allocated:\", round(torch.cuda.memory_allocated()/1024**3, 2), \"GB\")\n",
    "    print(\"Reserved: \", round(torch.cuda.memory_reserved()/1024**3, 2), \"GB\")\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "# Enable cudNN autotuner: faster convs when input shapes are static (typical in CV)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "try:\n",
    "    # Slightly faster matmul kernels on Ampere+; safe to ignore if unsupported\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# ========================\n",
    "# DATASET IMPLEMENTATION\n",
    "# ========================\n",
    "class KvasirDataset(Dataset):\n",
    "    \"\"\"Thin PyTorch Dataset wrapper around a DataFrame of image rows.\n",
    "    \n",
    "    Each row must contain:\n",
    "    - image_path     : relative path to the image file on disk (under IMG_ROOT)\n",
    "    - encoded_label  : integer class id (0..num_classes-1)\n",
    "    \n",
    "    Returns (image_tensor, label_int) for each index. If an image fails to load,\n",
    "    we emit a warning and return a black placeholder to keep the batch shapes valid.\n",
    "    \"\"\"\n",
    "    def __init__(self, df: pd.DataFrame, img_root: str, transform):\n",
    "        # Keep a defensive copy with consecutive indices\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.img_root = img_root\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        # Required for PyTorch to know how many items are in the dataset\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Index into the DataFrame to fetch the sample\n",
    "        row = self.df.iloc[idx]\n",
    "        rel = row[\"image_path\"]  # relative path under IMG_ROOT\n",
    "        path = os.path.join(self.img_root, rel)\n",
    "        try:\n",
    "            # Open image robustly and convert to 3-channel RGB\n",
    "            img = Image.open(path).convert(\"RGB\")\n",
    "        except (FileNotFoundError, UnidentifiedImageError) as e:\n",
    "            # If the file is missing or corrupted, warn and provide a dummy image\n",
    "            print(f\"[WARNING] Failed to open {path}: {e}\")\n",
    "            img = Image.new(\"RGB\", (IMG_SIZE, IMG_SIZE), (0, 0, 0))\n",
    "        # Apply torchvision transforms (resize, flip, tensorize, normalize)\n",
    "        x = self.transform(img)\n",
    "        # Labels are expected to already be numeric in the CSV\n",
    "        y = int(row[\"encoded_label\"])  # integer class id\n",
    "        return x, y\n",
    "\n",
    "# ==================\n",
    "# LOAD & VALIDATE CSV\n",
    "# ==================\n",
    "# The CSV is expected to contain at least: image_path, encoded_label, video_id\n",
    "# (finding_class is optional, used only for pretty printing class names later.)\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "print(\"First rows of metadata_normalized.csv:\")\n",
    "print(df.head())\n",
    "\n",
    "# Basic schema checks to fail fast if the CSV is malformed\n",
    "required_cols = {\"image_path\", \"encoded_label\", \"video_id\"}\n",
    "missing = required_cols - set(df.columns)\n",
    "assert not missing, f\"CSV is missing required columns: {missing}\"\n",
    "\n",
    "# Determine number of unique classes from the encoded label column\n",
    "num_classes = df[\"encoded_label\"].nunique()\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "\n",
    "# Prepare (optional) mapping from integer labels to human-readable names.\n",
    "# We'll finalize it *after* the split, using the most common name per label in TRAIN.\n",
    "if \"finding_class\" in df.columns:\n",
    "    label_to_names = None  # filled later from the training subset\n",
    "else:\n",
    "    label_to_names = None  # will default to generic names\n",
    "\n",
    "# ==============================================\n",
    "# TRAIN/VAL SPLIT â€” **VIDEO-INDEPENDENT** SPLIT\n",
    "# ==============================================\n",
    "# GroupShuffleSplit ensures all frames from the same video_id are kept together\n",
    "# (i.e., no leakage where the model sees frames of the same video in both splits.)\n",
    "gss = GroupShuffleSplit(n_splits=1, train_size=0.8, random_state=SEED)\n",
    "train_idx, val_idx = next(gss.split(df, groups=df[\"video_id\"]))\n",
    "df_train, df_val = df.iloc[train_idx].copy(), df.iloc[val_idx].copy()\n",
    "\n",
    "print(f\"Train samples: {len(df_train)} | Val samples: {len(df_val)}\")\n",
    "print(f\"Train videos: {df_train['video_id'].nunique()} | Val videos: {df_val['video_id'].nunique()}\")\n",
    "\n",
    "# --------------- LABEL NAMES (preferred: from LabelEncoder) ---------------\n",
    "# Try to load class names saved during preprocessing with scikit-learn's LabelEncoder.\n",
    "# This guarantees the names align EXACTLY with how `encoded_label` was produced.\n",
    "try:\n",
    "    classes = np.load(LABELS_NPY, allow_pickle=True)\n",
    "    label_to_names = {i: str(name) for i, name in enumerate(classes)}\n",
    "    print(f\"Loaded {len(label_to_names)} class names from {LABELS_NPY}\")\n",
    "except Exception as e:\n",
    "    # Graceful fallback: infer from TRAIN frequencies (previous behavior)\n",
    "    print(f\"[WARN] Could not load {LABELS_NPY} ({e}). Falling back to inferring from TRAIN.\")\n",
    "    if \"finding_class\" in df.columns:\n",
    "        names = (\n",
    "            df_train.groupby([\"encoded_label\", \"finding_class\"]).size()\n",
    "            .reset_index(name=\"count\")\n",
    "            .sort_values([\"encoded_label\", \"count\"], ascending=[True, False])\n",
    "            .drop_duplicates(subset=[\"encoded_label\"])  # keep top name per label\n",
    "        )\n",
    "        label_to_names = dict(zip(names[\"encoded_label\"].astype(int), names[\"finding_class\"].astype(str)))\n",
    "    else:\n",
    "        label_to_names = {i: f\"class_{i}\" for i in range(num_classes)}\n",
    "\n",
    "# =====================\n",
    "# IMAGE TRANSFORMATIONS\n",
    "# =====================\n",
    "# Standard ImageNet normalization statistics for ResNet-family backbones\n",
    "mean = (0.485, 0.456, 0.406)\n",
    "std  = (0.229, 0.224, 0.225)\n",
    "\n",
    "# TRAIN pipeline: deterministic resize â†’ light augmentation â†’ tensorize â†’ normalize\n",
    "train_tf = T.Compose([\n",
    "    T.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    T.RandomHorizontalFlip(p=0.5),  # helps against left/right biases and small dataset size\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean, std),\n",
    "])\n",
    "\n",
    "# VAL pipeline: resize + normalization only (no randomness)\n",
    "val_tf = T.Compose([\n",
    "    T.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean, std),\n",
    "])\n",
    "\n",
    "# ==================\n",
    "# DATA LOADERS (PyTorch)\n",
    "# ==================\n",
    "# pin_memory=True: speeds up hostâ†’GPU transfers\n",
    "# persistent_workers=True: avoids worker restart overhead across epochs\n",
    "# prefetch_factor: how many batches each worker preloads; tune with NUM_WORKERS\n",
    "train_ds = KvasirDataset(df_train, IMG_ROOT, train_tf)\n",
    "val_ds   = KvasirDataset(df_val,   IMG_ROOT, val_tf)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "    num_workers=NUM_WORKERS, pin_memory=True,\n",
    "    persistent_workers=True, prefetch_factor=4, drop_last=True  # drop_last=True for even batch shapes on AMP\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "    num_workers=NUM_WORKERS, pin_memory=True,\n",
    "    persistent_workers=True, prefetch_factor=4, drop_last=False\n",
    ")\n",
    "\n",
    "# Quick sanity print to confirm tensor shapes and label dtype\n",
    "print(\"One training batch (images, labels):\")\n",
    "sample_imgs, sample_labels = next(iter(train_loader))\n",
    "print(\"Image batch shape:\", sample_imgs.shape)   # [B, 3, H, W]\n",
    "print(\"Label batch shape:\", sample_labels.shape) # [B]\n",
    "\n",
    "# ==============\n",
    "# MODEL DEFINITION\n",
    "# ==============\n",
    "# Load a ResNet50 backbone; optionally with ImageNet-1K weights for faster convergence.\n",
    "if USE_PRETRAINED:\n",
    "    try:\n",
    "        weights = models.ResNet50_Weights.IMAGENET1K_V2\n",
    "    except AttributeError:\n",
    "        # TorchVision < 0.13 compatibility fallback\n",
    "        weights = \"IMAGENET1K_V2\"\n",
    "    model = models.resnet50(weights=weights)\n",
    "else:\n",
    "    model = models.resnet50(weights=None)\n",
    "\n",
    "# Replace the final classification head to match our dataset's class count\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "\n",
    "# Optionally freeze the convolutional backbone so only the final FC trains.\n",
    "# This is great for quick sanity checks; later you can unfreeze for full finetuning.\n",
    "if FREEZE_BACKBONE:\n",
    "    for name, p in model.named_parameters():\n",
    "        if not name.startswith(\"fc.\"):\n",
    "            p.requires_grad_(False)\n",
    "\n",
    "# Move the model to the chosen device (GPU or CPU)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "# ======================\n",
    "# LOSS FUNCTION & OPTIMIZER\n",
    "# ======================\n",
    "# CrossEntropyLoss with label-smoothing mitigates over-confident predictions and can\n",
    "# slightly improve generalization on small / imbalanced datasets.\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=LABEL_SMOOTH)\n",
    "\n",
    "# Only optimize parameters that are marked as trainable (respects FREEZE_BACKBONE)\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.Adam(params, lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# AMP gradient scaler for stable mixed-precision training on CUDA\n",
    "scaler = amp.GradScaler('cuda') if DEVICE == 'cuda' else None\n",
    "\n",
    "# ======================\n",
    "# TRAIN / EVAL UTILITIES\n",
    "# ======================\n",
    "@torch.no_grad()\n",
    "def _accuracy(logits: torch.Tensor, labels: torch.Tensor) -> float:\n",
    "    \"\"\"Compute top-1 accuracy for a batch (utility; not used in final printouts).\"\"\"\n",
    "    return (logits.argmax(dim=1) == labels).float().mean().item()\n",
    "\n",
    "\n",
    "def run_epoch(loader, train: bool = True, desc: str = \"train\"):\n",
    "    \"\"\"Runs one pass over a DataLoader.\n",
    "    \n",
    "    Args:\n",
    "      loader: DataLoader yielding (images, labels)\n",
    "      train : if True, updates model weights; if False, evaluation-only\n",
    "      desc  : string for logging (\"train\"/\"val\")\n",
    "    Returns:\n",
    "      (avg_loss, avg_accuracy)\n",
    "    \"\"\"\n",
    "    model.train(train)  # toggles dropout/batchnorm behavior as appropriate\n",
    "\n",
    "    total_loss, total_correct, total_seen = 0.0, 0, 0\n",
    "\n",
    "    for step, (images, labels) in enumerate(loader, 1):\n",
    "        # Non-blocking copies overlap data transfer with compute on CUDA\n",
    "        images = images.to(DEVICE, non_blocking=True)\n",
    "        labels = labels.to(DEVICE, non_blocking=True)\n",
    "\n",
    "        # Create an autocast context for mixed precision on CUDA; CPU uses full precision\n",
    "        if DEVICE == 'cuda':\n",
    "            autocast_ctx = amp.autocast('cuda', enabled=True)\n",
    "        else:\n",
    "            from contextlib import nullcontext\n",
    "            autocast_ctx = nullcontext()\n",
    "\n",
    "        # Forward pass (mixed precision when enabled)\n",
    "        with autocast_ctx:\n",
    "            logits = model(images)\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "        if train:\n",
    "            # Standard fused optimizer step; with AMP we scale/unscale grads to avoid underflow\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            if scaler is not None:\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        # --- Metrics bookkeeping in FP32 ---\n",
    "        with torch.no_grad():\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct = (preds == labels).sum().item()\n",
    "            batch_sz = labels.size(0)\n",
    "            total_correct += correct\n",
    "            total_seen += batch_sz\n",
    "            total_loss += float(loss.item()) * batch_sz  # sum of per-sample losses\n",
    "\n",
    "        # Periodic progress print for visibility\n",
    "        if step % LOG_EVERY == 0 or step == 1:\n",
    "            batch_acc = correct / batch_sz\n",
    "            print(f\"  Step {step}/{len(loader)} | Loss: {loss.item():.4f} | Batch acc: {batch_acc:.2f}\")\n",
    "\n",
    "    # Compute dataset-wide averages\n",
    "    avg_loss = total_loss / max(1, total_seen)\n",
    "    avg_acc  = total_correct / max(1, total_seen)\n",
    "    return avg_loss, avg_acc\n",
    "\n",
    "\n",
    "def collect_predictions(loader):\n",
    "    \"\"\"Run the model in eval mode and collect all predictions + labels.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    y_true : np.ndarray of shape (N,)\n",
    "        Ground-truth integer labels for the entire split.\n",
    "    y_pred : np.ndarray of shape (N,)\n",
    "        Predicted class indices (argmax over logits) for the entire split.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds  = []\n",
    "\n",
    "    for images, labels in loader:\n",
    "        images = images.to(DEVICE, non_blocking=True)\n",
    "        labels = labels.to(DEVICE, non_blocking=True)\n",
    "        with torch.no_grad():\n",
    "            if DEVICE == 'cuda':\n",
    "                with amp.autocast('cuda', enabled=True):\n",
    "                    logits = model(images)\n",
    "            else:\n",
    "                logits = model(images)\n",
    "            preds = logits.argmax(dim=1)\n",
    "        all_labels.append(labels.cpu().numpy())\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "\n",
    "    # Concatenate list of arrays into one long vector per side\n",
    "    y_true = np.concatenate(all_labels)\n",
    "    y_pred = np.concatenate(all_preds)\n",
    "    return y_true, y_pred\n",
    "\n",
    "# ============================\n",
    "# MAIN TRAINING CONTROL-LOOP\n",
    "# ============================\n",
    "print(\"\\n==== START TRAINING ====\\n\")\n",
    "best_val_acc = 0.0  # track best validation accuracy to decide checkpointing\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    print(f\"\\nEpoch {epoch}/{EPOCHS}\")\n",
    "\n",
    "    # One training epoch\n",
    "    tr_loss, tr_acc = run_epoch(train_loader, train=True, desc=\"train\")\n",
    "    print(f\"Train: loss={tr_loss:.4f}, acc={tr_acc:.3f}\")\n",
    "\n",
    "    # One validation epoch (no weight updates)\n",
    "    vl_loss, vl_acc = run_epoch(val_loader, train=False, desc=\"val\")\n",
    "    print(f\"Val:   loss={vl_loss:.4f}, acc={vl_acc:.3f}\")\n",
    "\n",
    "    # Save a checkpoint if validation accuracy improved\n",
    "    if vl_acc > best_val_acc:\n",
    "        best_val_acc = vl_acc\n",
    "        torch.save(model.state_dict(), SAVE_PATH)\n",
    "        print(f\"  âœ… New best model saved â†’ {SAVE_PATH} (acc={best_val_acc:.3f})\")\n",
    "\n",
    "    # --- Post-epoch diagnostics on the validation split ---\n",
    "    y_true, y_pred = collect_predictions(val_loader)\n",
    "\n",
    "    # 1) Confusion matrix: rows = actual class, columns = predicted class\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=list(range(num_classes)))\n",
    "    print(\"\\nValidation Confusion Matrix (rows=true, cols=pred):\")\n",
    "    # Pretty-print small matrices to console; for large K consider saving to file\n",
    "    with np.printoptions(threshold=200, linewidth=200):\n",
    "        print(cm)\n",
    "\n",
    "    # 2) Per-class precision/recall/F1 (macro/weighted) with human-friendly names\n",
    "    target_names = [label_to_names.get(i, f\"class_{i}\") for i in range(num_classes)]\n",
    "    print(\"\\nPer-class metrics (validation):\")\n",
    "    print(classification_report(y_true, y_pred,\n",
    "                                labels=list(range(num_classes)),\n",
    "                                target_names=target_names,\n",
    "                                digits=3))\n",
    "\n",
    "print(\"\\nTraining finished!\")\n",
    "print(\"Best validation accuracy:\", round(best_val_acc, 3))\n",
    "print(\"Elapsed time: %.1f sec\" % (time.time() - start_time))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ele670)",
   "language": "python",
   "name": "ele670"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
